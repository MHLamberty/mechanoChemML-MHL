
====================================================================================================
docs\conf.py
====================================================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# mechanoChemML documentation build configuration file, created by
# sphinx-quickstart on Fri Mar 13 09:37:45 2020.
#
# This file is execfile()d with the current directory set to its
# containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#
# import os
# import sys
# sys.path.insert(0, os.path.abspath('.'))

import sphinx_rtd_theme
import numpydoc
import sphinx


# will ensure that __init__() is documented
def skip(app, what, name, obj, would_skip, options):
    #if name == "__init__":
    #    return False
    return would_skip

def setup(app):
    app.connect("autodoc-skip-member", skip)

with open("../VERSION.md", "r") as f:
    current_version = f.read().strip()

# -- General configuration ------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#
# needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = ['sphinx.ext.autodoc',
    'sphinx.ext.doctest',
    'sphinx.ext.intersphinx',
    'sphinx.ext.todo',
    'sphinx.ext.autosummary',
    'sphinx.ext.coverage',
    'sphinx.ext.mathjax',
    'sphinx.ext.ifconfig',
    'sphinx.ext.viewcode',
    'sphinx.ext.extlinks',
    'sphinx.ext.githubpages',
    'sphinx_rtd_theme', # nice theme
    'sphinx.ext.napoleon', # to use together with numpydoc. 
    'numpydoc', # allow to use the numpydoc style and have parameters
    'autoapi.extension',
    ]
autoapi_dirs = [
        '../mechanoChemML/src/',
        '../mechanoChemML/workflows/',
        ]
# autoapi_dirs = ['../mechanoChemML/component', '../mechanoChemML/dns_wrapper', '../mechanoChemML/utility', '../mechanoChemML/workflows']
numpydoc_show_class_members = False # turn off some warning.

#Remove autoapi for examples and non-local calc files
autoapi_ignore = ["*example_*.py", "*Example*.py", "*src/graph_*", '*src/estimator*']


# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix(es) of source filenames.
# You can specify multiple suffix as a list of string:
#
# source_suffix = ['.rst', '.md']
source_suffix = '.rst'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = 'mechanoChemML'
copyright = '2021, the Computational Physics Group at the University of Michigan'
author = 'Computational Physics Group'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = current_version
# The full version, including alpha/beta/rc tags.
release = current_version

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#
# This is also used if you do content translation via gettext catalogs.
# Usually you set "language" from the command line for these cases.
language = None

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This patterns also effect to html_static_path and html_extra_path
exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']


# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# If true, `todo` and `todoList` produce output, else they produce nothing.
todo_include_todos = True


# -- Options for HTML output ----------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#
# html_theme = 'alabaster'
html_theme = "sphinx_rtd_theme"

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#
# html_theme_options = {}

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
# html_static_path = ['_static']

# Custom sidebar templates, must be a dictionary that maps document names
# to template names.
#
# This is required for the alabaster theme
# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
html_sidebars = {
    '**': [
        'relations.html',  # needs 'show_related': True theme option to display
        'searchbox.html',
    ]
}


html_show_sourcelink = True
# -- Options for HTMLHelp output ------------------------------------------

# Output file base name for HTML help builder.
htmlhelp_basename = 'mechanoChemMLdoc'


# -- Options for LaTeX output ---------------------------------------------

latex_elements = {
    # The paper size ('letterpaper' or 'a4paper').
    #
    # 'papersize': 'letterpaper',

    # The font size ('10pt', '11pt' or '12pt').
    #
    # 'pointsize': '10pt',

    # Additional stuff for the LaTeX preamble.
    #
    # 'preamble': '',

    # Latex figure (float) alignment
    #
    # 'figure_align': 'htbp',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title,
#  author, documentclass [howto, manual, or own class]).
latex_documents = [
    (master_doc, 'mechanoChemML.tex', 'mechanoChemML Documentation',
     'Computational Physics Group', 'manual'),
]


# -- Options for manual page output ---------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    (master_doc, 'mechanoChemML', 'mechanoChemML Documentation',
     [author], 1)
]


# -- Options for Texinfo output -------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
    (master_doc, 'mechanoChemML', 'mechanoChemML Documentation',
     author, 'mechanoChemML', 'One line description of project.',
     'Miscellaneous'),
]




# Example configuration for intersphinx: refer to the Python standard library.
intersphinx_mapping = {'https://docs.python.org/': None}


====================================================================================================
examples\active_learning\Example1_NiAl\__init__.py
====================================================================================================


====================================================================================================
examples\active_learning\Example1_NiAl\main.py
====================================================================================================
#!/usr/bin/env python
import os
from mechanoChemML.workflows.active_learning.active_learning import Active_learning

input_path = os.path.dirname(__file__)
if input_path: # when it is not ''
    input_path += '/NiAl_free_energy.ini'
else:
    input_path = 'NiAl_free_energy.ini' 

workflow = Active_learning(input_path)
workflow.main_workflow()

====================================================================================================
examples\active_learning\Example1_NiAl\main_test.py
====================================================================================================
#!/usr/bin/env python
import os
import tensorflow as tf
from mechanoChemML.workflows.active_learning.active_learning import Active_learning

physical_devices = tf.config.list_physical_devices('GPU')
try:
  tf.config.experimental.set_memory_growth(physical_devices[0], True)
except:
  # Invalid device or cannot modify virtual devices once initialized.
  pass

input_path = os.path.dirname(__file__)
if input_path: # when it is not ''
    input_path += '/NiAl_test.ini'
else:
    input_path = 'NiAl_test.ini' 

workflow = Active_learning(input_path,test=True)
workflow.main_workflow()


====================================================================================================
examples\install_tensorflow.py
====================================================================================================
import os
import pip

def get_cuda_version():
    def get_version(output_info):
        all_info = output_info.split('\n')
        release_info = [x for x in all_info if x.find('release')>= 0]
        return release_info[0].split(',')[-1].strip()[1:].split('.')

    cuda_version = None
    # first try
    output_info = os.popen("nvcc --version").read()
    if output_info.find('release') >= 0:
        cuda_version = get_version(output_info)
    # second try in case nvcc is not in the $PATH
    output_info = os.popen("/usr/local/cuda/bin/./nvcc --version").read()
    if output_info.find('release') >= 0:
        cuda_version = get_version(output_info)
    return cuda_version

# install tensorflow that is compatible with the detected cuda version
cuda_version = get_cuda_version()
if cuda_version is not None:
    if cuda_version[0] == '10':
        pip.main(["install", "tensorflow>=2.2,<2.4"])
    elif cuda_version[0] == '11':
        pip.main(["install", "tensorflow>=2.4"])
# install tensorflow without cuda support as cuda is not detected
else:
    pip.main(["install", "tensorflow>=2.2"])

import tensorflow as tf
# install tensorflow_probability
def force_install(package, versions=None):

    """install one package with versions """

    if versions is not None:
        pip.main(['install', package+'=='+versions])
    else:
        pip.main(['install', package])

def check_and_install_tfp(package, versions=None):

    """make sure tfp is compatiable with tf """

    try:
        __import__(package)
    except ImportError:
        force_install(package, versions)
    import tensorflow_probability as tfp
    tfp_version=tfp.__version__.split(".")
    if tfp_version[0:2] != versions.split(".")[0:2]:
        force_install(package, versions)

tf_version=tf.__version__.split(".")
if tf_version[0] == '2' and tf_version[1] == '8': check_and_install_tfp('tensorflow_probability', '0.16.0')
if tf_version[0] == '2' and tf_version[1] == '7': check_and_install_tfp('tensorflow_probability', '0.15.0')
if tf_version[0] == '2' and tf_version[1] == '6': check_and_install_tfp('tensorflow_probability', '0.14.0')
if tf_version[0] == '2' and tf_version[1] == '5': check_and_install_tfp('tensorflow_probability', '0.13.0')
if tf_version[0] == '2' and tf_version[1] == '4': check_and_install_tfp('tensorflow_probability', '0.12.0')
if tf_version[0] == '2' and tf_version[1] == '3': check_and_install_tfp('tensorflow_probability', '0.11.0')
if tf_version[0] == '2' and tf_version[1] == '2': check_and_install_tfp('tensorflow_probability', '0.10.0')


====================================================================================================
examples\mr_learning\Example1_single_microstructure_dnn\step1_hp_search_main\hyper_parameter_search.py
====================================================================================================
# for legacy python compatibility
from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

import os, sys
import datetime
import mechanoChemML.workflows.mr_learning.mrnn_utility as mrnn_utility
import mechanoChemML.workflows.mr_learning.mrnn_models as mrnn_models
import mechanoChemML.src.hparameters_dnn_grid as HParametersGridDNN
import mechanoChemML.src.kfold_train as KFoldTrain

args = mrnn_utility.sys_args()

args.configfile = 'dnn-free-energy-1dns.ini'
args.platform = 'gpu'
args.inspect = 0
args.debug = False
args.verbose = 1
args.show = 0

mrnn_utility.notebook_args(args)
config = mrnn_utility.read_config_file(args.configfile, args.debug)
dataset, labels, derivative, train_stats = mrnn_utility.load_all_data(config, args)

str_form = config['FORMAT']['PrintStringForm']
epochs = int(config['MODEL']['Epochs'])
batch_size = int(config['MODEL']['BatchSize'])
verbose = int(config['MODEL']['Verbose'])
n_splits = int(config['MODEL']['KFoldTrain'])

parameter = HParametersGridDNN.HyperParametersDNN(
    config,
    input_shape=len(dataset.keys()),
    output_shape=len(labels.keys()),
    uniform_sample_number=25,
    neighbor_sample_number=1,
    iteration_time=3,
    sample_ratio=0.3,
    best_model_number=20,
    max_total_parameter=680,
    repeat_train=n_splits,
    debug=args.debug)

the_kfolds = KFoldTrain.MLKFold(n_splits, dataset)

model_summary_list = []
while True:
    para_id, para_str, train_flag = parameter.get_next_model()
    if (train_flag and the_kfolds.any_left_fold()):
        model_name_id = str(para_id) + '-' + para_str
        print(str_form.format('Model: '), model_name_id)

        checkpoint_dir = config['RESTART']['CheckPointDir'] + config['MODEL']['ParameterID']
        model_path = checkpoint_dir + '/' + 'model.h5'

        train_dataset, train_labels, val_dataset, val_labels, test_dataset, test_labels, test_derivative = the_kfolds.get_next_fold(dataset, labels, derivative)

        model = mrnn_models.build_model(config, train_dataset, train_labels)

        metrics = mrnn_utility.getlist_str(config['MODEL']['Metrics'])
        optimizer = mrnn_models.build_optimizer(config)
        loss = mrnn_models.build_loss(config)
        model.compile(loss=loss, optimizer=optimizer, metrics=metrics)

        callbacks = mrnn_models.build_callbacks(config)
        history = model.fit(
            train_dataset.to_numpy(),
            train_labels.to_numpy(),
            epochs=epochs,
            batch_size=batch_size,
            validation_data=(val_dataset.to_numpy(), val_labels.to_numpy()),    # or validation_split= 0.1,
            verbose=verbose,
            callbacks=callbacks)

        model.summary()
        parameter.update_model_info(para_id, history.history)
        the_kfolds.update_kfold_status()


====================================================================================================
examples\mr_learning\Example1_single_microstructure_dnn\step2_final_dnn_main\dnn_1dns_final.py
====================================================================================================
# for legacy python compatibility
from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

import os, sys
import datetime
import mechanoChemML.workflows.mr_learning.mrnn_utility as mrnn_utility
import mechanoChemML.workflows.mr_learning.mrnn_models as mrnn_models
import mechanoChemML.src.kfold_train as KFoldTrain

args = mrnn_utility.sys_args()

args.configfile = 'dnn-free-energy-1dns-final.ini'

args.platform = 'gpu'
args.inspect = 0
args.debug = False
args.verbose = 1
args.show = 0

mrnn_utility.notebook_args(args)
config = mrnn_utility.read_config_file(args.configfile, args.debug)
dataset, labels, derivative, train_stats = mrnn_utility.load_all_data(config, args)

str_form = config['FORMAT']['PrintStringForm']
epochs = int(config['MODEL']['Epochs'])
batch_size = int(config['MODEL']['BatchSize'])
verbose = int(config['MODEL']['Verbose'])
n_splits = int(config['MODEL']['KFoldTrain'])

the_kfolds = KFoldTrain.MLKFold(n_splits, dataset)
train_dataset, train_labels, val_dataset, val_labels, test_dataset, test_labels, test_derivative = the_kfolds.get_next_fold(dataset, labels, derivative, final_data=True)

model_summary_list = []

config['RESTART']['CheckPointDir'] = './saved_weight'
config['MODEL']['ParameterID'] = ''
checkpoint_dir = config['RESTART']['CheckPointDir'] + config['MODEL']['ParameterID']
model_path = checkpoint_dir + '/' + 'model.h5'

model = mrnn_models.build_model(config, train_dataset, train_labels)

if (config['RESTART']['RestartWeight'].lower() == 'y'):
    print('checkpoint_dir for restart: ', checkpoint_dir)
    latest = tf.train.latest_checkpoint(checkpoint_dir)
    print("latest checkpoint: ", latest)
    if (latest != None):
        model.load_weights(latest)
        print("Successfully load weight: ", latest)
    else:
        print("No saved weights, start to train the model from the beginning!")
        pass

metrics = mrnn_utility.getlist_str(config['MODEL']['Metrics'])
optimizer = mrnn_models.build_optimizer(config)
loss = mrnn_models.build_loss(config)
model.compile(loss=loss, optimizer=optimizer, metrics=metrics)

callbacks = mrnn_models.build_callbacks(config)
history = model.fit(
    train_dataset,
    train_labels,
    epochs=epochs,
    batch_size=batch_size,
    validation_data=(val_dataset, val_labels),
    verbose=verbose,
    callbacks=callbacks)

model.summary()

label_scale = float(config['TEST']['LabelScale'])
all_data = {'test_label': [], 'test_nn': [], 'val_label': [], 'val_nn': [], 'train_label': [], 'train_nn': []}

test_nn = model.predict(test_dataset, verbose=0, batch_size=batch_size)
val_nn = model.predict(val_dataset, verbose=0, batch_size=batch_size)
train_nn = model.predict(train_dataset, verbose=0, batch_size=batch_size)

for i in np.squeeze(test_nn):
    all_data['test_nn'].append(i / label_scale)
for i in np.squeeze(val_nn):
    all_data['val_nn'].append(i / label_scale)
for i in np.squeeze(train_nn):
    all_data['train_nn'].append(i / label_scale)

for i in test_labels['Psi_me']:
    all_data['test_label'].append(i / label_scale)
for i in val_labels['Psi_me']:
    all_data['val_label'].append(i / label_scale)
for i in train_labels['Psi_me']:
    all_data['train_label'].append(i / label_scale)
# print('all_data: ', all_data)

import pickle
import time
now = time.strftime("%Y%m%d%H%M%S")
pickle_out = open('all_data_' + now + '.pickle', "wb")
pickle.dump(all_data, pickle_out)
pickle_out.close()

pickle_out = open('history_' + now + '.pickle', "wb")
pickle.dump(history.history, pickle_out)
pickle_out.close()
print('save to: ', 'all_data_' + now + '.pickle', 'history_' + now + '.pickle')


====================================================================================================
examples\mr_learning\Example1_single_microstructure_dnn\step2_final_dnn_main\plot_data_and_history.py
====================================================================================================
import sys
import pandas as pd
import matplotlib.pyplot as plt
import pickle

if __name__ == '__main__':
    if (len(sys.argv) == 1):
        print('please provide all_data_20200601081735.pickle or history_20200601081735.pickle file')
        exit(0)

    timemark = sys.argv[1].split('_')[-1].split('.pickle')[0]
    print('timemark:', timemark)

    history_file = 'history_' + timemark + '.pickle'
    all_data_file = 'all_data_' + timemark + '.pickle'
    print('loading data:', history_file, all_data_file)

    all_data = pickle.load(open(all_data_file, "rb"))
    history = pickle.load(open(history_file, "rb"))

    epoches = range(0, len(history['loss']))

    #----------------------plot 1---------------------------------------
    plt.clf()
    plt.semilogy(epoches, history['loss'], 'b', lw=1.0, label='Training')
    plt.semilogy(epoches, history['val_loss'], 'k', lw=1.0, label='Validation')
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.legend()
    # plt.axis('equal')
    plt.savefig('1dns-free-energy-learning-dnn.png', bbox_inches='tight', format='png')
    plt.show()

    #----------------------plot 2---------------------------------------
    plt.clf()
    plt.plot(all_data['test_label'], all_data['test_nn'], 'k.')
    xmin = min(min(all_data['test_label']), min(all_data['test_nn']))
    xmax = max(max(all_data['test_label']), max(all_data['test_nn']))
    plt.plot([xmin, xmax], [xmin, xmax], 'k-', lw=1.0)

    plt.xlim([xmin, xmax])
    plt.ylim([xmin, xmax])
    plt.xlabel('$\Psi^0_{\mathrm{mech,DNS}}$')
    plt.ylabel('$\Psi^0_{\mathrm{mech,DNN}}$')
    plt.savefig('1dns-free-energy-test-dnn.png', bbox_inches='tight', format='png')
    plt.show()
    # ----------------------------------------------------------------------

    all_dns_data = all_data['train_label']
    all_dns_data.extend(all_data['val_label'])
    all_dns_data.extend(all_data['test_label'])

    all_nn_data = all_data['train_nn']
    all_nn_data.extend(all_data['val_nn'])
    all_nn_data.extend(all_data['test_nn'])

    reference_data_file = '../data/data_8.csv'
    sep = ','
    fields = ['index', 'Psi_me']
    selected_cols = pd.read_csv(reference_data_file, index_col=False, sep=sep, usecols=fields, skipinitialspace=True)[fields]

    ordered_all_dns_data = []
    ordered_all_nn_data = []

    # sorted all the data based on the frame number
    for ref0 in selected_cols['Psi_me']:
        # print(ref0)
        for j0 in range(0, len(all_dns_data)):
            # print(j0, len(all_dns_data))
            val0 = all_dns_data[j0]
            if (abs(ref0 - val0) < 1e-9):
                ordered_all_dns_data.append(all_dns_data[j0])
                ordered_all_nn_data.append(all_nn_data[j0])
                del all_dns_data[j0]
                del all_nn_data[j0]
                break

    #----------------------plot 3---------------------------------------
    plt.clf()
    plt.plot(selected_cols['index'], ordered_all_dns_data, 'k', linewidth=4, label='DNS')
    plt.plot(selected_cols['index'], ordered_all_nn_data, 'r', label='DNN')
    # plt.plot( selected_cols['index'], selected_cols['Psi_me'], 'b')
    plt.xlabel('frame number')
    plt.ylabel('$\Psi^0_{\mathrm{mech}}$')
    plt.xlim([0, 900])
    # plt.axis('equal')
    plt.legend()
    plt.savefig('1dns-free-energy-predict-all-dnn.png', bbox_inches='tight', format='png')
    plt.show()


====================================================================================================
examples\mr_learning\Example1_single_microstructure_dnn\step3_hp_search_mrnn_detail\hyper_parameter_search.py
====================================================================================================
# for legacy python compatibility
from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

import os, sys
import datetime

import mechanoChemML.workflows.mr_learning.mrnn_utility as mrnn_utility
import mechanoChemML.workflows.mr_learning.mrnn_models as mrnn_models
import mechanoChemML.src.hparameters_dnn_grid as HParametersGridDNN
import mechanoChemML.src.kfold_train as KFoldTrain

args = mrnn_utility.sys_args()

args.configfile = 'kbnn-load-dnn-1-frame-hyperparameter-search.ini'
args.platform = 'gpu'
args.inspect = 0
args.debug = False
args.verbose = 1
args.show = 0

mrnn_utility.notebook_args(args)
config = mrnn_utility.read_config_file(args.configfile, args.debug)
dataset, labels, derivative, train_stats = mrnn_utility.load_all_data(config, args)

str_form = config['FORMAT']['PrintStringForm']
epochs = int(config['MODEL']['Epochs'])
batch_size = int(config['MODEL']['BatchSize'])
verbose = int(config['MODEL']['Verbose'])
n_splits = int(config['MODEL']['KFoldTrain'])

parameter = HParametersGridDNN.HyperParametersDNN(
    config,
    input_shape=len(dataset.keys()),
    output_shape=1,
    uniform_sample_number=25,
    neighbor_sample_number=1,
    iteration_time=3,
    sample_ratio=0.3,
    best_model_number=20,
    max_total_parameter=1280,
    repeat_train=n_splits,
    debug=args.debug)

the_kfolds = KFoldTrain.MLKFold(n_splits, dataset)

model_summary_list = []
while True:
    para_id, para_str, train_flag = parameter.get_next_model()
    if (train_flag and the_kfolds.any_left_fold()):
        model_name_id = str(para_id) + '-' + para_str
        print(str_form.format('Model: '), model_name_id)

        checkpoint_dir = config['RESTART']['CheckPointDir'] + config['MODEL']['ParameterID']
        model_path = checkpoint_dir + '/' + 'model.h5'

        train_dataset, train_labels, val_dataset, val_labels, test_dataset, test_labels, test_derivative = the_kfolds.get_next_fold(dataset, labels, derivative)

        model = mrnn_models.build_model(config, train_dataset, train_labels, train_stats=train_stats)
        # https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model

        metrics = mrnn_utility.getlist_str(config['MODEL']['Metrics'])
        optimizer = mrnn_models.build_optimizer(config)
        loss = mrnn_models.my_mse_loss_with_grad(BetaP=0.0)
        model.compile(loss=loss, optimizer=optimizer, metrics=metrics)
        label_scale = float(config['TEST']['LabelScale'])

        callbacks = mrnn_models.build_callbacks(config)

        train_dataset = train_dataset.to_numpy()
        train_labels = train_labels.to_numpy()
        val_dataset = val_dataset.to_numpy()
        val_labels = val_labels.to_numpy()
        test_dataset = test_dataset.to_numpy()
        test_labels = test_labels.to_numpy()

        # make sure that the derivative data is scaled correctly

        # The NN/DNS scaled derivative data should be: * label_scale * train_stats['std'] (has already multiplied by label_scale )

        # Since the feature is scaled, and label psi is scaled, the S_NN will be scaled to: label_scale * train_stats['std']
        # the model will scale S_NN back to no-scaled status.
        # here we scale F, and P to no-scaled status
        modified_label_scale = np.array(
            [1.0, 1.0 / label_scale, 1.0 / label_scale, 1.0 / label_scale, 1.0 / label_scale, 1.0 / label_scale, 1.0 / label_scale, 1.0 / label_scale, 1.0 / label_scale])
        train_labels = train_labels * modified_label_scale
        val_labels = val_labels * modified_label_scale
        test_labels = test_labels * modified_label_scale

        history = model.fit(
            train_dataset,
            train_labels,
            epochs=epochs,
            batch_size=batch_size,
            validation_data=(val_dataset, val_labels),    # or validation_split= 0.1,
            verbose=verbose,
            callbacks=callbacks)

        model.summary()
        parameter.update_model_info(para_id, history.history)
        the_kfolds.update_kfold_status()


====================================================================================================
examples\mr_learning\Example1_single_microstructure_dnn\step4_final_mrnn_no_penalize_P\kbnn_1_frame_dnn.py
====================================================================================================
# for legacy python compatibility
from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os, sys
import datetime

import mechanoChemML.workflows.mr_learning.mrnn_utility as mrnn_utility
import mechanoChemML.workflows.mr_learning.mrnn_models as mrnn_models
import mechanoChemML.src.kfold_train as KFoldTrain

args = mrnn_utility.sys_args()

args.configfile = 'kbnn-load-dnn-1-frame.ini'

args.platform = 'gpu'
args.inspect = 0
args.debug = False
args.verbose = 1
args.show = 0

mrnn_utility.notebook_args(args)
config = mrnn_utility.read_config_file(args.configfile, args.debug)
dataset, labels, derivative, train_stats = mrnn_utility.load_all_data(config, args)

str_form = config['FORMAT']['PrintStringForm']
epochs = int(config['MODEL']['Epochs'])
batch_size = int(config['MODEL']['BatchSize'])
verbose = int(config['MODEL']['Verbose'])
n_splits = int(config['MODEL']['KFoldTrain'])

the_kfolds = KFoldTrain.MLKFold(n_splits, dataset)
train_dataset, train_labels, val_dataset, val_labels, test_dataset, test_labels, test_derivative = the_kfolds.get_next_fold(dataset, labels, derivative, final_data=True)

model_summary_list = []

config['RESTART']['CheckPointDir'] = './saved_weight'
config['MODEL']['ParameterID'] = ''
checkpoint_dir = config['RESTART']['CheckPointDir'] + config['MODEL']['ParameterID']
model_path = checkpoint_dir + '/' + 'model.h5'

model = mrnn_models.build_model(config, train_dataset, train_labels, train_stats=train_stats)

if (config['RESTART']['RestartWeight'].lower() == 'y'):
    print('checkpoint_dir for restart: ', checkpoint_dir)
    latest = tf.train.latest_checkpoint(checkpoint_dir)
    print("latest checkpoint: ", latest)
    if (latest != None):
        model.load_weights(latest)
        print("Successfully load weight: ", latest)
    else:
        print("No saved weights, start to train the model from the beginning!")
        pass

metrics = mrnn_utility.getlist_str(config['MODEL']['Metrics'])
optimizer = mrnn_models.build_optimizer(config)
loss = mrnn_models.my_mse_loss_with_grad(BetaP=0.0)
model.compile(loss=loss, optimizer=optimizer, metrics=metrics)
label_scale = float(config['TEST']['LabelScale'])

callbacks = mrnn_models.build_callbacks(config)
train_dataset = train_dataset.to_numpy()
train_labels = train_labels.to_numpy()
val_dataset = val_dataset.to_numpy()
val_labels = val_labels.to_numpy()
test_dataset = test_dataset.to_numpy()
test_labels = test_labels.to_numpy()

# make sure that the derivative data is scaled correctly

# The NN/DNS scaled derivative data should be: * label_scale * train_stats['std'] (has already multiplied by label_scale )

# Since the feature is scaled, and label psi is scaled, the S_NN will be scaled to: label_scale * train_stats['std']
# the model will scale S_NN back to no-scaled status.
# here we scale F, and P to no-scaled status
modified_label_scale = np.array(
    [1.0, 1.0 / label_scale, 1.0 / label_scale, 1.0 / label_scale, 1.0 / label_scale, 1.0 / label_scale, 1.0 / label_scale, 1.0 / label_scale, 1.0 / label_scale])
train_labels = train_labels * modified_label_scale
val_labels = val_labels * modified_label_scale
test_labels = test_labels * modified_label_scale
# print(type(train_dataset))
history = model.fit(
    train_dataset,
    train_labels,
    epochs=epochs,
    batch_size=batch_size,
    validation_data=(val_dataset, val_labels),    # or validation_split= 0.1,
    verbose=verbose,
    callbacks=callbacks)

model.summary()
# print("history: " , history.history['loss'], history.history['val_loss'], history.history)

all_data = {'test_label': [], 'test_nn': [], 'val_label': [], 'val_nn': [], 'train_label': [], 'train_nn': []}

test_nn = model.predict(test_dataset, verbose=0, batch_size=batch_size)
val_nn = model.predict(val_dataset, verbose=0, batch_size=batch_size)
train_nn = model.predict(train_dataset, verbose=0, batch_size=batch_size)

for i in np.squeeze(test_nn):
    # print('test_nn:', i)
    all_data['test_nn'].append(i[0] / label_scale)
for i in np.squeeze(val_nn):
    all_data['val_nn'].append(i[0] / label_scale)
for i in np.squeeze(train_nn):
    all_data['train_nn'].append(i[0] / label_scale)

for i in test_labels:
    all_data['test_label'].append(i[0] / label_scale)
    # print('test_label: ', i)
for i in val_labels:
    all_data['val_label'].append(i[0] / label_scale)
for i in train_labels:
    all_data['train_label'].append(i[0] / label_scale)
# print('all_data: ', all_data)
print('test_nn shape: ', np.shape(np.squeeze(test_nn)))
print('test_labels shape: ', np.shape(test_labels))

import pickle
import time
now = time.strftime("%Y%m%d%H%M%S")
pickle_out = open('all_data_' + now + '.pickle', "wb")
pickle.dump(all_data, pickle_out)
pickle_out.close()

pickle_out = open('history_' + now + '.pickle', "wb")
pickle.dump(history.history, pickle_out)
pickle_out.close()

# all_data['P_DNS']= test_labels[:,1:4]/label_scale/train_stats['std'].to_numpy()[0:3]
# all_data['P_NN'] = test_nn[:,1:4]/label_scale/train_stats['std'].to_numpy()[0:3]
all_data['P_DNS'] = test_labels[:, 1:5]
all_data['P_NN'] = test_nn[:, 1:5]

pickle_out = open('all_P_' + now + '.pickle', "wb")
pickle.dump(all_data, pickle_out)
pickle_out.close()

print('save to: ', 'all_data_' + now + '.pickle', 'history_' + now + '.pickle', 'all_P_' + now + '.pickle')
print('the prediction of P and delta Psi_me is not the best model fit with lowest loss!')


====================================================================================================
examples\mr_learning\Example1_single_microstructure_dnn\step4_final_mrnn_no_penalize_P\plot_data_and_history_kbnn_P.py
====================================================================================================
#!/usr/bin/env python3
import sys
import pandas as pd
import matplotlib.pyplot as plt
import pickle
import numpy as np

if __name__ == '__main__':
    if (len(sys.argv) == 1):
        print('please provide all_data_20200601081735.pickle or history_20200601081735.pickle file')
        exit(0)

    timemark = sys.argv[1].split('_')[-1].split('.pickle')[0]
    print('timemark:', timemark)

    history_file = 'history_' + timemark + '.pickle'
    all_data_file = 'all_data_' + timemark + '.pickle'
    print('loading data:', history_file, all_data_file)

    all_data = pickle.load(open(all_data_file, "rb"))
    history = pickle.load(open(history_file, "rb"))

    epoches = range(0, len(history['loss']))

    #----------------------plot 1---------------------------------------
    plt.clf()
    plt.semilogy(epoches, history['loss'], 'b', lw=1.0, label='Training')
    plt.semilogy(epoches, history['val_loss'], 'k', lw=1.0, label='Validation')
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.legend()
    # plt.axis('equal')
    plt.savefig('kbnn-dnn-1-frame-learning.png', bbox_inches='tight', format='png')
    plt.show()

    #----------------------plot 2---------------------------------------
    plt.clf()
    plt.plot(all_data['test_label'], all_data['test_nn'], 'k.')
    xmin = min(min(all_data['test_label']), min(all_data['test_nn']))
    xmax = max(max(all_data['test_label']), max(all_data['test_nn']))
    plt.plot([xmin, xmax], [xmin, xmax], 'k-', lw=1.0)

    plt.xlim([xmin, xmax])
    plt.ylim([xmin, xmax])
    plt.xlabel('$\Delta\Psi_{\mathrm{mech,DNS}}$')
    plt.ylabel('$\Delta\Psi_{\mathrm{mech,KBNN}}$')
    plt.savefig('kbnn-dnn-1-frame-test.png', bbox_inches='tight', format='png')
    plt.show()

    #----------------------plot 3---------------------------------------
    all_P_file = 'all_P_' + timemark + '.pickle'
    print('loading P:', all_P_file)
    all_P = pickle.load(open(all_P_file, "rb"))

    def plot_P_one_field(plt, P_true, P_pred, ind0):
        plt.clf()
        plt.plot(P_true[:, ind0], P_pred[:, ind0], 'k.')
        amax = max(np.amax(P_pred[:, ind0]), np.amax(P_true[:, ind0]))
        amin = min(np.amin(P_pred[:, ind0]), np.amin(P_true[:, ind0]))
        if (ind0 == 0):
            pre_fix = 'P11'
        elif (ind0 == 1):
            pre_fix = 'P12'
        elif (ind0 == 2):
            pre_fix = 'P21'
        elif (ind0 == 3):
            pre_fix = 'P22'
        else:
            raise ValueError('Unknown value for ind0')
        # plt.gca().set_title(pre_fix)
        plt.xlabel('True Values')
        plt.ylabel('Predicted Values')
        plt.axis('equal')
        plt.axis('square')
        _ = plt.plot([amin, amax], [amin, amax], 'k-', lw=1.0)
        plt.xlim([amin, amax])
        plt.ylim([amin, amax])
        plt.savefig('kbnn-dnn-1-frame-' + pre_fix + '.png', bbox_inches='tight', format='png')
        plt.show()

    plot_P_one_field(plt, all_P['P_DNS'], all_P['P_NN'], 0)
    plot_P_one_field(plt, all_P['P_DNS'], all_P['P_NN'], 1)
    plot_P_one_field(plt, all_P['P_DNS'], all_P['P_NN'], 2)
    plot_P_one_field(plt, all_P['P_DNS'], all_P['P_NN'], 3)


====================================================================================================
examples\non_local_calculus\Example1_Derivative_Calculation\estimate_derivatives.py
====================================================================================================
#!/usr/bin/env python
import os
os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'
#
# Import python modules
import sys,glob,copy,itertools,os
import numpy as np
import pandas as pd

import matplotlib
import matplotlib.pyplot as plt


# Import user modules

# Global Variables
DELIMETER='__'
MAX_PROCESSES = 1
PARALLEL = 0

from mechanoChemML.src.graph_main import main

if __name__ == '__main__':



##############################################################
# Choose directories and data files

	cwd = os.path.abspath(os.path.expanduser('.'))
	directories_load = ['data']
	directories_dump = ['result']
	file = 'func_val.csv'
##############################################################
# Choose model parameters

	model_p = 3
	model_order = 2

##############################################################
# Set settings
	


	settings = {
		#Set Paths
		'cwd': cwd,
		'directories_load':directories_load,
		'directories_dump':directories_dump,
		'data_filename':file,


			
		#Model settings
		'model_order':[model_order],
		'model_p':[model_p],			

		#Graph structure settings
		'algebraic_operations': [[
			{'func':lambda df: df['x_1'] + df['x_2'] + df['x_3'], 'labels':'u_3'} , 
			]],
		
		#Graph structure settings
		'differential_operations':[[
			*[{'function':'u_%i'%i,
			'variable': ['x_%i'%j],
			'weight':['stencil'], 
			'adjacency':['nearest'], #(symmetric finite difference like scheme),
			'manifold':[['x_1', 'x_2', 'x_3']], 
			'accuracy': [2],						
			'dimension':[j-1],	#Index of dimension taking partial derivative about 			
			'order':1,
			'operation':['partial'],
			} for i in range(1,4) for j in range(1,4)]
			]]
		}
##############################################################
# Call main function
	
	main(settings = settings)
		

====================================================================================================
examples\non_local_calculus\Example1_Derivative_Calculation\generate_data.py
====================================================================================================
###################################
####### Required libraries: #######
#######      scikit-learn   #######
#######         numpy       #######
###################################

import sys, os
import numpy as np
import pandas as pd 

np.set_printoptions(precision=5)


if __name__ == "__main__":    
	
	
	# Choose directories and data files
	out_dir = './data/'
	out_file = 'func_val.csv'
	
	NumPoints = 1000
	#Randomly sample [x_1, x_2, x_3]\in [0,1]^3 
	x_1 = np.random.uniform(0,1,[NumPoints,1])
	x_2 = np.random.uniform(0,1,[NumPoints,1])
	x_3 = np.random.uniform(0,1,[NumPoints,1])
	
	#Define some functions
	u_1 = x_1**2 + x_2**2 + x_3**2 
	u_2 = (x_1**2 + x_2**2)*np.sin(10*x_1)
	
	df = pd.DataFrame(np.hstack((x_1, x_2, x_3, u_1, u_2)), columns = ['x_1','x_2','x_3','u_1','u_2'])
	
	
	if not os.path.exists(out_dir):
	  os.makedirs(out_dir)
	  print('New directory created: %s'%out_dir)
	df.to_csv(out_dir + out_file)

====================================================================================================
examples\non_local_calculus\Example2_Allen_Cahn\dns\dns.py
====================================================================================================
#!/usr/bin/env python

import os,sys,json,copy

import dolfin
from dolfin import MPI,XDMFFile,HDF5File
from dolfin import IntervalMesh,CompiledSubDomain, MeshFunction, CellVolume
from dolfin import VectorFunctionSpace, FunctionSpace,Function,TestFunction,Expression
from dolfin import project,inner,assemble,vertex_to_dof_map,dx, Measure
from dolfin import DirichletBC
from dolfin import NonlinearVariationalProblem,NonlinearVariationalSolver
from dolfin import dot, Constant, diff, div, grad,derivative

import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt



# Logging
import logging,logging.handlers
log = 'warning'


rootlogger = logging.getLogger()
rootlogger.setLevel(getattr(logging,log.upper()))
stdlogger = logging.StreamHandler(sys.stdout)
stdlogger.setLevel(getattr(logging,log.upper()))
rootlogger.addHandler(stdlogger)	


logger = logging.getLogger(__name__)
logger.setLevel(getattr(logging,log.upper()))



class dns:
	def __init__(self):
		self.defaults = {
			'path':'data',
                        'file':{'xdmf':'data','hdf5':'data','data':'data','log':'log','plot':'plot','observables':'data'},
                        'ext':{'xdmf':'xdmf','hdf5':'h5','data':'csv','log':'log','plot':'pdf','observables':'csv'},
			'fields':{
				'c':{
					'type':'scalar','dimension':1,'space':'V',
					'initial':'(1.0/5.0)*pow(x[0],5) - (1.0/3.0)*pow(x[0],3) + 1/(5.0*3.0)','bcs':'neumann',
					'plot':{'x':['Time'],'y':['Phi_1p'],'texify':{'Time':'t','x':'x','Phi_1p':'\\varphi'},'mplstyle':'plot.mplstyle','animate':1},
					}
				},
			'spaces': {
				'V':{'type':'FunctionSpace','family':'CG','degree':2},
				'W':{'type':'VectorFunctionSpace','family':'CG','degree':1},
				'Z':{'type':'FunctionSpace','family':'DG','degree':0},
				},
			'potential':{
				'expression':'0.25 - 0.5 * pow(c,2) + 0.25 * pow(c,4)',
				'degree':4,
				'derivative':'-c + pow(c,3)',
				'kwargs':{'c':None}
				},
			'num_elem':10,
			'L_0':0,
			'L_1':1,
			'D':1,
			'alpha':1,
			'dt':0.1,
			'N':10,
			'tol':1e-5,
			'plot':True
		}

		return


	def set_parameters(self,parameters={},reset=False):
		field = '_parameters'
		default = {}
		if not hasattr(self,field) or reset:
			setattr(self,field,default)
		if isinstance(parameters,str):
			with open(parameters,'r') as file:
				getattr(self,field).update(json.load(file))
		elif isinstance(parameters,dict):
			getattr(self,field).update(parameters)

		getattr(self,field).update({parameter: self.defaults[parameter] 
						   for parameter in self.defaults if parameter not in getattr(self,field)})

		self.set_paths()
		return

	def get_parameters(self):
		field = '_parameters'		
		return getattr(self,field)

	def set_data(self,data={},reset=False):
		field = '_data'
		default = {key: [] for key in data}

		paths = self.get_paths()		

		if not hasattr(self,field) or reset:
			setattr(self,field,default)
		for key in data:
			try:
				getattr(self,field)[key].append(data[key])
			except:
				getattr(self,field)[key] = [data[key]] if data[key] not in [[]] else data[key]


		for key in getattr(self,field):
			path = paths['data'].replace('.','_%s.'%(str(key)))
			self.dump({key:np.array(getattr(self,field)[key])},path)
		return

	def get_data(self):
		field = '_data'
		default = {}
		paths = self.get_paths()		
		parameters = self.get_parameters()
		if not hasattr(self,field):
			try:
				values = {}
				for key in parameters['fields']:
					path = paths['data'].replace('.','_%s.'%(str(key)))
					values.update({key: self.load(path)})
			except:
				values = default
			setattr(self,field,values)
		else:
			values = getattr(self,field)
		return values


	def set_observables(self,observables={},reset=False):
		header = {
			'Time':'Time',
			'total_energy':'TE','chi':'Chi',
			'total_energy_p':'TE_P','chi_p':'Chi_P','total_energy_m':'TE_M','chi_m':'Chi_M',
			'Phi_0p':'Phi_0P','Phi_1p':'Phi_1P','Phi_2p':'Phi_2P','Phi_3p':'Phi_3P','Phi_4p':'Phi_4P','Phi_5p':'Phi_5P',
			'Phi_0m':'Phi_0M','Phi_1m':'Phi_1M','Phi_2m':'Phi_2M','Phi_3m':'Phi_3M','Phi_4m':'Phi_4M','Phi_5m':'Phi_5M',					
			'gradient_energy':'GradE','landau_energy':'LanE','diffusion_energy':'LapC','spinodal_energy':'dLan',
			'gradient_energy_p':'GradE_P','landau_energy_p':'LanE_P','diffusion_energy_p':'LapC_P','spinodal_energy_p':'dLan_P',					
			'gradient_energy_m':'GradE_M','landau_energy_m':'LanE_M','diffusion_energy_m':'LapC_M','spinodal_energy_m':'dLan_M',										
		}	

		field = '_observables'
		default = {}

		paths = self.get_paths()		
		if not hasattr(self,field) or reset:
			setattr(self,field,default)

		for key in observables:
			for observable in observables[key]:
				try:
					getattr(self,field)[key][observable].append(observables[key][observable])
				except:
					try:
						getattr(self,field)[key][observable] = [observables[key][observable]]
					except:
						getattr(self,field)[key] = {}
						getattr(self,field)[key][observable] = [observables[key][observable]]



		for key in getattr(self,field):
			path = paths['observables'].replace('.','_%s.'%(str(key)))
			self.dump({key:{header.get(k,k): getattr(self,field)[key][k] for k in getattr(self,field)[key]}},path)
		return


	def get_observables(self):
		field = '_observables'
		default = {}
		paths = self.get_paths()		
		parameters = self.get_parameters()
		if not hasattr(self,field):
			try:
				values = {}
				for key in parameters['fields']:
					path = paths['observables'].replace('.','_%s.'%(str(key)))
					values.update({key: self.load(path)})
			except:
				values = default
			setattr(self,field,values)
		else:
			values = getattr(self,field)
		return values


	def set_paths(self):
		field = '_paths'
		parameters = self.get_parameters()
		paths = {file:os.path.abspath(os.path.join(parameters['path'],'%s.%s'%(parameters['file'][file],parameters['ext'][file])))
					for file in parameters['file']}



		for path in paths:
			directory = os.path.dirname(paths[path])
			if not os.path.exists(directory):
				os.makedirs(directory)


		setattr(self,field,paths)

		return

	def get_paths(self):
		field = '_paths'
		return getattr(self,field)


	def set_logger(self,logger):

		field = '_logger'

		if logger is None:
			logger = logging.getLogger(__name__)
			logger.setLevel(getattr(logging,log.upper()))

		filelogger = logging.handlers.RotatingFileHandler(self.get_paths()['log'])
		fileloggerformatter = logging.Formatter(
			fmt='%(asctime)s: %(message)s',
			datefmt='%Y-%m-%d %H:%M:%S')
		filelogger.setFormatter(fileloggerformatter)
		filelogger.setLevel(getattr(logging,log.upper()))
		if len(rootlogger.handlers) == 2:
			rootlogger.removeHandler(rootlogger.handlers[-1])
		rootlogger.addHandler(filelogger)

		setattr(self,field,getattr(logger,log))

		return


	def get_logger(self):
		field = '_logger'
		return getattr(self,field)

	def load(self,path,wr='r'):
		ext = path.split('.')[-1]
		if ext in ['csv']:
			data = pd.read_csv(path)
		elif ext in ['npy']:
			data = np.load(path)
		return data

	def dump(self,data,path,wr='w'):
		for key in data:
			if isinstance(data[key],dict):

				values = data[key]
				header = list(values)
				values = np.atleast_2d([values[observable] for observable in values]).T
				
				# kwargs = {'fname':path,'X':values,'header':header,'fmt':'%0.8f','delimiter':',','comments':''}
				# np.savetxt(**kwargs)

				values = pd.DataFrame(values,columns=header)
				kwargs = {'index':False}
				values.to_csv(path,**kwargs)

			elif isinstance(data[key],np.ndarray):
				np.save(path,data[key])

		return

	def simulate(self,parameters=None,logger=None):
		self.model(parameters,logger)
		

	def model(self,parameters=None,logger=None):


		def format(arr):
			return np.array(arr)

		# Setup parameters, logger
		self.set_parameters(parameters)
		self.set_logger(logger)

		# Get parameters
		parameters = self.get_parameters()


		#SS#if not parameters.get('simulate'):
		#SS#	return


		# Setup data
		self.set_data(reset=True)
		self.set_observables(reset=True)



		# Show simulation settings
		self.get_logger()('\n\t'.join(['Simulating:',*['%s : %r'%(param,parameters[param] if not isinstance(parameters[param],dict) else list(parameters[param])) 
														for param in ['fields','num_elem','N','dt','D','alpha','tol']]]))


		# Data mesh
		mesh = IntervalMesh(MPI.comm_world,parameters['num_elem'],parameters['L_0'],parameters['L_1'])

		# Fields
		V = {}
		W = {}
		fields_n = {}
		fields = {}		
		w = {}
		fields_0 = {}
		potential = {}
		potential_derivative = {}
		bcs = {}
		R = {}
		J = {}
		# v2d_vector = {}
		observables = {}
		for field in parameters['fields']:

			# Define functions
			V[field] = {}
			w[field] = {}
			for space in parameters['spaces']:
				V[field][space] = getattr(dolfin,parameters['spaces'][space]['type'])(mesh,parameters['spaces'][space]['family'],parameters['spaces'][space]['degree'])
				w[field][space] = TestFunction(V[field][space])

			
			space = V[field][parameters['fields'][field]['space']]
			test = w[field][parameters['fields'][field]['space']]

			fields_n[field] = Function(space,name='%sn'%(field))
			fields[field] = Function(space,name=field)


			# Inital condition
			fields_0[field] = Expression(parameters['fields'][field]['initial'],element = space.ufl_element())
			fields_n[field] = project(fields_0[field],space)
			fields[field].assign(fields_n[field])


			# Define potential
			if parameters['potential'].get('kwargs') is None:
				parameters['potential']['kwargs'] = {}
			for k in parameters['potential']['kwargs']:
				if parameters['potential']['kwargs'][k] is None:
					parameters['potential']['kwargs'][k] = fields[k]

			potential[field] = Expression(parameters['potential']['expression'], degree=parameters['potential']['degree'],**parameters['potential']['kwargs'])
			potential_derivative[field] = Expression(parameters['potential']['derivative'], degree=parameters['potential']['degree']-1,**parameters['potential']['kwargs'])

			#Subdomain for defining Positive grain
			sub_domains = MeshFunction('size_t', mesh, mesh.topology().dim(), 0)

			#BC condition
			bcs[field] = []
			if parameters['fields'][field]['bcs'] == 'dirichlet':
				BC_l =  CompiledSubDomain('near(x[0], side) && on_boundary', side = parameters['L_0'])
				BC_r=  CompiledSubDomain('near(x[0], side) && on_boundary', side = parameters['L_1'])
				bcl = DirichletBC(V, fields_n[field], BC_l)
				bcr = DirichletBC(V, fields_n[field], BC_r)
				bcs[field].extend([bcl,bcr])
			elif parameters['fields'][field]['bcs'] == 'neumann':
				bcs[field].extend([])


			# Residual and Jacobian
			R[field] = (((fields[field]-fields_n[field])/parameters['dt']*test*dx) + 
						(inner(parameters['D']*grad(test),grad(fields[field]))*dx) + 
						(parameters['alpha']*potential_derivative[field]*test*dx))
			
			J[field] = derivative(R[field], fields[field])



			# Observables
			observables[field] = {}


		files = {
			'xdmf':XDMFFile(MPI.comm_world,self.get_paths()['xdmf']),
			'hdf5':HDF5File(MPI.comm_world,self.get_paths()['hdf5'],'w'),
			}			

		files['hdf5'].write(mesh,'/mesh')
		eps = lambda n,key,field,observables,tol: (n==0) or (abs(observables[key]['total_energy'][n]-observables[key]['total_energy'][n-1])/(observables[key]['total_energy'][0]) > tol)
		flag = {field: True for field in parameters['fields']}		
		tol = {field: parameters['tol'][field] if isinstance(parameters['tol'],dict) else parameters['tol'] for field in parameters['fields']}		
		phases = {'p':1,'m':2}
		n=0
		problem = {}
		solver = {}
		while(n<parameters['N'] and any([flag[field] for field in parameters['fields']])):

			self.get_logger()('Time: %d'%(n))
			
			for field in parameters['fields']:

				if not flag[field]:
					continue

				# Solve
				problem[field] = NonlinearVariationalProblem(R[field],fields[field],bcs[field],J[field])
				solver[field] = NonlinearVariationalSolver(problem[field])
				solver[field].solve()


				# Get field array
				array = assemble((1/CellVolume(mesh))*inner(fields[field], w[field]['Z'])*dx).get_local()


 
				# Observables
				observables[field]['Time'] = parameters['dt']*n

				# observables[field]['energy_density'] = format(0.5*dot(grad(fields[field]),grad(fields[field]))*dx)
				observables[field]['gradient_energy'] = format(assemble(0.5*dot(grad(fields[field]),grad(fields[field]))*dx(domain=mesh)))
				observables[field]['landau_energy'] = format(assemble(potential[field]*dx(domain=mesh)))
				observables[field]['diffusion_energy'] = format(assemble(project(div(project(grad(fields[field]),V[field]['W'])),V[field]['Z'])*dx(domain=mesh))) #Diffusion part of chemical potential
				observables[field]['spinodal_energy'] = format(assemble(potential_derivative[field]*dx(domain=mesh))) #Spinodal part of chemical potential

				observables[field]['total_energy'] = parameters['D'] * observables[field]['gradient_energy'] + parameters['alpha'] *observables[field]['landau_energy']
				observables[field]['chi'] =  parameters['alpha']*observables[field]['diffusion_energy'] - parameters['D']*observables[field]['spinodal_energy']





				# Phase observables
				sub_domains.set_all(0)
				sub_domains.array()[:] = np.where(array > 0.0, phases['p'],phases['m'])
				
				phases_dxp = Measure('dx', domain = mesh, subdomain_data = sub_domains)
				for phase in phases:
					dxp = phases_dxp(phases[phase])

					observables[field]['Phi_0%s'%(phase)] = format(assemble(1*dxp))
					observables[field]['Phi_1%s'%(phase)] = format(assemble(fields[field]*dxp))
					observables[field]['Phi_2%s'%(phase)] = format(assemble(fields[field]*fields[field]*dxp))
					observables[field]['Phi_3%s'%(phase)] = format(assemble(fields[field]*fields[field]*fields[field]*dxp))
					observables[field]['Phi_4%s'%(phase)] = format(assemble(fields[field]*fields[field]*fields[field]*fields[field]*dxp))
					observables[field]['Phi_5%s'%(phase)] = format(assemble(fields[field]*fields[field]*fields[field]*fields[field]*fields[field]*dxp))

					observables[field]['gradient_energy_%s'%(phase)] = format(assemble(0.5*dot(grad(fields[field]),grad(fields[field]))*dxp))	
					observables[field]['landau_energy_%s'%(phase)] = format(assemble(potential[field]*dxp))
					observables[field]['total_energy_%s'%(phase)] = parameters['D'] * observables[field]['gradient_energy_%s'%(phase)] + parameters['alpha'] *observables[field]['landau_energy_%s'%(phase)]			

					observables[field]['diffusion_energy_%s'%(phase)] = format(assemble(project(div(project(grad(fields[field]),V[field]['W'])),V[field]['Z'])*dxp)) #Diffusion part of chemical potential
					observables[field]['spinodal_energy_%s'%(phase)] = format(assemble(potential_derivative[field]*dxp)) #Spinodal part of chemical potential

					observables[field]['chi_%s'%(phase)] =  parameters['alpha']*observables[field]['spinodal_energy_%s'%(phase)] - parameters['D']*observables[field]['diffusion_energy_%s'%(phase)]


				files['hdf5'].write(fields[field],'/%s'%(field),n)
				files['xdmf'].write(fields[field],n)

				fields_n[field].assign(fields[field])
		
				self.set_data({field: array})
				self.set_observables({field: observables[field]})


				flag[field] = eps(n,field,fields[field],self.get_observables(),tol[field])


			n+=1



		for file in files:
			files[file].close()

		return

	def plot(self):

		parameters = self.get_parameters()
		data = self.get_data()
		observables = self.get_observables()
		paths = self.get_paths()



		plot = parameters.get('plot')

		if plot in [{},None,False]:
			return
		if not isinstance(plot,dict):
			plot = {}


		path = paths['plot']


		for key in parameters['fields']:

			plotting = parameters['fields'][key].get('plot')

			if plotting in [{},None,False]:
				continue

			plotting.update({k:plot[k] for k in plot if k not in plotting})
	
			self.get_logger()('\n'.join(['Plotting %s : %r'%(key,np.shape(data[key]))]))

			N = len(data[key])

			animate = plotting.get('animate')

			if animate:
				path = os.path.join(os.path.dirname(path),'plots_%s'%(key),os.path.basename(path))
				directory = os.path.dirname(path)
				if not os.path.exists(directory):
					os.makedirs(directory)



			with matplotlib.style.context(plotting.get('mplstyle',matplotlib.matplotlib_fname())):

				# Data plot
				fig,ax = plt.subplots()

				if not animate:
					Nrange = range(0,N,20) 
				else:
					Nrange = range(N)


				#Nrange = [150]


				x = 'x'
				v = ['x','t']
				y = key


				
				
				for i in Nrange:

					self.get_logger()('Plotting %s %s'%(key,str(i)))

					size = data[key][i].size
					
					plotprops = {
						'label':r'$t = {%s}_{}$'%(str(i)) if N>0 else None,
						'linewidth':3,			
						'color':'dimgrey'			
						}

					fillprops = [
						{'color':'tab:blue','alpha':0.8,'hatch':r''},
						{'color':'tab:blue','alpha':0.6,'hatch':r''},
					]

					legprops = {
						# 'title':r'\textrm{Time}',
						'ncol':1,'loc':(0.85,0.75),
						'framealpha':0,
						'handletextpad':-2.0, 'handlelength':0,
						}


					X = np.linspace(parameters['L_0'],parameters['L_1'],size)
					Y = data[key][i]


					ax.plot(X,Y,**plotprops)
					plt.fill_between(X[Y>0],0*X[Y>0],Y[Y>0],**fillprops[0])
					plt.fill_between(X[Y<0],0*X[Y<0],Y[Y<0],**fillprops[1])


					ax.set_xlabel(r'$%s_{}$'%(plotting['texify'].get(x,x)))
					ax.set_ylabel(r'${%s}_{}%s$'%(plotting['texify'].get(y,y),'(%s)'%(','.join([plotting['texify'].get(u,u) for u in v])) if len(v)>0 else ''))
					ax.set_xlim(-0.05,1.05)
					ax.set_ylim(-1.2,1.2)
					ax.set_xticks([0,0.25,0.5,0.75,1.0])
					ax.set_yticks([-1,-0.5,0,0.5,1])
					ax.grid(True,alpha=0.7)
					ax.annotate(r'$\large{\varphi}$',xy=(0.11,0.26))
					ax.annotate(r'$\large{\bar{\varphi}}$',xy=(0.6,-0.30))
					if N>0:
						ax.legend(**legprops)						
					if animate:
						file = path.replace('.','_data_%s_%s.'%(str(key),str(i)))
						fig.savefig(file,bbox_inches='tight')
						ax.clear()


				# if N>0 and not animate:
				

				if not animate:
					path = paths['plot'].replace('.','_data_%s.'%(str(key)))
					fig.savefig(path,bbox_inches='tight')

				# Observables plot
				for x,y in zip(plotting['x'],plotting['y']):
					fig,ax = plt.subplots()
					X = observables[key][x]
					Y = observables[key][y]
					ax.plot(X,Y)
					ax.set_xlabel(r'${%s}_{}$'%(plotting['texify'].get(x,x)))
					ax.set_ylabel(r'${%s}_{}$'%(plotting['texify'].get(y,y)))
					path = paths['plot'].replace('.','_observables_%s_%s_%s.'%(str(key),str(x),str(y)))
					fig.savefig(path,bbox_inches='tight')

		return


if __name__ == '__main__':

	if len(sys.argv) > 1: 
		parameters = sys.argv[1]
	else:
		print('usage: dns.py settings.prm')
		exit()

	model=dns()
	model.set_parameters(parameters)
	parameters = copy.deepcopy(model.get_parameters())
	
	#For random initial conditions
	#Set the number of Random Initial Conditions
	N = [i for i in range(0,100)]
	settings = {
		'a0':{'func':np.random.uniform,'args':[0.0,1.0],'kwargs':{},'value':None,'placement':[0,2]},
		'a1':{'func':np.random.uniform,'args':[-0.25,0.25],'kwargs':{},'value':None,'placement':[1]},
	}
	##For fixed initial conditions
	##Set the sample id
	#N = [1102] #Sample ID
	#settings = {
	#	'a0':{'func':lambda *args,**kwargs: 2.0,'args':[0.0,1.0],'kwargs':{},'value':None,'placement':[0,2]},
	#	'a1':{'func':lambda *args,**kwargs: 0.2,'args':[-0.25,0.25],'kwargs':{},'value':None,'placement':[1]},        	
	#}

	funcs = {'fields':{'c':{'initial':'%f /(1+exp( (x[0]-0.5 + %f)/ 0.2 )) - 0.5*%f'} }}

	for i in N:

		params = copy.deepcopy(parameters)

		updates = {}
		for setting in settings:
			settings[setting]['value'] = settings[setting]['func'](*settings[setting]['args'],**settings[setting]['kwargs'])


		values = {k:settings[setting]['value'] for setting in settings for k in settings[setting]['placement']}
		values = tuple([values[k] for k in range(len(values))])
		for func in funcs:
			if isinstance(funcs[func],dict):
				updates[func] = {}
				for k in funcs[func]:
					if isinstance(funcs[func][k],dict):
						updates[func][k] = {}
						for v in funcs[func][k]:
							if '%' in funcs[func][k][v]:	
								updates[func][k][v] = funcs[func][k][v]%(values)							

					else:
						if '%' in funcs[func][k]:	
							updates[func][k] = funcs[func][k]%(values)
			else:
				if '%' in funcs[func]:	
					updates[func] = funcs[func]%(values)

		updates.update({'path':os.path.join(params['path'],'Sample%d'%i)})
		for update in updates:
			if update not in params:
				params[update] = updates[update]
			elif isinstance(params[update],dict):
				for param in updates[update]:
					if isinstance(params[update].get(param),dict):
						params[update][param].update(updates[update][param])
					else:
						params[update].update({param: updates[update][param]})
			else:
				params.update({update:updates[update]})


		model.set_parameters(params)
		model.simulate()
		model.plot()	




====================================================================================================
examples\non_local_calculus\Example2_Allen_Cahn\estimate_derivatives.py
====================================================================================================
#!/usr/bin/env python
#
# Import python modules
import sys,glob,copy,itertools,os
import numpy as np
import pandas as pd

import matplotlib
import matplotlib.pyplot as plt


# Import user modules

# Global Variables
DELIMETER='__'
MAX_PROCESSES = 1
PARALLEL = 0

from mechanoChemML.src.graph_main import main


if __name__ == '__main__':



##############################################################
# Choose directories and data files


	cwd = '.'
	samples = 'dns/data/Sample%i'			
	file = 'data_c.csv'
	folder = 'ProcessDump'
	samples_ID = [i for i in range(0,100)]
	samples_ID = [0,]
	cwd = os.path.abspath(os.path.expanduser(cwd))
	directories_load = [samples%(i) for i in samples_ID]
	directories_dump = [os.path.join(samples%(i),folder) for i in samples_ID]

##############################################################
# Choose model parameters

	model_p = 1
	model_order = 2
	
##############################################################
#Temporary variable for calculation of derivative 
	_derivative_list = ['Phi_1P','Phi_2P','Phi_3P','Phi_4P','Phi_5P',
				'Phi_1M','Phi_2M','Phi_3M','Phi_4M','Phi_5M',
				'GradE_P','LapC_P',
				'LanE_P','dLan_P',	
				'GradE_M','LapC_M',
				'LanE_M','dLan_M',]
##############################################################
# Set settings
	


	settings = {
		#Set Paths
		'cwd': cwd,
		'directories_load':directories_load,
		'directories_dump':directories_dump,
		'data_filename':file,


			
		#Model settings
		'model_order':[model_order],
		'model_p':[model_p],			

		#Graph structure settings
		#'algebraic_operations': [[
		#	{'func':lambda df: df['partial__1__TE__Phi_1P__stencil'] + df['partial__1__Phi_1P__Time__stencil'], 'labels':'DiffVal'} , 
		#	]],
		
		'algebraic_operations': [[
			*[{'func':lambda df: df['Phi_%iP'%i] + df['Phi_%iM'%i], 'labels':'Phi_%i'%i}
			for i in range(6)],
			]], 	

		#Graph structure settings
		'differential_operations':[[

			{'function':'Phi_1P',
			'variable': ['Time'],
			'weight':['stencil'], 
			'adjacency':['backward_nearest'], #(symmetric finite difference like scheme),
			'manifold':[['Time']], #(or could specify 't' if the vertices are strictly defined by time and adjacency is defined by that, and not per say euclidean distance in 'u' space)
			'accuracy': [1],						
			'dimension':[0],				
			'order':1,
			'operation':['partial'],
			},
			*[{'function':'TE',
			'variable': [x],
			'weight':['stencil'], 
			'adjacency':['nearest'], #(symmetric finite difference like scheme),
			'manifold':[[x]], #(or could specify 't' if the vertices are strictly defined by time and adjacency is defined by that, and not per say euclidean distance in 'u' space)
			'accuracy': [2],		
			'dimension':[0],
			'order':1,
			'operation':['partial'],
			} for x in _derivative_list]					
			]]}
##############################################################
# Call main function
	
	main(settings = settings)
		

====================================================================================================
examples\non_local_calculus\Example2_Allen_Cahn\train_model.py
====================================================================================================
import sys, os
import pandas as pd 
import numpy as np
from mechanoChemML.workflows.systemID.systemID import systemID
np.set_printoptions(precision=5)

#Run as: python train_model config_allen_cahn.ini
if __name__ == "__main__":    
	
	print('======= NonLocal Calculus Example 2: Allen Cahn dynamics =======')
	
	# Choose directories and data files
	samples = './dns/data/Sample%i'			
	file = 'data.csv'
	folder = 'ProcessDump'
	samples_ID = [i for i in range(100)]
	directories_dump = [os.path.join(samples%(i),folder) for i in samples_ID]
	
	#Read data
	dataset = {'set%i'%i: pd.read_csv(directories_dump[i]+ '/'+ file) for i in range(len(samples_ID))}
	
	#Concatenate different datasets
	training_df = pd.concat([dataset['set%i'%i] for i in range(len(samples_ID))])
	
	print(training_df)
	#Residue data for dc/dt = \sum_i^N (coef_i *basis_i)
	###Extract important basis terms
	keys = [
		'B1',
		'B2',
		'B3',
		]
	for key in keys:
		lhs_basis = ['partial__1__Phi_1P__Time__stencil']
		_rhs_basis = [
				'Phi_1P','Phi_2P','Phi_3P','Phi_4P','Phi_5P',
				'Phi_1M','Phi_2M','Phi_3M','Phi_4M','Phi_5M',
				'GradE_P','LapC_P',
				'LanE_P','dLan_P',	
				'GradE_M','LapC_M',
				'LanE_M','dLan_M',					
				]
		rhs_basis = { 'B1': [*['partial__1__TE__'+_term+'__stencil' for _term in _rhs_basis],], 
			'B2': [*['partial__1__TE__'+_term+'__stencil' for _term in _rhs_basis], 
			*['partial__1__TE__'+_term+'__stencil' for _term in _rhs_basis]], 
			'B3': [*['partial__1__TE__'+_term+'__stencil' for _term in _rhs_basis], 
			*['partial__1__TE__'+_term+'__stencil' for _term in _rhs_basis],
			*[_term for _term in _rhs_basis]]
			}[key]
		training_data_lhs = training_df.filter(lhs_basis).to_numpy()
		training_data_rhs = training_df.filter(rhs_basis).to_numpy()

		data_mat = np.hstack((training_data_lhs,training_data_rhs)) 
		print('Num data points: %i'%data_mat.shape[0])
		print('Num basis: %i'%data_mat.shape[1])
		#Stepwise regression
		problem = systemID()
		problem.identifying(data_mat)

		#Output results
		print('System identification results:')
		prefactor=-problem.results['prefactor']

		active_basis = [i for i in range(len(rhs_basis)) if abs(prefactor[i])>1e-12]
		result_str = lhs_basis[0] + '=' + '+'.join([str(prefactor[i])+rhs_basis[i] for i in active_basis])
		print('Prefactors:',prefactor)
		print(result_str)
		gamma_matrix = problem.results['model'].gamma_matrix
		for i in range(gamma_matrix.shape[1]):
			print(' Gamma matrix at iterations %i: %s and loss:%f'%(i,gamma_matrix[:,i],problem.results['model'].loss[i]) )
		#Saving as csv
		gamma_matrix = gamma_matrix.transpose()
		_temp = np.abs(gamma_matrix) > 1e-14
		_temp = np.flip(np.argsort(_temp.sum(axis=0)))
		result_df = pd.DataFrame(gamma_matrix[:,_temp], columns =  [rhs_basis[i] for i in _temp])
		
		result_df.insert(0, 'loss', problem.results['model'].loss[:])
		result_df.insert(0, 'iter', np.arange(1,gamma_matrix.shape[0]+1))
		
		if not os.path.isdir('./result'):
			os.mkdir('./result')
		result_df.to_csv ('./result/model_'+key+ '.csv', index = False, header=True)



====================================================================================================
examples\pde_solver\Example1_diffusion_steady_state\main.py
====================================================================================================
from mechanoChemML.workflows.pde_solver.pde_system_diffusion_steady_state import WeakPDESteadyStateDiffusion as thisPDESystem

if __name__ == '__main__':
    problem = thisPDESystem()
    problem.run()


====================================================================================================
examples\pde_solver\Example2_linear_elasticity\main.py
====================================================================================================
from mechanoChemML.workflows.pde_solver.pde_system_elasticity_linear import WeakPDELinearElasticity as thisPDESystem

if __name__ == '__main__':
    problem = thisPDESystem()
    problem.run()


====================================================================================================
examples\pde_solver\Example3_nonlinear_elasticity\main.py
====================================================================================================
from mechanoChemML.workflows.pde_solver.pde_system_elasticity_nonlinear import WeakPDENonLinearElasticity as thisPDESystem

if __name__ == '__main__':
    problem = thisPDESystem()
    problem.run()


====================================================================================================
examples\systemID\Example1_pattern_forming\__init__.py
====================================================================================================


====================================================================================================
examples\systemID\Example1_pattern_forming\main.py
====================================================================================================
###################################
####### Required libraries: #######
#######        FEniCS       #######
#######      scikit-learn   #######
#######         numpy       #######
###################################
#Plaese use "python main.py config_Example1.ini" to run the problem with the configuration file.#

import sys
import numpy as np
from mechanoChemML.workflows.systemID.systemID import systemID
#from systemID import systemID
np.set_printoptions(precision=3)

print('start')
if __name__ == "__main__":    
    print('=====COMMENTS: SystemID Example 1: Pattern formation-Schnakenberg_model =======')
    ####### Require FEniCS: #######
    #generate data 
    # from mechanoChemML.workflows.systemID.forward_model import *
    #Schnakenberg_model()
    
    #construct operators
    # choose data to use
    # data_list=np.arange(50,60)
    # from mechanoChemML.workflows.systemID.generate_basis import *
    # Schnakenberg_basis(data_list=data_list)
    print('\n=====COMMENTS: In this example we constructed 13 operators in Eq. 19 and Eq.20 =======')

    ###################################

    used_time_step=[51]
    sigma=0
    data=np.zeros(0)
    for step in used_time_step:
        if np.size(data)<1:
            data=np.loadtxt('../datasets/Turing_system_basis/basis_sigma_'+str(sigma)+'_step_'+str(step)+'.dat')
        else:
            data=np.append(data,np.loadtxt('../datasets/Turing_system_basis/basis_sigma_'+str(sigma)+'_step_'+str(step)+'.dat'),0)   
    #read data to be identified
    
    # #
    # # #################
    print('\n=====COMMENTS:In default setting:\n the target is dC_1/dt, and the correct solution is dC_1/dt=0.05nabla^2C_1+0.1-1C_1+1C_1^2C_2=======')
    print('\n=====COMMENTS: \nabla^2C_1, constant(1), C_1 and C_1^2C_2 are the 1th, third, forth and sixth operators in the lists=======')
    print('\n=====COMMENTS:"next the System identification results will summarize all prefactors for all operators.')
    problem = systemID()
    problem.identifying(data)
    print('System identification results:')
    prefactor=problem.results['prefactor']
    print('Final result:',prefactor)
    print(' loss at each iterations',problem.results['model'].loss )
    print('\n=====COMMENTS:We correctly identify all operators and eliminate all non-active operators (with prefactor "0")=======')

    # #
    # #
    # #################
    print('\n=====COMMENTS: SystemID by stepwise regression by confirmation_of_consistency =======')
    problem.confirmation_of_consistency(data)
    #print('results of confirmation_of_consistency :\n',problem.results['prefactor'])
    cos_similiarity=np.triu(problem.results['cos_similiarity'])
    #
    index=np.where(np.abs(np.abs(cos_similiarity)-1+np.identity(cos_similiarity.shape[0]))<1.0e-5)
    print('consistent pairs :\n',list(zip(index[0], index[1])))
    print('\n=====COMMENTS: The above consistent operator pairs tell the two operators are consistently "shown" in the data')
    print('\nFrom the above list, we could identify two sets of equations in the data:')
    print('\nEquation 1 with operator 0,2,3,5,12 as they are consistent with each other ')
    print('\nEquation 2 with operator 7,8,11,13 as they are consistent with each other =======')





====================================================================================================
examples\systemID\Example2_soft_materials\__init__.py
====================================================================================================


====================================================================================================
examples\systemID\Example2_soft_materials\main.py
====================================================================================================
###################################
####### Required libraries: #######
#######        FEniCS       #######
#######      scikit-learn   #######
#######         numpy       #######
###################################

import sys
import numpy as np
from mechanoChemML.workflows.systemID.systemID import systemID
np.set_printoptions(precision=3)


if __name__ == "__main__":    
    print('======= SystemID Example 2: Constitutive modeling of soft materials =======')
    print('\n=====COMMENTS: This is the "simple version" of Exmaple in our paper: =======')
    print(' Z. Wang, J.B. Estrada, E.M. Arruda, K. Garikipati, Inference of deformation mechanisms and constitutive response of soft material surrogates of biological tissue by full-field characterization and data-driven variational system identification, Journal of the Mechanics and Physics of Solids, Volume 153, 2021. =======')

    #generate data 
    # from mechanoChemML.workflows.systemID.forward_model import *
    #threeField_neo_Hookean()
    
    #construct operators
    # from mechanoChemML.workflows.systemID.generate_basis import *
    # data_list=['extension','extension_2','bending','torsion']
    # threeField_neo_Hookean_basis(data_list=data_list)
    data_list=['bending']
    data=np.zeros(0)
    for shape in data_list:
      data=np.loadtxt('../datasets/soft_materials/'+shape+'.dat')
      # #
      # # #################
      # # print('\n======= SystemID by stepwise regression by specified_target =======')
      problem = systemID()
      problem.identifying(data)
      print('System identification results:')
      prefactor=-problem.results['prefactor']
      print('Final result:',prefactor)
      print(' loss at each iterations',problem.results['model'].loss )


====================================================================================================
mechanoChemML\__init__.py
====================================================================================================


====================================================================================================
mechanoChemML\src\__init__.py
====================================================================================================


====================================================================================================
mechanoChemML\src\dictionary.py
====================================================================================================
 #!/usr/bin/env python

# Import python modules
import os,sys,copy,warnings,itertools

warnings.simplefilter("ignore", (UserWarning,DeprecationWarning,FutureWarning))


def _copier(key,value,_copy):
	'''
	Copy value based on associated key 

	Args:
		key (string): key associated with value to be copied
		value (python object): data to be copied
		_copy (bool,dict,None): boolean or None whether to copy value, or dictionary with keys on whether to copy value
	Returns:
		Copy of value
	'''

	# Check if _copy is a dictionary and key is in _copy and is True to copy value
	if ((not _copy) or (isinstance(_copy,dict) and (not _copy.get(key)))):
		return value
	else:
		return copy.deepcopy(value)



def _clone(iterable,twin,_copy=False):
	'''
	Shallow in-place copy of iterable to twin

	Args:
		iterable (dict): dictionary to be copied
		twin (dict): dictionary to be modified in-place with copy of iterable
		_copy (bool,dict,None): boolean or None whether to copy value, or dictionary with keys on whether to copy value
	'''	

	# Iterate through iterable and copy values in-place to twin dictionary
	for key in iterable:
		if isinstance(iterable[key],dict):
			if twin.get(key) is None:
				twin[key] = {}
			_clone(iterable[key],twin[key],_copy)
		else:
			twin[key] = _copier(key,iterable[key],_copy)
	return




def _set(iterable,elements,value,_split=False,_copy=False,_reset=True):
	'''
	Set nested value in iterable with nested elements keys

	Args:
		iterable (dict): dictionary to be set in-place with value
		elements (str,list): DELIMITER separated string or list to nested keys of location to set value
		value (python object): data to be set in iterable
		_split (bool,str,None): boolean or None or delimiter on whether to split string elements into list of nested keys
		_copy (bool,dict,None): boolean or None whether to copy value, or dictionary with keys on whether to copy value
		_reset (bool): boolean on whether to replace value at key with value, or update the nested dictionary
	'''

	# Get copy of value in elements
	i = iterable
	e = 0
	value = _copier(elements,value,_copy)
	
	assert isinstance(iterable,dict), "Error - iterable is not dictionary"

	# Convert string instance of elements to list, splitting string based on _split delimiter
	if isinstance(elements,str) and _split:
		elements = elements.split(_split)

	# Boolean whether elements is a list, otherwise is python object that is explicit key in dictionary
	islist = isinstance(elements,list)

	# Update iterable with elements 
	if not islist:
		# elements is python object and iterable is to be updated at first level of nesting
		isdict = not _reset and isinstance(i.get(elements),dict) and isinstance(value,dict)
		if isdict:
			i[elements].update(value)
		else:
			i[elements] = value
	else:
		# elements is list of nested keys and the nested values are to be extracted from iterable and set with value
		try:
			while e<len(elements)-1:
				if i.get(elements[e]) is None:
					i[elements[e]] = {}
				i = i[elements[e]]
				e+=1
			isdict = not _reset and isinstance(i.get(elements[e]),dict) and isinstance(value,dict)
			if isdict:
				i[elements[e]].update(value)
			else:
				i[elements[e]] = value
		except:
			pass

	return

def _get(iterable,elements,default=None,_split=False,_copy=False):
	'''
	Get nested value in iterable with nested elements keys

	Args:
		iterable (dict): dictionary of values
		elements (str,list): DELIMITER separated string or list to nested keys of location to get value
		default (python object): default data to return if elements not in nested iterable
		_split (bool,str,None): boolean or None or delimiter on whether to split string elements into list of nested keys
		_copy (bool,dict,None): boolean or None whether to copy value, or dictionary with keys on whether to copy value

	Returns:
		Value at nested keys elements of iterable
	'''	


	i = iterable
	e = 0
	
	# Convert string instance of elements to list, splitting string based on _split delimiter
	if isinstance(elements,str) and _split:
		elements = elements.split(_split)

	# Get nested element if iterable, based on elements
	if not isinstance(elements,list):
		# elements is python object and value is to be got from iterable at first level of nesting
		try:
			return i[elements]
		except:
			return default
	else:
		# elements is list of nested keys and the nested values are to be extracted from iterable
		try:
			while e<len(elements):
				i = i[elements[e]]
				e+=1			
		except:
			return default

	return _copier(elements[e-1],i,_copy)

def _pop(iterable,elements,default=None,_split=False,_copy=False):
	'''
	Pop nested value in iterable with nested elements keys

	Args:
		iterable (dict): dictionary to be popped in-place
		elements (str,list): DELIMITER separated string or list to nested keys of location to pop value
		default (python object): default data to return if elements not in nested iterable
		_split (bool,str,None): boolean or None or delimiter on whether to split string elements into list of nested keys
		_copy (bool,dict,None): boolean or None whether to copy value, or dictionary with keys on whether to copy value

	Returns:
		Value at nested keys elements of iterable
	'''		
	
	i = iterable
	e = 0

	# Convert string instance of elements to list, splitting string based on _split delimiter	
	if isinstance(elements,str) and _split:
		elements = elements.split(_split)

	if not isinstance(elements,list):
		# elements is python object and value is to be got from iterable at first level of nesting		
		try:
			return i.pop(elements)
		except:
			return default
	else:
		# elements is list of nested keys and the nested values are to be extracted from iterable		
		try:
			while e<(len(elements)-1):
				i = i[elements[e]]
				e+=1			
		except:
			return default

	return _copier(e,i.pop(elements[e],default),_copy)

def _has(iterable,elements,_split=False):
	'''
	Check if nested iterable has nested elements keys

	Args:
		iterable (dict): dictionary to be searched
		elements (str,list): DELIMITER separated string or list to nested keys of location to set value
		_split (bool,str,None): boolean or None or delimiter on whether to split string elements into list of nested keys

	Returns:
		Boolean value if nested keys elements are in iterable
	'''		

	i = iterable
	e = 0

	# Convert string instance of elements to list, splitting string based on _split delimiter	
	if isinstance(elements,str) and _split:
		elements = elements.split(_split)
	try:
		if not isinstance(elements,list):
			# elements is python object and value is to be got from iterable at first level of nesting				
			i = i[element]
		else:
			# elements is list of nested keys and the nested values are to be extracted from iterable		
			while e<len(elements):
				i = i[elements[e]]
				e+=1			
		return True
	except:
		return False

def _update(iterable,elements,_copy=False,_clear=True,_func=None):
	'''
	Update nested iterable with elements

	Args:
		iterable (dict): dictionary to be updated in-place
		elements (dict): dictionary of nested values to update iterable
		_copy (bool,dict,None): boolean or None whether to copy value, or dictionary with keys on whether to copy value
		_clear (bool): boolean of whether to clear iterable when the element's value is an empty dictionary
		_func(callable,None): Callable function that accepts key,iterable,elements arguments to modify value to be updated based on the given dictionaries
	'''		

	# Setup _func as callable
	if not callable(_func):
		_func = lambda key,iterable,elements: elements[key]

	# Clear iterable if _clear and elements is empty dictionary
	if _clear and elements == {}:
		iterable.clear()

	if not isinstance(elements,(dict)):
		# elements is python object and iterable is directly set as elements
		iterable = elements
		return

	# Recursively update iterable with elements
	for e in elements:
		if isinstance(iterable.get(e),dict):
			if e not in iterable:
				iterable.update({e: _copier(e,_func(e,iterable,elements),_copy)})
			else:
				_update(iterable[e],elements[e],_copy=_copy,_clear=_clear,_func=_func)
		else:
			iterable.update({e:_copier(e,_func(e,iterable,elements),_copy)})
	return

def _permute(dictionary,_copy=False,_groups=None,_ordered=True):
	'''
	Get all combinations of values of dictionary of lists

	Args:
		dictionary (dict): dictionary of keys with lists of values to be combined in all combinations across lists
		_copy (bool,dict,None): boolean or None whether to copy value, or dictionary with keys on whether to copy value
		_groups (list,None): List of lists of groups of keys that should not have their values permuted in all combinations, but should be combined in sequence element wise. For example groups = [[key0,key1]], where dictionary[key0] = [value_00,value_01,value_02],dictionary[key1] = [value_10,value_11,value_12], then the permuted dictionary will have key0 and key1 keys with only pairwise values of [{key0:value_00,key1:value_10},{key0:value_01,key1:value_11},{key0:value_02,key1:value_12}].
		_ordered (bool): Boolean on whether to return dictionaries with same ordering of keys as dictionary

	Returns:
		List of dictionaries with all combinations of lists of values in dictionary
	'''		
	def indexer(keys,values,_groups):
		'''
		Get lists of values for each group of keys in _groups
		'''
		_groups = copy.deepcopy(_groups)
		if _groups is not None:
			inds = [[keys.index(k) for k in g if k in keys] for g in _groups]
		else:
			inds = []
			_groups = []
		N = len(_groups)
		_groups.extend([[k] for k in keys if all([k not in g for g in _groups])])
		inds.extend([[keys.index(k) for k in g if k in keys] for g in _groups[N:]])
		values = [[values[j] for j in i ] for i in inds]
		return _groups,values

	def zipper(keys,values,_copy): 
		'''
		Get list of dictionaries with keys, based on list of lists in values, retaining ordering in case of grouped values
		'''
		return [{k:_copier(k,u,_copy) for k,u in zip(keys,v)} for v in zip(*values)]

	def unzipper(dictionary):
		'''
		Zip keys of dictionary of list, and values of dictionary as list
		'''
		keys, values = zip(*dictionary.items())	
		return keys,values

	def permuter(dictionaries): 
		'''
		Get all list of dictionaries of all permutations of sub-dictionaries
		'''
		return [{k:d[k] for d in dicts for k in d} for dicts in itertools.product(*dictionaries)]

	def nester(keys,values):
		'''
		Get values of permuted nested dictionaries in values.
		Recurse permute until values are lists and not dictionaries.
		'''
		keys,values = list(keys),list(values)
		for i,(key,value) in enumerate(zip(keys,values)):
			if isinstance(value,dict):
				if isinstance(_groups,dict):
					_group = _groups.get(key,_group)
				else:
					_group = _groups
				values[i] = _permute(value,_copy=_copy,_groups=_group)    
		return keys,values


	if dictionary in [None,{}]:
		return [{}]

	# Get list of all keys from dictionary, and list of lists of values for each key
	keys,values = unzipper(dictionary)


	# Get values of permuted nested dictionaries in values
	keys,values = nester(keys,values)

	# Retain ordering of keys in dictionary
	keys_ordered = keys
	
	# Get groups of keys based on _groups and get lists of values for each group
	keys,values = indexer(keys,values,_groups)

	# Zip keys with lists of lists in values into list of dictionaries
	dictionaries = [zipper(k,v,_copy) for k,v in zip(keys,values)]


	# Get all permutations of list of dictionaries into one list of dictionaries with all keys
	dictionaries = permuter(dictionaries)


	# Retain original ordering of keys if _ordered is True
	if _ordered:
		for i,d in enumerate(dictionaries):
			dictionaries[i] = {k: dictionaries[i][k] for k in keys_ordered}    
	return dictionaries



def _find(iterable,key):
	'''
	Find and yield key in nested iterable

	Args:
		iterable (dict): dictionary to search
		key (python object): key to find in iterable dictionary

	Yields:
		Found values with key in iterable
	'''	

	# Recursively find and yield value associated with key in iterable		
	try:
		if not isinstance(iterable,dict):
			raise
		for k in iterable:
			# print(k)
			if k == key:
				yield iterable[k]
			for v in _find(iterable[k],key):
				yield v
	except:
		pass
	return
				
def _replace(iterable,key,replacement,_append=False,_copy=True,_values=False):
	'''
	Find and replace key in-place in iterable with replacement key

	Args:
		iterable (dict): dictionary to be searched
		key (python object): key to be replaced with replacement key
		replacement (python object): dictionary key to replace key
		_append (bool): boolean on  whether to append replacement key to dictionary with value associated with key
		_copy (bool,dict,None): boolean or None whether to copy value, or dictionary with keys on whether to copy value
		_values (bool): boolean of whether to replace any values that equal key with replacement in the iterable 
	'''	

	# Recursively find where nested iterable keys exist, and replace or append in-place with replacement key
	try:
		keys = list(iterable)
		for k in keys:
			if k == key:
				if _append:
					iterable[replacement] = _copier(replacement,iterable.get(key),_copy)
					k = replacement
				else:
					iterable[replacement] = _copier(replacement,iterable.pop(key),_copy)
					k = replacement   
			if _values and iterable[k] == key:
				iterable[k] = _copier(k,replacement,_copy)
			_replace(iterable[k],key,replacement,_append=_append,_copy=_copy,_values=_values)
	except Exception as e:
		pass
	return



def _formatstring(key,iterable,elements,*args,**kwargs):

	'''
	Format values in iterable based on key and elements

	Args:
		key (python object): key to index iterable for formatting
		iterable (dict): dictionary with values to be formatted
		elements (dict): dictionary of elements to format iterable values

	Returns:
		Formatted value based on key,iterable, and elements
	'''	


	# Get value associated with key for iterable and elements dictionaries
	try:
		i = iterable[key]
	except:
		i = None
	e = elements[key]
	n = 0
	m = 0


	# Return elements[key] if kwargs[key] not passed to function, or elements[key] is not a type to be formatted
	if key not in kwargs or not isinstance(e,(str,tuple,list)):
		return e

	# Check for different cases of types of iterable[key] and elements[key] to be formatted

	# If iterable[key] is not a string, or iterable tuple or list, return value based on elements[key]
	if not isinstance(i,(str,tuple,list)):

		# If elements[key] is a string, string format elements[key] with args and kwargs and return the formatted value
		if isinstance(e,str):
			m = e.count('%')
			if m == 0:
				return e
			else:
				return e%(tuple((*args,*kwargs[key]))[:m])

		# If elements[key] is an iterable tuple or list, string format each element of elements[key] with args and kwargs and return the formatted value as a tuple
		elif isinstance(e,(tuple,list)):
			m = 0
			e = [x for x in e]
			c = [j for j,x in enumerate(e) if isinstance(x,str) and x.count('%')>0]
			for j,x in enumerate(e):
				if not isinstance(x,str):
					continue
				m = x.count('%')
				if m > 0:
					_j = c.index(j)
					e[j] = x%(tuple((*args,*kwargs[key]))[_j:m+_j])
			e = tuple(x for x in e)
			return e

		# If elements[key] is other python object, return elements[key]
		else:
			return e

	# If iterable[key] is a string, format iterable[key] based on elements[key]
	elif isinstance(i,str):

		# Get number of formatting elements in iterable[key] string to be formatted
		n = i.count('%')
		if n == 0:
			# If iterable[key] has no formatting elements, return based on elements[key]

			# If elements[key] is a string, string format elements[key] with args and kwargs and return the formatted value
			if isinstance(e,str):
				m = e.count('%')
				if m == 0:
					return e
				else:
					return e%(tuple((i,*args,*kwargs[key]))[:m])

			# If elements[key] is an iterable tuple or list, string format each element of elements[key] with args and kwargs and return the formatted value as a tuple
			elif isinstance(e,(tuple,list)):
				m = 0
				e = [x for x in e]
				c = [j for j,x in enumerate(e) if isinstance(x,str) and x.count('%')>0]	
				for j,x in enumerate(e):
					if not isinstance(x,str):
						continue
					m = x.count('%')
					if m > 0:
						_j = c.index(j)
						if isinstance(i,str):
							e[j] = x%(tuple((i,*args,*kwargs[key]))[_j:m+_j])
						else:
							e[j] = x%(tuple((*i,*args,*kwargs[key]))[_j:m+_j])										
				e = tuple(x for x in e)
				return e

			# If elements[key] is other python object, return elements[key]
			else:
				return e
		# If iterable[key] string has non-zero formatting elements, format iterable[key] string with elements[key], args, and kwargs
		else:
			if isinstance(e,str):
				return i%(tuple((e,*args,*kwargs[key]))[:n])
			elif isinstance(e,(tuple,list)):
				return i%(tuple((*e,*args,*kwargs[key]))[:n])
			else:
				return e

	# If iterable[key] is an iterable tuple or list, string format each element of iterable[key] with elements[key],args and kwargs and return the formatted value as a tuple
	elif isinstance(i,(tuple,list)):
		i = [str(x) for x in i]
		n = 0
		c = [j for j,x in enumerate(i) if isinstance(x,str) and x.count('%')>0]	
		for j,x in enumerate(i):
			n = x.count('%')
			if n > 0:
				_j = c.index(j)				
				if isinstance(e,str):
					i[j] = x%(tuple((e,*args,*kwargs[key]))[_j:n+_j])
				else:
					i[j] = x%(tuple((*e,*args,*kwargs[key]))[_j:n+_j])										

		if n == 0:
			if isinstance(e,str):
				m = e.count('%')
				if m == 0:
					return e
				else:
					return e%(tuple((i,*args,*kwargs[key]))[:m])
			elif isinstance(e,(tuple,list)):
				m = 0
				e = [x for x in e]
				c = [j for j,x in enumerate(e) if isinstance(x,str) and x.count('%')>0]					
				for j,x in enumerate(e):
					if not isinstance(x,str):
						continue
					m = x.count('%')
					if m > 0:
						_j = c.index(j)				
						if isinstance(i,str):
							e[j] = x%(tuple((i,*args,*kwargs[key]))[_j:m+_j])
						else:
							e[j] = x%(tuple((*i,*args,*kwargs[key]))[_j:m+_j])										
				e = tuple(x for x in e)
				return e
			else:
				return e			
			return e
		else:
			i = tuple(x for x in i)
			return i
	else:
		return e

====================================================================================================
mechanoChemML\src\estimator.py
====================================================================================================
#!/usr/bin/env python


# Import python modules
import os,sys,copy,itertools,functools,inspect,timeit
import gc

import numpy as np
import scipy as sp
import scipy.stats,scipy.signal,scipy.cluster
import matplotlib as plt


from sklearn import linear_model,model_selection

# import multiprocess as mp
# import multithreading as mt
import joblib
import multiprocessing as multiprocessing
import multiprocessing.dummy as multithreading


# Logging
import logging
log = 'info'
logger = logging.getLogger(__name__)
#logger.setLevel(getattr(logging,log.upper()))


# Global Variables
DELIMITER='__'

# Import user modules
from .graph_utilities import set_loss,set_score,set_norm,set_scale,set_criteria
from .graph_utilities import Pool,nullPool,Parallelize,Parallel,nullParallel,nullcontext,nullfunc,wrapper
from .graph_utilities import delete,take,isin,rank,cond,gram,project
from .graph_utilities import call,shuffle,where,getattribute,setattribute
from .texify import scinotation
#from .plot import plot


class Base(object):
	__slots__ = ('_param_names')	
	def __init__(self, *args, **kwargs):
		self.set_params(*args,**kwargs)
		return

	def get_params(self,*args,deep=False,**kwargs):
		field = '_param_names'
		default = {}
		if not hasattr(self,field):
			getattr(self,'_set%s%s'%('_' if not field.startswith('_') else '',field))(default,*args,**kwargs)			
			self.set_params()
		return {k: getattr(self,k,None) for k in getattr(self,field)}	

	def set_params(self,*args,**kwargs):
		field = '_param_names'
		default = {}
		if not hasattr(self,field):
			getattr(self,'_set%s%s'%('_' if not field.startswith('_') else '',field))(default,*args,**kwargs)			
		
		params = getattr(self,field)
		params.update(kwargs)
		
		for param in params:
			value = params[param]			
			try:
				getattr(self,'set_%s'%param)(*args,**params)
			except (AttributeError,TypeError) as e:
				pass
			setattr(self,param,value)
			if param not in getattr(self,field):
				getattr(self,field)[param] = value
		return self	

	def _set_param_names(self,param_names,*args,**kwargs):
		field = '_param_names'
		param_names.update(kwargs)
		setattr(self,field,param_names)
		return

	def _get_param_names(self):
		field = '_param_names'
		default = {}
		if not hasattr(self,field):
			getattr(self,'_set%s%s'%('_' if not field.startswith('_') else '',field))(default,*args,**kwargs)						
		return getattr(self,field)	


class Estimator(Base):
	__slots__ = tuple(('%s%s'%(s,name) for name in 
			['loss_func','score_func','criteria_func',
			 'normalize','fit_intercept','parallel',
			 'stats']
			for s in ['','_']))	

	def __init__(self,*args,**kwargs):
		_kwargs = {
			'solver':'pinv',
			'loss_func':None,
			'score_func':None,
			'criteria_func':None,
			'normalize':None,
			'fit_intercept':False,
			'parallel':None,
			'n_jobs':1,
			'backend':'loky',
			'stats':None,
			'collect':True,
			'prioritize':False,
			'verbose':False,
		}
		kwargs.update({**_kwargs,**kwargs})
		call(super(),'__init__',*args,**kwargs)        


	def set_stats(self,stats,*args,**kwargs):
		field = '_stats'
		default = {}
		if hasattr(self,field):
			if stats is not None:
				getattr(self,field).update(stats)
			else:
				getattr(self,field).update(default)
		else:            
			if stats is not None:
				setattr(self,field,stats)
			else:      
				setattr(self,field,default)
				self.statistics(X=None,y=None,stats=getattr(self,field),index=None,append=True)  
		return


	def get_stats(self,*args,**kwargs):
		field = '_stats'
		default = None
		if not hasattr(self,field):
			getattr(self,'set%s%s'%('_' if not field.startswith('_') else '',field))(default,*args,**kwargs)
		return getattr(self,field)


	def statistics(self,X,y,stats,index,append,*args,**kwargs):

		def structure(obj):
			_structure = np.array
			if obj is None:
				return obj
			if not isinstance(obj,np.ndarray):
				obj = np.array(obj)
			if obj.ndim < 1:
				obj = obj.reshape(-1)
			return obj
		def join(key,obj,objs,index=None,append=True):
			obj = structure(obj)
			if key not in objs:
				objs[key] = None
			if index is not None:
				try:
					objs[key][index] = obj
				except:
					try:
						if obj.size > 1:
							objs[key] = obj.reshape((1,*obj.shape))
						else:
							raise Exception
					except:
						objs[key] = obj
			elif append:
				try:
					objs[key] = np.concatenate((objs[key],obj))
				except:
					try:
						if obj.size > 1:
							objs[key] = obj.reshape(1,*obj.shape)
						else:
							raise Exception
					except:
						objs[key] = obj
			elif obj is None:
				objs[key] = obj
			return
		fields = {'stats_modules': ['predict','loss','score','coef_','criteria'],
				  'stats_params':  ['index_','index','dim_','rank_','condition_','basis_','complexity_','coefficient'],
				  'stats_collect':['predict']}
		for field in fields:
			if not hasattr(self,field):
				setattr(self,field,fields[field])
			else:
				pass
		if stats is None:
			stats = self.get_stats()

		isXy = X is None and y is None
		for key in self.stats_modules:   
			try:
				value = kwargs.get(key,getattr(self,key))
			except:
				continue
			if key in kwargs:
				pass
			elif callable(value) and not isXy: 
				try:
					value = value(X,y,*args,**kwargs)
				except:
					value = None      
			else:
				value = None
			join(key,value,stats,index=index,append=append)

		for key in self.stats_params:             
			try:
				value = kwargs.get(key,self.get_stats().get(key)[index])
			except:
				value = None
			join(key,value,stats,index=index,append=append)

		return    

	def _set_estimator(self,estimator,*args,**kwargs):
		default = 'OLS'
		estimators = {
					 'CrossValidate':CrossValidate,
					 'Stepwise':Stepwise,
					 'OLS':OLS,
					 'Tikhonov':Tikhonov,
					 'Ridge':Ridge,
					 'Lasso':Lasso,
					 'Enet':Enet,
					 'TikhonovCV':TikhonovCV,					 
					 'RidgeCV':RidgeCV,
					 'LassoCV':LassoCV,
					 'EnetCV':EnetCV,
					}
		if isinstance(estimator,str):
			variables = {'args':args,'kwargs':kwargs}
			for variable in variables:
				_variable = 'estimator_%s'%(variable)
				if kwargs.get(_variable) is not None:
					if variable == 'args':
						variables[variable] = kwargs[_variable]
					elif variable == 'kwargs':
						variables[variable].update(kwargs[_variable])
			estimator = call(None,estimators.get(estimator,estimators[default]),*variables['args'],**variables['kwargs'])
		return estimator

	def set_estimator(self,estimator,*args,**kwargs):
		field = '_estimator'
		estimator = getattr(self,'_set%s%s'%('_' if not field.startswith('_') else '',field))(estimator,*args,**kwargs)
		setattr(self,field,estimator)
		return

	def get_estimator(self,*args,**kwargs):
		field = '_estimator'
		default = None
		if not hasattr(self,field):
			getattr(self,'set%s%s'%('_' if not field.startswith('_') else '',field))(default,*args,**kwargs)
		return getattr(self,field)

	def set_loss_func(self,loss_func,*args,**kwargs):
		field = '_loss_func'
		if callable(loss_func):
			setattr(self,field,loss_func)
		else:
			if isinstance(loss_func,str):
				kwds = {**kwargs,'loss_func':loss_func,'axis':0}
			else:
				kwds = {**kwargs,'loss_func':'rmse','axis':0}
			setattr(self,field,set_loss(**kwds))
		return

	def get_loss_func(self,*args,**kwargs):
		field = '_loss_func'
		default = None
		if not hasattr(self,field):
			getattr(self,'set%s%s'%('_' if not field.startswith('_') else '',field))(default,*args,**kwargs)
		return getattr(self,field) 

	def set_score_func(self,score_func,*args,**kwargs):
		field = '_score_func'
		if callable(score_func):
			setattr(self,field,score_func)            
		else:
			if isinstance(score_func,str):
				kwds = {**kwargs,'score_func':score_func,'axis':0}
			else:
				kwds = {**kwargs,'score_func':'rmse','axis':0}
			setattr(self,field,set_score(**kwds))
		return

	def get_score_func(self,*args,**kwargs):
		field = '_score_func'
		default = None
		if not hasattr(self,field):
			getattr(self,'set%s%s'%('_' if not field.startswith('_') else '',field))(default,*args,**kwargs)
		return getattr(self,field)     


	def set_criteria_func(self,criteria_func,*args,**kwargs):
		field = '_criteria_func'
		if callable(criteria_func):
			setattr(self,field,criteria_func)
		else:
			if isinstance(criteria_func,str):
				kwds = {**kwargs,'criteria_func':criteria_func}
			else:
				kwds = {**kwargs,'criteria_func':'F_test'}
			setattr(self,field,set_criteria(**kwds))
		return

	def get_criteria_func(self,*args,**kwargs):
		field = '_criteria_func'
		default = None
		if not hasattr(self,field):
			getattr(self,'set%s%s'%('_' if not field.startswith('_') else '',field))(default,*args,**kwargs)
		return getattr(self,field) 


	def set_normalize(self,normalize,*args,**kwargs):
		field = '_normalize'
		if callable(normalize):
			setattr(self,field,normalize)
		else:
			if isinstance(normalize,str):
				kwds = {**kwargs,'norm_func':normalize,'axis':0}
			else:
				kwds = {**kwargs,'norm_func':'l2','axis':0}
			setattr(self,field,set_scale(**kwds))
		return

	def get_normalize(self,*args,**kwargs):
		field = '_normalize'
		default = None
		if not hasattr(self,field):
			getattr(self,'set%s%s'%('_' if not field.startswith('_') else '',field))(default,*args,**kwargs)
		return getattr(self,field) 


	def set_solver(self,solver,*args,**kwargs):
		field = '_solver'
		setattr(self,field,Solver(solver))
		return

	def get_solver(self,*args,**kwargs):
		field = '_solver'		
		return getattr(self,field)


	def set_coef_(self,coef_,*args,**kwargs):
		field = 'coef_'
		if coef_ is None and hasattr(self.get_estimator(),'get%s%s'%('_' if not field.startswith('_') else '',field)):
			coef_ = getattr(self.get_estimator(),'get%s%s'%('_' if not field.startswith('_') else '',field))()
		if coef_ is None and hasattr(self.get_estimator(),field):
			coef_ = getattr(self.get_estimator(),field)

		setattr(self,field,coef_)

		if self.get_estimator():
			if hasattr(self.get_estimator(),'set%s%s'%('_' if not field.startswith('_') else '',field)):
				getattr(self.get_estimator(),'set%s%s'%('_' if not field.startswith('_') else '',field))(coef_)
			else:
				setattr(self.get_estimator(),field,coef_)

		return


	def get_coef_(self,*args,**kwargs):
		field = 'coef_'
		default = None
		if not hasattr(self,field):
			getattr(self,'set%s%s'%('_' if not field.startswith('_') else '',field))(default,*args,**kwargs)            
		return getattr(self,field) 


	def set_intercept_(self,X,y,*args,**kwargs):
		field = 'intercept_'
		default = 0.0
		if not self.fit_intercept:
			setattr(self,field,default)
		else:
			setattr(self,field,y.mean(axis=0) - (X.mean(axis=0)).dot(self.get_coef_()))

	def get_intercept_(self,*args,**kwargs):
		field = 'intercept_'
		default = None
		if not hasattr(self,field):
			getattr(self,'set%s%s'%('_' if not field.startswith('_') else '',field))(default,*args,**kwargs)            
		return getattr(self,field) 

	def set_predict(self,predict,*args,**kwargs):
		field = '_predict'
		setattr(self,field,predict)
		return

	def get_predict(self,*args,**kwargs):
		field = '_predict'
		default = None
		if not hasattr(self,field):
			getattr(self,'set%s%s'%('_' if not field.startswith('_') else '',field))(default,*args,**kwargs)            
		return getattr(self,field) 	


	def loss(self,X,y,*args,**kwargs):
		if self.prioritize:
			kwargs.update(self.get_params())				
		y_pred = self.predict(X,y,**kwargs)        
		return self.get_loss_func()(y_pred,y,*args,**kwargs)

	def score(self,X,y,*args,**kwargs):
		if self.prioritize:
			kwargs.update(self.get_params())				
		y_pred = self.predict(X,y,**kwargs)        
		return self.get_score_func()(y_pred,y,*args,**kwargs)    

	def criteria(self,X,y,*args,**kwargs):
		if self.prioritize:
			kwargs.update(self.get_params())				
		keys =  ['loss','complexity_','losses','complexities_']
		for k in keys:
			if k in kwargs:
				pass
			elif k in ['loss']:
				kwargs[k] = self.loss(X,y,*args,**kwargs)
			elif k in ['losses']:
				kwargs[k] = self.get_stats()['loss']
			elif k in ['complexity_']:
				kwargs[k] = getattr(self,k,self.get_coef_().size)
			elif k in ['complexities_']:
				kwargs[k] = self.get_stats()['complexity_']
		return self.get_criteria_func()(**kwargs)

	def normalized(self,X,y,*args,**kwargs):
		if self.prioritize:
			kwargs.update(self.get_params())				
		normalize_ = self.get_normalize()
		scale_X = normalize_(X)
		scale_y = normalize_(y)
		self.fit(X,y,*args,**kwargs)
		X /= scale_X
		y /= scale_y
		coef_ = self.get_coef()
		coef_ /= scale_X
		coef_ *= scale_y
		self.set_coef_(coef_)
		return self

	def plot(self,X,y,*args,**kwargs):
		self.fit(X,y,*args,**kwargs)
		
		parameters = {}
		parameters['name'] = r'\textrm{%s}'%(str(self.__class__.__name__).replace('_',r'\_')) #split('.')[-1].replace("'>",''))
		parameters['path'] = 'plot.pdf'        
		parameters['loss']  = self.loss(X,y,*args,**kwargs)       
		
		parameters['fig'] = kwargs.get('fig')
		parameters['axes'] = kwargs.get('axes')
		
		parameters['fit__y'] = y
		parameters['fit__x'] = np.arange(parameters['fit__y'].shape[0])
		parameters['fit__y_pred'] = self.predict(X,y,*args,**kwargs)
		parameters['fit__x_pred'] = np.arange(parameters['fit__y_pred'].shape[0])
		parameters['fit__label'] = r'$y_{}$'
		parameters['fit__label_pred'] = r'$y_{%s} - %s$'%(parameters['name'],scinotation(parameters['loss']))
		parameters['settings'] = {
			'fit':{
				'fig':{
					'set_size_inches':{'h':10,'w':15},
					'tight_layout':{},
					'savefig':{'fname':parameters['path'],'bbox_inches':'tight'},
				},
				'ax':{
					'plot':[
						*[{
						 'x':parameters['fit__x'],
						 'y':parameters['fit__y'],
						 'label':parameters['fit__label'],                         
						 'linestyle':'--',
						 'alpha':0.7,
						  } if parameters['fig'] is None else []],
						*[{
						 'x':parameters['fit__x_pred'],
						 'y':parameters['fit__y_pred'],
						 'label':parameters['fit__label_pred'],
						 'linestyle':'-',
						 'alpha':0.6,
						  }]
						],
					'set_xlabel':{'xlabel':r'$x_{}$'},
					'set_ylabel':{'ylabel':r'$y_{}$'},
					'legend':{'loc':'best','prop':{'size':15}}
					},
				'style':{
					'layout':{'nrows':1,'ncols':2,'index':1},
				},
			},                       
		}
		fig,axes = plot(settings=parameters['settings'],fig=parameters['fig'],axes=parameters['axes'])
		return fig,axes  

	
class KFolder(object):
	def __init__(self,n_splits,random_state,test_size,**kwargs):
		call(super(),'__init__')        
		self.n_splits = n_splits
		self.random_state = random_state
		self.test_size = test_size
		for k in kwargs:
			setattr(self,k,kwargs[k])
		self.set_Folder()

	def split(self,X,y,groups=None):		
		self.set_Folder(groups=groups)
		if groups is None:
			splits = self.get_Folder().split(X,y)
		else:
			splits = self.get_Folder().split(X,y,groups=groups)
		for train,test in splits:
			yield train,test

	def get_n_splits(self,*args,**kwargs):
		return self.n_splits

	def set_Folder(self,groups=None):
		if groups is None:
			self.Folder = model_selection.ShuffleSplit(n_splits=self.n_splits,
									   test_size=self.test_size,
									   random_state=self.random_state)
		else:
			self.Folder = model_selection.GroupShuffleSplit(n_splits=self.n_splits,
											test_size=self.test_size,
											random_state=self.random_state)
		return

	def get_Folder(self,*args,**kwargs):
		field = 'Folder'
		if not hasattr(self,field):
			getattr(self,'set%s%s'%('_' if not field.startswith('_') else '',field))(*args,**kwargs)
		return getattr(self,field)



class UniformFolder(object):
	def __init__(self,n_splits,random_state,test_size,**kwargs):
		self.n_splits = n_splits
		self.random_state = random_state
		self.test_size = test_size
		for k in kwargs:
			setattr(self,k,kwargs[k])

	def split(self,X,y,groups=None):		
		splits = ((slice(None),slice(None)),)*self.get_n_splits()
		for train,test in splits:
			yield train,test

	def get_n_splits(self,*args,**kwargs):
		return self.n_splits


class GridSearchCV(Base):
	def __init__(self,estimator,param_grid,cv,*args,**kwargs):
		_kwargs = {
			'estimator':estimator,
			'param_grid':param_grid,
			'cv':cv,
			'n_jobs':1,
			'backend':'loky',			
			'prefer':None,
			'parallel':None,
			'scoring':'score',
			'refit':True,
		}
		kwargs.update({**_kwargs,**kwargs})
		call(super(),'__init__',*args,**kwargs)       
		return


	def _fit(self,X,y,train,test,scoring,params,*args,**kwargs):
		cls = self.get_estimator().__class__
		params = {**self.get_estimator().get_params(),**kwargs,**params}
		estimator = cls(*args,**params)
		estimator.fit(X[train],y[train],*args,**kwargs)
		value = getattr(estimator,scoring)(X[test],y[test],*args,**kwargs) 
		del estimator
		return value


	def fit(self,X,y,*args,**kwargs):

		parallelize = Parallelize(self.n_jobs,self.backend,prefer=self.prefer,parallel=self.parallel,verbose=self.verbose)

		func = self._fit
		scoring = 'score'
		param_grid = self.get_param_grid()
		cv = self.get_cv()

		arguments = []
		keywords = {}

		arguments.extend(args)
		keywords.update(kwargs)
		keywords.update({
			'X':X,'y':y,'scoring':scoring,
			})

		iterables = ['train','test','params']		
		iterable = (dict(zip(iterables,(train,test,params)))					                     
					for params in param_grid
					for train,test in cv.split(X,y,*args,**kwargs))
		
		values = []

		parallelize(func,iterable,values,*arguments,**keywords)

		self.set_cv_results_(values)
		if self.refit:
			self.get_best_estimator_().fit(X,y,*args,**kwargs)

		return self


	# def predict(self,X,y=None,**kwargs):
	# 	if self.prioritize:
	# 		kwargs.update(self.get_params())			
	# 	try:
	# 		return self.get_estimator().predict(X,**kwargs)
	# 	except:
	# 		self.fit(X,y,**kwargs)
	# 		return self.get_estimator().predict(X,**kwargs)

	# def loss(self,X,y=None,**kwargs):
	# 	if self.prioritize:
	# 		kwargs.update(self.get_params())			
	# 	try:
	# 		return self.get_estimator().loss(X,y,*args,**kwargs)
	# 	except:
	# 		self.fit(X,y,**kwargs)
	# 		return self.get_estimator().loss(X,y,*args,**kwargs)

	# def score(self,X,y=None,**kwargs):
	# 	if self.prioritize:
	# 		kwargs.update(self.get_params())			
	# 	try:
	# 		return self.get_estimator().score(X,y,*args,**kwargs)
	# 	except:
	# 		self.fit(X,y,**kwargs)
	# 		return self.get_estimator().score(X,y,*args,**kwargs)

	# def criteria(self,X,y=None,**kwargs):
	# 	if self.prioritize:
	# 		kwargs.update(self.get_params())			
	# 	try:
	# 		return self.get_estimator().criteria(X,y,*args,**kwargs)
	# 	except:
	# 		self.fit(X,y,**kwargs)
	# 		return self.get_estimator().criteria(X,y,*args,**kwargs)


	def set_estimator(self,estimator,*args,**kwargs):
		field = '_estimator'
		setattr(self,field,estimator)
		return
	
	def get_estimator(self,*args,**kwargs):
		field = '_estimator'
		return getattr(self,field)
	
	def set_param_grid(self,param_grid,*args,**kwargs):
		field = '_param_grid'
		_field = '_param_grid_names'
		if isinstance(param_grid,dict):
			setattr(self,_field,param_grid)
			setattr(self,field,self._permute(param_grid))
		else:
			keys = list(set([k for params in param_grid for k in params]))
			values = {k: [params[k] for params in params_grid if k in params]
						 for k in keys}
			setattr(self,_field,values)
			setattr(self,field,param_grid)
		
		self.set_n_params(len(getattr(self,field)))
		return
	
	def get_param_grid(self,*args,**kwargs):
		field = '_param_grid'
		return getattr(self,field)

	def _get_cv(self,cv,n_splits,**kwargs):
		try:
			return cv(n_splits,**kwargs)
		except TypeError:
			return cv(n_splits)

	def get_cv(self,*args,**kwargs):
		field = '_cv'
		return getattr(self,field)

	def set_cv(self,cv,*args,**kwargs):
		default = 'KFold'
		cvs = {'KFold':model_selection.KFold,'RepeatedKFold':model_selection.RepeatedKFold,
			   'TimeSeriesSplit':model_selection.TimeSeriesSplit,
			   'KFolder':KFolder,
			   'UniformFolder':UniformFolder,
			   }
		if isinstance(cv,dict):
			field = 'cv'
			value = cv.get(field)
			if value is None:
				cv[field] = cvs[default]
			elif isinstance(value,str):
				cv[field] = cvs.get(value,cvs[default])
			kwargs.update(cv)
			cv = self._get_cv(**kwargs)
		field = '_cv'			
		setattr(self,field,cv)
		self.set_n_splits(getattr(cv,'n_splits',getattr(cv,'get_n_splits',nullfunc)()))
		return

	def set_n_params(self,n_params,*args,**kwargs):
		field = 'n_params'
		return setattr(self,field,n_params)

	def get_n_params(self,*args,**kwargs):
		field = 'n_params'
		return getattr(self,field)

	def set_n_splits(self,n_splits,*args,**kwargs):
		field = 'n_splits'
		return setattr(self,field,n_splits)

	def get_n_splits(self,*args,**kwargs):
		field = 'n_splits'
		return getattr(self,field)


	def set_cv_results_(self,cv_results_,*args,**kwargs):
		field = 'cv_results_'
		_cv_results_ = np.array(cv_results_).reshape((self.get_n_params(),self.get_n_splits()))
		cv_results_ = {}
		cv_results_.update({
			**{'param_%s'%(param): np.array([params[param] for params in self.get_param_grid()]) for param in self._param_grid_names},			
			**{'split%d_test_score'%(i):_cv_results_[:,i] for i in range(self.get_n_splits())},
			**{'mean_test_score':_cv_results_.mean(axis=1),'std_test_score':_cv_results_.std(axis=1)},
			**{'params':self.get_param_grid()},
			})
		setattr(self,field,cv_results_)

		best_index_ = np.argmax(cv_results_['mean_test_score'])
		best_score_ = cv_results_['mean_test_score'][best_index_]
		best_params_ = cv_results_['params'][best_index_]
		best_cls_ = self.get_estimator().__class__
		best_estimator_ = best_cls_(**best_params_)

		self.set_best_index_(best_index_)
		self.set_best_score_(best_score_)
		self.set_best_params_(best_params_)
		self.set_best_estimator_(best_estimator_)

		logger.log(self.verbose,"Best params: %s"%(' ,'.join(['%s: %r'%(k,self.get_best_params_()[k]) for k in self.get_best_params_()])))
		return
	
	def get_cv_results_(self,*args,**kwargs):
		field = 'cv_results_'        
		return getattr(self,field)

	def set_best_index_(self,best_index_,*args,**kwargs):
		field = 'best_index_'
		setattr(self,field,best_index_)
		return

	def get_best_index_(self,*args,**kwargs):
		field = 'best_index_'        
		return getattr(self,field)

	def set_best_estimator_(self,best_estimator_,*args,**kwargs):
		field = 'best_estimator_'
		setattr(self,field,best_estimator_)

	def get_best_estimator_(self,*args,**kwargs):
		field = 'best_estimator_'
		return getattr(self,field)

	def set_best_params_(self,best_params_,*args,**kwargs):
		field = 'best_params_'
		setattr(self,field,best_params_)
		return

	def get_best_params_(self,*args,**kwargs):
		field = 'best_params_'        
		return getattr(self,field)

	def set_best_score_(self,best_score_,*args,**kwargs):
		field = 'best_score_'
		setattr(self,field,best_score_)
		return

	def get_best_score_(self,*args,**kwargs):
		field = 'best_score_'        
		return getattr(self,field)
	
	
	def _permute(self,dictionary,_copy=False,_groups=None,_ordered=True):

		def _copier(key,value,_copy):
			# _copy is a boolean, or dictionary with keys
			if ((not _copy) or (isinstance(_copy,dict) and (not _copy.get(key)))):
				return value
			else:
				return copy.deepcopy(value)
		
		def indexer(keys,values,_groups):
			_groups = copy.deepcopy(_groups)
			if _groups is not None:
				inds = [[keys.index(k) for k in g] for g in _groups]
			else:
				inds = []
				_groups = []
			N = len(_groups)
			_groups.extend([[k] for k in keys if all([k not in g for g in _groups])])
			inds.extend([[keys.index(k) for k in g] for g in _groups[N:]])
			values = [[values[j] for j in i ] for i in inds]

			return _groups,values

		def zipper(keys,values,_copy): 
			return [{k:_copier(k,u,_copy) for k,u in zip(keys,v)} for v in zip(*values)]

		def unzipper(dictionary):
			keys, values = zip(*dictionary.items())	
			return keys,values

		def permuter(dictionaries): 
			return [{k:d[k] for d in dicts for k in d} for dicts in itertools.product(*dictionaries)]

		if dictionary in [None,{}]:
			return [{}]

		keys,values = unzipper(dictionary)

		keys_ordered = keys

		keys,values = indexer(keys,values,_groups)

		dictionaries = [zipper(k,v,_copy) for k,v in zip(keys,values)]

		dictionaries = permuter(dictionaries)

		if _ordered:
			for i,d in enumerate(dictionaries):
				dictionaries[i] = {k: dictionaries[i][k] for k in keys_ordered}    
		return dictionaries
	




class CrossValidate(Estimator):
	def __init__(self,*args,**kwargs):
		call(super(),'__init__',*args,**kwargs)

		fields = ['estimator','param_grid','cv']
		for field in fields:
			kwargs[field] = getattr(self,field,kwargs.get(field))

		self.gridsearch = call(None,GridSearchCV,*args,**kwargs)
		return

	def fit(self,X,y,**kwargs):
		if self.prioritize:
			kwargs.update(self.get_params())				
		self.gridsearch.fit(X,y)
		self.set_estimator(None)
		self.set_coef_(None)	
		return self
	
	def predict(self,X,y=None,**kwargs):
		if self.prioritize:
			kwargs.update(self.get_params())	
		try:
			return self.get_estimator().predict(X,**kwargs)
		except:
			self.fit(X,y,**kwargs)
			return self.get_estimator().predict(X,**kwargs)

	def loss(self,X,y,*args,**kwargs):
		if self.prioritize:
			kwargs.update(self.get_params())			
		try:
			return self.get_estimator().loss(X,y,*args,**kwargs)
		except:
			self.fit(X,y,**kwargs)
			return self.get_estimator().loss(X,y,*args,**kwargs)

	def score(self,X,y,*args,**kwargs):
		if self.prioritize:
			kwargs.update(self.get_params())			
		try:
			return self.get_estimator().score(X,y,*args,**kwargs)
		except:
			self.fit(X,y,**kwargs)
			return self.get_estimator().score(X,y,*args,**kwargs)

	def criteria(self,X,y,*args,**kwargs):
		if self.prioritize:
			kwargs.update(self.get_params())			
		try:
			return self.get_estimator().criteria(X,y,*args,**kwargs)
		except:
			# self.fit(X,y,**kwargs)
			return self.get_estimator().criteria(X,y,*args,**kwargs)

	def _get_coef_(self,*args,**kwargs):
		field = 'coef_'
		return getattr(self.get_estimator(),field,getattr(self,field))			


	def set_coef_(self,coef_,*args,**kwargs):
		field = 'coef_'		
		if coef_ is None and hasattr(self.get_estimator(),'get%s%s'%('_' if not field.startswith('_') else '',field)):
			coef_ = getattr(self.get_estimator(),'get%s%s'%('_' if not field.startswith('_') else '',field))()
		if coef_ is None and hasattr(self.get_estimator(),field):
			coef_ = getattr(self.get_estimator(),field)

		setattr(self,field,coef_)

		if self.get_estimator():
			if hasattr(self.get_estimator(),'set%s%s'%('_' if not field.startswith('_') else '',field)):
				getattr(self.get_estimator(),'set%s%s'%('_' if not field.startswith('_') else '',field))(coef_)
			else:
				setattr(self.get_estimator(),field,coef_)
		if self.get_best_estimator_():
			if hasattr(self.get_best_estimator_(),'set%s%s'%('_' if not field.startswith('_') else '',field)):
				getattr(self.get_best_estimator_(),'set%s%s'%('_' if not field.startswith('_') else '',field))(coef_)
			else:
				setattr(self.get_best_estimator_(),field,coef_)		
		return		


	def get_coef_(self,*args,**kwargs):
		field = 'coef_'
		return getattr(self,field,getattr(self,'_get%s%s'%('_' if not field.startswith('_') else '',field))(*args,**kwargs))


	def _get_best_estimator_(self,*args,**kwargs):
		field = 'best_estimator_'
		_field = 'estimator'
		default = getattr(self,'get%s%s'%('_' if not _field.startswith('_') else '',_field))(*args,**kwargs)
		return getattr(self.gridsearch,field,getattr(self,field,default))

	def set_best_estimator_(self,best_estimator_,*args,**kwargs):
		field = 'best_estimator_'
		if best_estimator_ is None:
			best_estimator_ = self.get_estimator()
		setattr(self,field,best_estimator_)
		try:
			setattr(self.gridsearch,field)
		except:
			pass
		return

	def get_best_estimator_(self,*args,**kwargs):
		field = 'best_estimator_'
		return getattr(self,'_get%s%s'%('_' if not field.startswith('_') else '',field))(*args,**kwargs)


	def set_estimator(self,estimator,*args,**kwargs):		
		field = '_estimator'
		_field = 'best_estimator_'
		if estimator is None:
			estimator = getattr(self,'get%s%s'%('_' if not _field.startswith('_') else '',_field))(*args,**kwargs)
		if estimator is None and hasattr(self,field):
			estimator = getattr(self,'get%s%s'%('_' if not field.startswith('_') else '',field))(*args,**kwargs)

		estimator = getattr(self,'_set%s%s'%('_' if not field.startswith('_') else '',field))(estimator,*args,**kwargs)
		setattr(self,field,estimator)
		getattr(self,'set%s%s'%('_' if not _field.startswith('_') else '',_field))(estimator,*args,**kwargs)
		return
	

	def get_best_params_(self,*args,**kwargs):
		field = 'best_params_'        
		default = self.get_params()
		try:
			return getattr(self.gridsearch,field,default)
		except:
			return default
	
	def set_best_params_(self,best_params_,*args,**kwargs):
		field = 'best_params_'
		setattr(self.gridsearch,field,best_params_)
		return

	def get_cv_results_(self,*args,**kwargs):
		field = 'cv_results_'        
		default = {}
		try:
			return getattr(self.gridsearch,field,default)
		except:
			return default
	
	def set_cv_results_(self,cv_results_,*args,**kwargs):
		field = 'cv_results_'
		setattr(self.gridsearch,field,cv_results_)
		return    


	def _get_cv(self,cv,n_splits,**kwargs):
		try:
			return cv(n_splits,**kwargs)
		except TypeError:
			return cv(n_splits)

	def get_cv(self,*args,**kwargs):
		field = '_cv'
		return getattr(self,field)

	def set_cv(self,cv,*args,**kwargs):
		default = 'KFold'
		cvs = {'KFold':model_selection.KFold,'RepeatedKFold':model_selection.RepeatedKFold,
			   'TimeSeriesSplit':model_selection.TimeSeriesSplit,
			   'KFolder':KFolder,
			   'UniformFolder':UniformFolder,
			   }
		if isinstance(cv,dict):
			field = 'cv'		
			value = cv.get(field)
			if value is None:
				cv[field] = cvs[default]
			elif isinstance(value,str):
				cv[field] = cvs.get(value,cvs[default])
			kwargs.update(cv)
			cv = self._get_cv(**kwargs)
		field = '_cv'				
		setattr(self,field,cv)
		self.set_n_splits(getattr(cv,'n_splits',getattr(cv,'get_n_splits',nullfunc)()))
		return


	def set_n_splits(self,n_splits,*args,**kwargs):
		field = 'n_splits'
		return setattr(self,field,n_splits)

	def get_n_splits(self,*args,**kwargs):
		field = 'n_splits'
		return getattr(self,field)


	def plot(self,X,y,*args,**kwargs):
		self.fit(X,y,*args,**kwargs)
		
		parameters = {}
		parameters['name'] = r'\textrm{%s}'%(str(self.__class__.__name__).replace('_',r'\_')) #split('.')[-1].replace("'>",''))
		parameters['path'] = 'plot.pdf'
		parameters['figsize'] = {'h':15,'w':30}
		parameters['fig'] = kwargs.get('fig')
		parameters['axes'] = kwargs.get('axes')

		parameters['param'] = kwargs.get('plot_param','alpha_')
		parameters['params'] = np.asarray(self.get_cv_results_().get('param_%s'%(parameters['param']), 
								   self.get_cv_results_()['param_%s'%('alpha_')]))
		parameters['result_mean'] = kwargs.get('plot_result','mean_test_loss')
		parameters['result_mean_'] = {'mean_test_loss': 'mean_test_score'}.get(parameters['result_mean'],parameters['result_mean'])

		parameters['result_std'] = kwargs.get('plot_result','std_test_loss')
		parameters['result_std_'] = {'std_test_loss': 'std_test_score'}.get(parameters['result_std'],parameters['result_std'])        
		
		parameters['score_mean'] = self.get_cv_results_().get(parameters['result_mean_'],self.get_cv_results_()['mean_test_score'])
		parameters['score_mean'] *= (-1 if 'loss' in parameters['result_mean'] else 1)
		parameters['score_std'] = self.get_cv_results_().get(parameters['result_std_'],self.get_cv_results_()['std_test_score'])
		parameters['score_mean_best'] = getattr(np,'min' if 'loss' in parameters['result_mean'] else 'max')(parameters['score_mean'])
		parameters['param_best'] = parameters['params'][getattr(np,'argmin' if 'loss' in parameters['result_mean'] else 'argmax')(parameters['score_mean'])]

		parameters['fit__y'] = y
		parameters['fit__x'] = np.arange(parameters['fit__y'].shape[0])
		parameters['fit__y_pred'] = self.predict(X,y,*args,**kwargs)
		parameters['fit__x_pred'] = np.arange(parameters['fit__y_pred'].shape[0])
		parameters['fit__label'] = r'$y_{}$'
		parameters['fit__label_pred'] = r'$y_{%s} - %s$'%(parameters['name'],scinotation(parameters['score_mean_best']))
		
		parameters['score__y'] = parameters['score_mean']
		parameters['score__x'] = parameters['params']
		parameters['score__y_error'] = parameters['score_std']        
		parameters['score__x_best'] = parameters['param_best']
		parameters['score__label'] = r'${%s}$'%(parameters['name'])
		parameters['score__label_best'] = r'$\lambda_{\textrm{%s}} : %s,~\textrm{Loss} : %s$'%(parameters['name'],scinotation(parameters['param_best']),scinotation(parameters['score_mean_best']))
		

		parameters['result_mean_manual'] = parameters['result_mean']
		parameters['result_mean_manual_'] = parameters['result_mean_']

		parameters['result_std_manual'] = parameters['result_std']
		parameters['result_std_manual_'] = parameters['result_std_']
		parameters['param_manual'] = parameters['param']
		parameters['params_manual'] = parameters['params']
		parameters['score_mean_manual'] = np.zeros(parameters['params_manual'].shape)
		parameters['score_mean_manual'] = np.zeros(parameters['params_manual'].shape)
		for i,param in enumerate(parameters['params_manual']):
			self.get_estimator().set_params(**{parameters['param_manual']:param})
			self.get_estimator().fit(X,y,*args,**kwargs)
			parameters['score_mean_manual'][i] = self.get_estimator().score(X,y,*args,**kwargs)
		parameters['score_mean_manual'] *= (-1 if 'loss' in parameters['result_mean_manual'] else 1)
		parameters['score_mean_best_manual'] = getattr(np,'min' if 'loss' in parameters['result_mean_manual'] else 'max')(parameters['score_mean_manual'])
		parameters['param_best_manual'] = parameters['params_manual'][getattr(np,'argmin' if 'loss' in parameters['result_mean_manual'] else 'argmax')(parameters['score_mean_manual'])]


		self.get_estimator().set_params(**{parameters['param_manual']:parameters['param_best_manual']})
		self.get_estimator().fit(X,y,*args,**kwargs)
		# self.set_estimator(None)
		parameters['fit__y_pred_manual'] = self.get_estimator().predict(X,y,*args,**kwargs)
		parameters['fit__x_pred_manual'] = np.arange(parameters['fit__y_pred'].shape[0])
		parameters['fit__label_pred_manual'] = r'$y_{%s_{\textrm{manual}}} - %s$'%(parameters['name'],scinotation(parameters['score_mean_best_manual']))

		parameters['score__y_manual'] = parameters['score_mean_manual']
		parameters['score__x_manual'] = parameters['params_manual']
		parameters['score__y_error_manual'] = 0        
		parameters['score__x_best_manual'] = parameters['param_best_manual']
		parameters['score__label_manual'] = r'${%s}_{\textrm{manual}}$'%(parameters['name'])
		parameters['score__label_best_manual'] = r'$\lambda_{\textrm{%s}_{\textrm{manual}}} : %s,~\textrm{Loss} : %s$'%(parameters['name'],scinotation(parameters['param_best_manual']),scinotation(parameters['score_mean_best_manual']))		
		



		parameters['settings'] = {
			'fit':{
				'fig':{
					'set_size_inches':parameters['figsize'],
					'tight_layout':{},
					'savefig':{'fname':parameters['path'],'bbox_inches':'tight'},
				},
				'ax':{
					'plot':[
						# *[{
						#  'x':parameters['fit__x'],
						#  'y':parameters['fit__y'],
						#  'label':parameters['fit__label'],                         
						#  'linestyle':'--',
						#  'alpha':0.7,
						#   } if parameters['fig'] is None else []],
						*[{
						 'x':parameters['fit__x_pred'],
						 'y':parameters['fit__y_pred']-parameters['fit__y'],
						 'label':parameters['fit__label_pred'],
						 'linestyle':'-',
						 'alpha':0.6,
						  }],
						*[{
						 'x':parameters['fit__x_pred_manual'],
						 'y':parameters['fit__y_pred_manual']-parameters['fit__y'],
						 'label':parameters['fit__label_pred_manual'],
						 'linestyle':'-',
						 'alpha':0.6,
						  }]						  
						],
					'set_xlabel':{'xlabel':r'$x_{}$'},
					'set_ylabel':{'ylabel':r'$y_{}$'},
					'legend':{'loc':'best','prop':{'size':15}}
					},
				'style':{
					'layout':{'nrows':1,'ncols':2,'index':1},
				},
			},
			'score':{
				'fig':{
					'set_size_inches':parameters['figsize'],
					'tight_layout':{},
					'savefig':{'fname':parameters['path'],'bbox_inches':'tight'},
				},
				'ax':{
					'plot':[
						*[{
						 'x':parameters['score__x'],
						 'y':parameters['score__y'],
						 # 'yerr':parameters['score__y_error'],
						 'label':parameters['score__label'],
						 'linestyle':'-',
						 'linewidth':1,						 						 
						 'alpha':0.7,
						  }],
						*[{
						 'x':parameters['score__x_manual'],
						 'y':parameters['score__y_manual'],
						 # 'yerr':parameters['score__y_error_manual'],
						 'label':parameters['score__label_manual'],
						 'linestyle':'-',
						 'linewidth':1,						 						 
						 'alpha':0.7,
						  }],						  
						],
					'axvline':[
						*[{
						 'x':parameters['score__x_best'],
						 'ymin':0,
						 'ymax':1,
						 'label':parameters['score__label_best'],
						 'linestyle':'--',
						 'linewidth':2,
						 'alpha':0.8,
						 # 'color':'__lines__',
						  }],
						*[{
						 'x':parameters['score__x_best_manual'],
						 'ymin':0,
						 'ymax':1,
						 'label':parameters['score__label_best_manual'],
						 'linestyle':'--',
						 'alpha':0.8,
						 'linewidth':2,						 
						 'color':'__cycle__',
						  }],						  
						],
					'set_xlabel':{'xlabel':r'${\lambda}_{}$'},
					'set_ylabel':{'ylabel':r'${\textrm{%s}}_{}$'%('Loss' if 'loss' in parameters['result_mean'] else 'Score')},                  
					'set_xscale':{'value':'log'},
					'set_yscale':{'value':'log'},  
					'set_xnbins':{'numticks':5},
					'set_ynbins':{'numticks':5},
					# "set_xmajor_formatter":{"ticker":"ScalarFormatter"},											
					# "set_ymajor_formatter":{"ticker":"ScalarFormatter"},											
					# "ticklabel_format":{"axis":"x","style":"sci","scilimits":[-1,2]},
					# "ticklabel_format":{"axis":"y","style":"sci","scilimits":[-1,2]},						         
					'legend':{'loc':'best','prop':{'size':15}}
					 },
				'style':{
					'layout':{'nrows':1,'ncols':2,'index':2},
				},                        
			}            
					
		}
		fig,axes = plot(settings=parameters['settings'],fig=parameters['fig'],axes=parameters['axes'])
#         fig,axes = parameters['fig'],parameters['axes']

		return fig,axes  


class Solver(object):
	def __init__(self,solver,*args,**kwargs):	

		self.solvers = {name: getattr(self,name) 
			for name in [
			'pinv',
			'lstsq',
			'solve',
			'svd',
			'normal',    
			'normalsolve',
			'normalpinv',    
			'normallstsq',
			'linearregression',
			'ridge',
			'lasso',
			]
		}
		self.default = 'lstsq'
		self.set_solver(solver,*args,**kwargs)
		return

	def set_solver(self,solver,*args,**kwargs):
		self.solver = self.solvers.get(solver,self.solvers[self.default])
		return

	def get_solver(self,*args,**kwargs):
		return self.solver

	def __call__(self,X,y,*args,**kwargs):
		return self.get_solver()(X,y,*args,**kwargs)

	def pinv(self,X,y,*args,**kwargs):
		return np.linalg.pinv(X).dot(y)

	def lstsq(self,X,y,*args,**kwargs):
		rcond = kwargs.get('rcond',None)
		return np.linalg.lstsq(X,y,rcond=rcond)[0]

	def solve(self,X,y,*args,**kwargs):
		return np.linalg.solve(X,y)

	def svd(self,X,y,*args,**kwargs):
		alpha_ = kwargs.get('alpha_',0.0)
		U,S,VT = np.linalg.svd(X,full_matrices=False)
		return (VT.T*(S/(S**2 + alpha_))).dot(U.T.dot(y))

	def normal(self,X,y,*args,**kwargs):
		alpha_ = kwargs.get('alpha_',0.0)
		A = gram(X) + alpha_*np.identity(X.shape[1])
		b = project(X,y)
		if alpha_ > 0:
			return self.solve(A,b,*args,**kwargs)
		else:
			return self.lstsq(X,y,*args,**kwargs)

	def normalsolve(self,X,y,*args,**kwargs):
		alpha_ = kwargs.get('alpha_',0.0)
		A = gram(X) + alpha_*np.identity(X.shape[1])
		b = project(X,y)
		return self.solve(A,b,*args,**kwargs)

	def normalpinv(self,X,y,*args,**kwargs):
		alpha_ = kwargs.get('alpha_',0.0)
		A = gram(X) + alpha_*np.identity(X.shape[1])
		b = project(X,y)
		return self.pinv(A,b,*args,**kwargs)		

	def normallstsq(self,X,y,*args,**kwargs):
		alpha_ = kwargs.get('alpha_',0.0)
		A = gram(X) + alpha_*np.identity(X.shape[1])
		b = project(X,y)
		return self.lstsq(A,b,*args,**kwargs)		

	def linearregression(self,X,y,*args,**kwargs):
		model = call(None,linear_model.LinearRegression,*args,**kwargs)
		model.fit(X,y)
		return model.coef_

	def ridge(self,X,y,*args,**kwargs):
		model = call(None,linear_model.Ridge,*args,**kwargs)
		model.fit(X,y)
		return model.coef_

	def lasso(self,X,y,*args,**kwargs):
		model = call(None,linear_model.Lasso,*args,**kwargs)
		model.fit(X,y)
		return model.coef_	    	    


class LinearRegression(Estimator):
	def __init__(self,*args,**kwargs):
		call(super(),'__init__',*args,**kwargs)        
		return

	def predict(self,X,y=None,**kwargs):
		try:
			coef_ = kwargs.get('coef_',self.get_coef_())
			return X.dot(coef_)
		except:
			self.fit(X,y,**kwargs)
			return X.dot(self.get_coef_())
		return

class OLS(LinearRegression):
	def __init__(self,*args,**kwargs):
		kwargs['solver'] = kwargs.get('solver','pinv')
		call(super(),'__init__',*args,**kwargs)
		return

	def fit(self,X,y,*args,**kwargs):
		if self.prioritize:
			kwargs.update(self.get_params())				
		self.set_coef_(self.get_solver()(X,y,*args,**kwargs))
		self.set_intercept_(X,y,*args,**kwargs)		
		return self

class Tikhonov(LinearRegression):
	def __init__(self,*args,**kwargs):
		kwargs['solver'] = kwargs.get('solver','solve')
		call(super(),'__init__',*args,**kwargs)
		return
	def fit(self,X,y,*args,**kwargs):
		if self.prioritize:
			kwargs.update(self.get_params())				
		alpha_ = getattr(self,'alpha_',kwargs.get('alpha_',kwargs.get('alpha',1e-30)))
		I = getattr(self,'constraint_',kwargs.get('constraint_',np.identity(X.shape[1])))	
		
		A = gram(X) + alpha_*gram(I)
		b = project(X,y)
		self.set_coef_(self.get_solver()(A,b,*args,**kwargs))
		self.set_intercept_(X,y,*args,**kwargs)				
		
		# logger.log(self.verbose,'alpha_: %r'%(alpha_))
		return self


class Ridge(LinearRegression):
	def __init__(self,*args,**kwargs):
		kwargs['solver'] = kwargs.get('solver','normal')
		call(super(),'__init__',*args,**kwargs)
		return
	def fit(self,X,y,*args,**kwargs):
		if self.prioritize:
			kwargs.update(self.get_params())				
		kwargs['alpha_'] = getattr(self,'alpha_',kwargs.get('alpha_',kwargs.get('alpha',1e-30)))
		self.set_coef_(self.get_solver()(X,y,*args,**kwargs))
		self.set_intercept_(X,y,*args,**kwargs)				
		
		# logger.log(self.verbose,'alpha_: %r'%(kwargs['alpha_']))
		return self


class Lasso(LinearRegression):
	def __init__(self,*args,**kwargs):
		kwargs['solver'] = kwargs.get('solver','normal')		
		call(super(),'__init__',*args,**kwargs)
		return
	def fit(self,X,y,*args,**kwargs):
		if self.prioritize:
			kwargs.update(self.get_params())				
		kwargs['alpha'] = getattr(self,'alpha_',kwargs.get('alpha_',kwargs.get('alpha',1e-30)))
		coef_, self.dual_gap_, self.eps_, self.n_iter_ = self.enet_coordinate_descent(X,y,**kwargs)
		self.set_coef_(np.asarray(coef_))
		self.set_intercept_(X,y,*args,**kwargs)				
		return self

	def enet_coordinate_descent(self,X,y,*args,**kwargs):
		# Enet coordinate descent
		settings = {
			'w':None,
			'alpha':1.0,
			'beta':0.0,
			'X':X,
			'y':y,
			'max_iter':1000,
			'tol':1e-4,
			'rng':None,
			'random':'random',
			'positive':False,
		}
		parsing = {
			'w': lambda **kwargs: np.asarray(kwargs['w'] if kwargs['w'] is not None else zeros(kwargs['X'].shape[1]),order='F'),
			'X': lambda **kwargs: np.asarray(X, order='F'),
			'y': lambda **kwargs: np.asarray(y, order='F'),
			'rng':lambda **kwargs:seed(kwargs['rng']),
			'random': lambda **kwargs: {'random':1,'cyclic':0}.get(kwargs['random'],0)
				  }
		settings.update({k: kwargs[k] for k in kwargs if k in settings})
		settings.update({k: parsing[k](**settings) for k in parsing})

		from sklearn import linear_model
		coef_,dual_gap_,eps_,n_iter_ = linear_model._cd_fast.enet_coordinate_descent(*[settings[k] for k in settings])

		return coef_,dual_gap_,eps_,n_iter_



class Enet(LinearRegression):
	def __init__(self):
		kwargs['solver'] = kwargs.get('solver','normal')		
		call(super(),'__init__',*args,**kwargs)        
		return
	def fit(self,X,y,*args,**kwargs):
		if self.prioritize:
			kwargs.update(self.get_params())				
		kwargs['alpha'] = kwargs.get('alpha_',kwargs.get('alpha',1.0))
		kwargs['beta'] = kwargs.get('beta_',kwargs.get('beta',1.0))
		coef_, self.dual_gap_, self.eps_, self.n_iter_ = self.enet_coordinate_descent(X,y,**kwargs)
		self.set_coef_(np.asarray(coef_))
		self.set_intercept_(X,y,*args,**kwargs)				
		return self

	def enet_coordinate_descent(self,X,y,*args,**kwargs):
		# Enet coordinate descent
		settings = {
			'w':None,
			'alpha':1.0,
			'beta':0.0,
			'X':X,
			'y':y,
			'max_iter':1000,
			'tol':1e-4,
			'rng':None,
			'random':'random',
			'positive':False,
		}
		parsing = {
			'w': lambda **kwargs: np.asarray(kwargs['w'] if kwargs['w'] is not None else zeros(kwargs['X'].shape[1]),order='F'),
			'X': lambda **kwargs: np.asarray(X, order='F'),
			'y': lambda **kwargs: np.asarray(y, order='F'),
			'rng':lambda **kwargs:seed(kwargs['rng']),
			'random': lambda **kwargs: {'random':1,'cyclic':0}.get(kwargs['random'],0)
				  }
		settings.update({k: kwargs[k] for k in kwargs if k in settings})
		settings.update({k: parsing[k](**settings) for k in parsing})

		from sklearn import linear_model
		coef_,dual_gap_,eps_,n_iter_ = linear_model._cd_fast.enet_coordinate_descent(*[settings[k] for k in settings])

		return coef_,dual_gap_,eps_,n_iter_


class TikhonovCV(CrossValidate):
	def __init__(self,*args,**kwargs):
		self.set_estimator(kwargs.pop('estimator','Ridge'),*args,**kwargs)
		kwargs['estimator'] = self.get_estimator()
		kwargs['param_grid'] = {
			**{'alpha_':
				kwargs.get('alphas',[*([0] if kwargs.get('alpha_zero') else []),
									 *np.logspace(*kwargs.get('alpha',[-10,-1,10]) if hasattr(kwargs.get('alpha',[-10,-1,10]),'__iter__') else [-10,1,10])
									]),
			},
			**kwargs.get('param_grid',{})
			}
		kwargs['cv'] = kwargs.get('cv')
		call(super(),'__init__',*args,**kwargs)        
		return
	
class RidgeCV(CrossValidate):
	def __init__(self,*args,**kwargs):
		self.set_estimator(kwargs.pop('estimator','Ridge'),*args,**kwargs)
		kwargs['estimator'] = self.get_estimator()
		kwargs['param_grid'] = {
			**{'alpha_':
				kwargs.get('alphas',[*([0] if kwargs.get('alpha_zero') else []),
									 *np.logspace(*kwargs.get('alpha',[-10,-1,10]) if hasattr(kwargs.get('alpha',[-10,-1,10]),'__iter__') else [-10,1,10])
									]),
			},
			**kwargs.get('param_grid',{})
			}
		kwargs['cv'] = kwargs.get('cv')
		call(super(),'__init__',*args,**kwargs)        
		return
	
	
class LassoCV(CrossValidate):
	def __init__(self,*args,**kwargs):
		self.set_estimator('Lasso',*args,**kwargs)
		kwargs['estimator'] = self.get_estimator()
		kwargs['param_grid'] = {
			**{'alpha_':
				kwargs.get('alphas',[*([0] if kwargs.get('alpha_zero') else []),
									 *np.logspace(*kwargs.get('alpha',[-10,-1,10]) if hasattr(kwargs.get('alpha',[-10,-1,10]),'__iter__') else [-10,1,10])
									]),
			},
			**kwargs.get('param_grid',{})
			}		
		kwargs['cv'] = kwargs.get('cv')
		call(super(),'__init__',*args,**kwargs)        
		return
	
class EnetCV(CrossValidate):
	def __init__(self,*args,**kwargs):
		self.set_estimator('Enet',*args,**kwargs)
		kwargs['estimator'] = self.get_estimator()
		kwargs['param_grid'] = {
			**{'alpha_':
				kwargs.get('alphas',[*([0] if kwargs.get('alpha_zero') else []),
									 *np.logspace(*kwargs.get('alpha',[-10,-1,10]) if hasattr(kwargs.get('alpha',[-10,-1,10]),'__iter__') else [-10,1,10])
									]),
				'beta_':
				kwargs.get('betas',[*([0] if kwargs.get('beta_zero') else []),
									 *np.logspace(*kwargs.get('beta',[-10,-1,10]) if hasattr(kwargs.get('beta',[-10,-1,10]),'__iter__') else [-10,1,10])
									]),
			},
			**kwargs.get('param_grid',{})
			}		
		kwargs['cv'] = kwargs.get('cv')
		call(super(),'__init__',*args,**kwargs)        
		return


class RegressionCV(CrossValidate):
	def __init__(self,*args,**kwargs):
		self.set_estimator(kwargs.get('estimator'),*args,**kwargs)
		kwargs['estimator'] = self.get_estimator()
		kwargs['param_grid'] = {'alpha_':kwargs.get('alphas',np.logspace(*kwargs.get('alpha',[-10,-1,10]) if hasattr(kwargs.get('alpha',[-10,-1,10]),'__iter__') else [-10,1,10])),
								'beta_':kwargs.get('betas',np.logspace(*kwargs.get('beta',[-10,-1,10])))}
		kwargs['cv'] = kwargs.get('cv')
		call(super(),'__init__',*args,**kwargs)        
		return




class Stepwise(Estimator):
	def __init__(self,*args,**kwargs):
		call(super(),'__init__',*args,**kwargs)        
		return

	def _fit(self,X,y,*args,**kwargs):
		self.get_estimator().fit(X,y,*args,**kwargs)
		self.set_coef_(None)
		self.set_intercept_(X,y,*args,**kwargs)
		return self 

	def fit(self,X,y,*args,**kwargs):
		if self.prioritize:
			kwargs.update(self.get_params())	

		Start = timeit.default_timer()

		# Get data shape
		if y.ndim < 2:
			y = y.reshape((*y.shape,*[1]*(2-y.ndim)))
		if X.ndim < 3:
			X = X.reshape((*X.shape,*[1]*(3-X.ndim)))
		Ndata,Ncoef,Ndim = X.shape
		Ndata,Ndim = y.shape

		# Get methods and included indices
		method = getattribute(self,'method','cheapest') # Method for applying statistical criteria
		threshold = getattribute(self,'threshold',1e20) # Threshold for criteria
		fixed = getattribute(self,'fixed',{}) # Fixed indices and coef_ in basis
		included = getattribute(self,'included',[]) # Included X indices in basis

		# Get mappings between stepwise indices, local indices of X', and global indices of X, 
		# where local position of index is X' index and mapped index is X index
		# i.e) indices_local = [0,2] : Stepwise index 1 -> X' index 2 (excludes fixed and included indices)
		# i.e) indices_basis = [0,1,2,3] : X' index 2 -> X' index 2 (excludes fixed indices)
		# i.e) indices_global = [1,2,4,5] : X' index 2: X index 4 (excludes fixed indices)

		local_global = np.arange(Ncoef) # X' indices to X indices
		
		local_fixed = np.array([i for i in fixed if i<Ncoef],dtype=int) # Fixed X' indices to X indices
		local_included = np.array([i for i in included if i<Ncoef and i not in local_fixed],dtype=int) # Included X' indices to X indices

		local_free = local_global[isin(local_global,local_fixed,invert=True)] # Free X' indices to X indices
		coef_fixed = np.array([fixed[i] for i in fixed if i<Ncoef]) # Fixed coef_ at X' indices

		indices_global = local_global[isin(local_global,local_fixed,invert=True)] # Free X' indices to X indices, without fixed indices
		indices_basis = np.arange(indices_global.size)
		indices_local = indices_basis[isin(indices_global,local_included,invert=True)]


		# Account for fixed coefficients in X,y
		X_fixed = X.copy()
		y_fixed = np.zeros((Ndata,Ndim))
		if local_fixed.size > 0:
			for dim in range(Ndim):
				y_fixed[:,dim] = self.predict(X[:,local_fixed,dim],y[...,dim],*args,coef_=coef_fixed,**kwargs)
				y[:,dim] -= y_fixed[:,dim]
			X = X[:,local_free]


		# Size of basis
		# Number of potential operators Nmin <= N <= Nmax, number of total operators G
		N = indices_local.size
		G = local_global.size
		Nmin = getattribute(self,'complexity_min',None)
		Nmax = getattribute(self,'complexity_max',None)
		Nmin = max(0 if N<Ncoef else 1 ,Nmin if isinstance(Nmin,(int,np.integer)) else 0)
		Nmax = min(N,Nmax if isinstance(Nmax,(int,np.integer)) else Ncoef)

		# Stats to be collected with shape, dtype, and default value of numpy array
		collect = getattribute(self,'collect',True)
		_stats = {}
		if collect:	
			_stats.update({'predict':{'shape':[Ncoef,Ndata],'dtype':float,'default':0},})

		_stats.update({
			'loss':{'shape':[Ncoef],'dtype':float,'default':0},
			'score':{'shape':[Ncoef],'dtype':float,'default':0},
			'coef_':{'shape':[Ncoef,Ncoef],'dtype':float,'default':0},
			'criteria':{'shape':[Ncoef],'dtype':float,'default':0},
			'index_':{'shape':[Ncoef],'dtype':int,'default':0},
			'index':{'shape':[Ncoef],'dtype':int,'default':0},
			'dim_':{'shape':[Ncoef],'dtype':int,'default':0},
			'rank_':{'shape':[Ncoef,Ndim],'dtype':int,'default':0},
			'condition_':{'shape':[Ncoef,Ndim],'dtype':float,'default':0},
			'basis_':{'shape':[Ncoef,Ncoef],'dtype':int,'default':-1},
			'complexity_':{'shape':[Ncoef],'dtype':int,'default':0},
			'size_':{'shape':[Ncoef],'dtype':int,'default':0},
			'iterations':{'shape':[Ncoef],'dtype':int,'default':1},
			'coefficient':{'shape':[Ncoef],'dtype':float,'default':0},
		})

		fields = {'stats_modules': [k for k in ['predict','loss','score','coef_','criteria'] if k in _stats],
				  'stats_params':  [k for k in ['index_','index','dim_','rank_','condition_','basis_','complexity_','size_','coefficient',] if k in _stats]}
		for field in fields:
			if not hasattr(self,field):
				setattr(self,field,fields[field])

		stats = {}
		stats.update({k:_stats[k].pop('default')*np.ones(**_stats[k])for k in _stats})
		if method in ['update']:
			stats.update({k: getattr(self,'stats',{}).get(k,stats[k]) for k in getattr(self,'stats_params',[])})

		field = 'coef_'
		stats[field][:,local_fixed] = coef_fixed

		indexes = np.arange(N)
		dims = np.arange(Ndim)
		values = np.zeros((Ndim,Ncoef))


		# Variables
		globs = globals()
		locs = locals()
		
		# Get/Set  kwargs
		fields = {
			**{k:'set' for k in []},
			**{k:'pop' for k in []}
			}
		for field in fields:
			if field in locs:
				if fields[field] in ['set']:
					kwargs[field] = locs[field]
				elif fields[field] in ['pop']:
					kwargs.pop(field)


		# Setup parallelization
		# parallelizer = Parallelize(self.n_jobs,self.backend)
		parallelizer = nullcontext()
		parallel = self.parallel
		processes = self.n_jobs
		def pooler(processes=processes,parallel=parallel,scope=globs): 
			return (getattribute(scope.get(parallel) if isinstance(scope,dict) else parallel,'Pool',nullPool)(processes=processes))


		logger.log(self.verbose,"Start Stepwise Regression")

		with parallelizer as parallelize:
			iteration = 0        
			while ((iteration == 0) or (N<=Nmax and N>Nmin)):
				indices = indexes[:N]
				values[:] = 0
				_values = []
				
				step_kwargs = {'index':None,'dim':None,
								'mapping':indices_local,'iteration':iteration,
								'X':X,'y':y,'fit':self._fit,'func':self.loss,'verbose':False,
								'kwargs':kwargs}
				callback_kwargs = {'index':None,'dim':None,'values':values}

				# Setup indices			
				if method in ['update']:
					indices = [stats['index'][iteration]]
				elif iteration == 0 and method in ['cheapest','smallest']:
					indices = [[]]
		
			
				start = timeit.default_timer()

				# func = self.step
				# iterables = ['index','dim']	
				# iterable = (dict(zip(iterables,(index,dim)))
				# 			for dim in dims
				# 			for index in indices)                     		

				# start = timeit.default_timer()	
				# parallelize(func,iterable,_values,**step_kwargs)
				# try:
				# 	values[:,:N] = np.asarray(_values).reshape((Ndim,N)) 
				# except:
				# 	values[:,-1] = np.asarray(_values).reshape((Ndim)) 
				# end = timeit.default_timer()

				
				pool = pooler()
				for dim in dims:			
					for index in indices:
						step_kwargs.update({'index':index,'dim':dim})
						callback_kwargs.update({'index':index,'dim':dim})
						
						func = wrapper(self.step,**step_kwargs)

						callback = wrapper(self.callback_step,**callback_kwargs)
						pool.apply_async(func=func,callback=callback)	
				pool.close()
				pool.join()
				
				end = timeit.default_timer()

				if iteration == 0:
					index = -1
					dim = dims[(values[:,index]).argmin(axis=0)]
					
					index_local = -1
					index_basis = -1
					index_free = -1
					index_global = -1
					index_local_global = -1
				else:
					criteria = where((np.abs(self.criteria(X,y,
														loss=values[:,indices],
														complexity_=N-1,
														losses=stats['loss'][:iteration],
														complexities_=stats['complexity_'][:iteration]))<=threshold
										).all(axis=0))
					if criteria.size == 0:
						break
					
					index = indices[criteria[((values[:,indices]).sum(axis=0)[criteria]).argmin(axis=0)]]
					dim = dims[(values[:,index]).argmin(axis=0)]


					index_local = indices_local[index]
					index_basis = indices_basis[index_local]
					index_global = indices_global[index_basis]
					index_local_global = local_global[index_global]
					X = delete(X,index_basis,axis=1)	
					
					indices_local = delete(indices_local,index,axis=0)
					indices_local[indices_local>index_local] -= 1								
					indices_basis = delete(indices_basis,index_local,axis=0)
					indices_basis[indices_basis>index_basis] -= 1				
					indices_global = delete(indices_global,index_basis,axis=0)				
					indices_global[indices_global>index_global] -= 1
					local_global = delete(local_global,index_global,axis=0)				

					
					N -= 1
					G -= 1
				
				_estimator = self._fit(X[:,:,dim],y[:,dim],**kwargs)


				if collect:
					stats['predict'][iteration] = self.predict(X[:,:,dim],y[:,dim],**kwargs) + y_fixed[:,dim]
				stats['loss'][iteration] = self.loss(X[:,:,dim],y[:,dim],**kwargs)
				stats['score'][iteration] = self.score(X[:,:,dim],y[:,dim],**kwargs)
				stats['criteria'][iteration] = self.criteria(X[:,:,dim],y[:,dim],
														loss=stats['loss'][iteration],
														complexity_=N,
														losses=stats['loss'][:iteration],
														complexities_=stats['complexity_'][:iteration])
				stats['coef_'][iteration][local_global[indices_global]] = self.get_coef_().copy()
				stats['index_'][iteration] = index_local_global
				stats['index'][iteration] = index
				stats['dim_'][iteration] = dim
				stats['rank_'][iteration] = [rank(X[:,:,dim]) for dim in dims]
				stats['condition_'][iteration] = [cond(X[:,:,dim]) for dim in dims]
				stats['basis_'][iteration][:G] = local_global
				stats['coefficient'][iteration] = np.abs(stats['coef_'][iteration-1][index_local_global])
				stats['complexity_'][iteration] = N
				stats['size_'][iteration] = G
				stats['iterations'][iteration] = iteration+1


				self.set_coef_(stats['coef_'][iteration])

				logger.log(self.verbose,'Iter: %d, Index: (%d,%d,%d), Loss: %0.4e, Rank: %r, Cond: %r, N: %d, Time: %0.4e'%(
									iteration,index,index_local_global,dim,
									stats['loss'][iteration],
									stats['rank_'][iteration].tolist()[slice(None) if len(dims)>1 else 0],
									stats['condition_'][iteration].tolist()[slice(None) if len(dims)>1 else 0],
									N,end-start))

				iteration += 1

		self.set_stats({k: stats[k][:iteration] for k in stats})
		

		End = timeit.default_timer()

		# Restore X,y
		X = X_fixed
		for dim in range(Ndim):
			y[:,dim] += y_fixed[:,dim]

		logger.log(self.verbose,"Done Stepwise Regression")

		return self		


	def predict(self,X,y=None,**kwargs):
		try:
			return self.get_estimator().predict(X,y,**kwargs)
		except:
			self.fit(X,y,**kwargs)
			return self.get_estimator().predict(X,y,**kwargs)

	# Stepwise iteration
	@staticmethod
	def step(iteration=None,index=None,dim=None,mapping=None,X=None,y=None,fit=None,func=None,verbose=False,kwargs={}):
		logger.log(verbose,'Iter: %r, Dim: %r, Index: %r'%(iteration,dim,index))			
		try:
			_X = delete(X[:,:,dim],mapping[index],axis=1)	
			_y = y[:,dim]
		except:
			_X = X[:,:,dim]
			_y = y[:,dim]
		fit(_X,_y,**kwargs)
		return func(_X,_y,**kwargs)

	# Stepwise callback
	@staticmethod
	def callback_step(value=None,index=None,dim=None,values=None):
		try:
			values[dim][index] = value
		except:
			values[dim] = value
		return

	def plot(self,X,y,*args,**kwargs):
		
		self.fit(X,y,*args,**kwargs)

		parameters = {}
		parameters['path'] = 'stepwise.pdf'
		parameters['name'] = str(self.get_estimator().__class__.__name__)
		parameters['nrows'] = 1
		parameters['ncols'] = 2 if 'CV' not in parameters['name'] else 3
		parameters['figsize'] = {'h':10,'w':20} if 'CV' not in parameters['name'] else {'h':10,'w':30}
		parameters['subplots_adjust'] = {"hspace":2,"wspace":2} if 'CV' not in parameters['name'] else {"hspace":2,"wspace":3}

		
		parameters['iterations'] = [i if i is not None else self.get_stats()['size_'].max() for i in [None,10,5,1] if i is None or (i > self.get_stats()['size_'].min())]
		parameters['iterations'] = list(sorted(list(set(parameters['iterations'])),key=lambda i: parameters['iterations'].index(i)))
		
		parameters['fig'] = kwargs.get('fig')
		parameters['axes'] = kwargs.get('axes')
		
		parameters['fit__x'] = X[:,0,0] if X.ndim>2 else X[:,0]
		parameters['fit__y'] = y[:,0] if y.ndim > 1 else y
		parameters['fit__label'] = r'$y_{}$'

		for i in parameters['iterations']:
			parameters['fit__x_pred_%d'%(i)] = X[:,0,0] if X.ndim>2 else X[:,0]
			parameters['fit__y_pred_%d'%(i)] = self.get_stats()['predict'][self.get_stats()['size_'].max()-i]
			parameters['fit__label_pred_%d'%(i)] = r'$y^{(%d)}_{\textrm{%s}}$'%(i,parameters['name'].replace('_',r'\_'))

		parameters['loss__x'] = self.get_stats()['complexity_']
		parameters['loss__y'] = self.get_stats()['loss']
		parameters['loss__label'] = r'$\textrm{%s}$'%(str(self.get_estimator().__class__.__name__).replace('_',r'\_'))


		if 'CV' in parameters['name']:
			parameters['param'] = kwargs.get('plot_param','alpha_')
			parameters['params'] = np.asarray(self.get_estimator().get_cv_results_().get('param_%s'%(parameters['param']), 
									   self.get_estimator().get_cv_results_()['param_%s'%('alpha_')]))
			parameters['result_mean'] = kwargs.get('plot_result','mean_test_loss')
			parameters['result_mean_'] = {'mean_test_loss': 'mean_test_score'}.get(parameters['result_mean'],parameters['result_mean'])

			parameters['result_std'] = kwargs.get('plot_result','std_test_loss')
			parameters['result_std_'] = {'std_test_loss': 'std_test_score'}.get(parameters['result_std'],parameters['result_std'])        
			
			parameters['score_mean'] = self.get_estimator().get_cv_results_().get(parameters['result_mean_'],self.get_estimator().get_cv_results_()['mean_test_score'])
			parameters['score_mean'] *= (-1 if 'loss' in parameters['result_mean'] else 1)
			parameters['score_std'] = self.get_estimator().get_cv_results_().get(parameters['result_std_'],self.get_estimator().get_cv_results_()['std_test_score'])
			parameters['score_mean_best'] = getattr(np,'min' if 'loss' in parameters['result_mean'] else 'max')(parameters['score_mean'])
			parameters['param_best'] = parameters['params'][getattr(np,'argmin' if 'loss' in parameters['result_mean'] else 'argmax')(parameters['score_mean'])]
			
			parameters['score__y'] = parameters['score_mean']
			parameters['score__x'] = parameters['params']
			parameters['score__y_error'] = parameters['score_std']        
			parameters['score__x_best'] = parameters['param_best']
			parameters['score__label'] = r'${%s}$'%(parameters['name'])
			parameters['score__label_best'] = r'$\lambda_{\textrm{%s}} : %s,~\textrm{Loss} : %s$'%(parameters['name'],scinotation(parameters['param_best']),scinotation(parameters['score_mean_best']))




		parameters['settings'] = {
			'fit':{
				'fig':{
					'set_size_inches':parameters['figsize'],
					'subplots_adjust':parameters['subplots_adjust'],
					'tight_layout':{},
					'savefig':{'fname':parameters['path'],'bbox_inches':'tight'},
				},
				'ax':{
					'plot':[
						*[{
						 'x':parameters['fit__x'],
						 'y':parameters['fit__y'],
						 'label':parameters['fit__label'],                         
						 'linestyle':'--',
						 'alpha':0.7,
						  } if parameters['fig'] is None else []],
						*[{
						 'x':parameters['fit__x_pred_%d'%(i)],
						 'y':parameters['fit__y_pred_%d'%(i)],
						 'label':parameters['fit__label_pred_%d'%(i)],
						 'linestyle':'-',
						 'alpha':0.6,
						  } for i in parameters['iterations']],
						],
					'set_xlabel':{'xlabel':r'$x_{}$'},
					'set_ylabel':{'ylabel':r'$y_{}$'},
					'legend':{'loc':'best','prop':{'size':15}}
					},
				'style':{
					'layout':{'nrows':parameters['nrows'],'ncols':parameters['ncols'],'index':1},
				},
			},
			'loss':{
				'fig':{
					'set_size_inches':parameters['figsize'],
					'subplots_adjust':parameters['subplots_adjust'],					
					'tight_layout':{},
					'savefig':{'fname':parameters['path'],'bbox_inches':'tight'},
				},
				'ax':{
					'plot':[
						*[{
						 'x':parameters['loss__x'],
						 'y':parameters['loss__y'],
						 'label':parameters['loss__label'],
						 'linestyle':'-',
						 'marker':'s',
						 'linewidth':1,						 						 
						 'alpha':0.7,
						  }],
						],
					'set_xlabel':{'xlabel':r'${N_{\textrm{Coefficients}}}$'},
					'set_ylabel':{'ylabel':r'${\textrm{Loss}}_{}$'},                  
					'set_xscale':{'value':'linear'},
					'set_yscale':{'value':'log'},  
					'set_xticks':{'ticks':np.linspace(min(parameters['loss__x'].min()-1,0),parameters['loss__x'].max()+3,max(4,int(parameters['loss__x'].max()-parameters['loss__x'].min())//5)).astype(int)},
					'set_ynbins':{'numticks':5},
					'set_xlim':{'xmin':parameters['loss__x'].max()+3,'xmax':min(parameters['loss__x'].min()-1,0)},
					# "set_xmajor_formatter":{"ticker":"ScalarFormatter"},											
					# "set_ymajor_formatter":{"ticker":"ScalarFormatter"},											
					# "ticklabel_format":{"axis":"x","style":"sci","scilimits":[-1,2]},
					# "ticklabel_format":{"axis":"y","style":"sci","scilimits":[-1,2]},						         
					'legend':{'loc':'best','prop':{'size':15}}
					 },
				'style':{
					'layout':{'nrows':parameters['nrows'],'ncols':parameters['ncols'],'index':2},
				},                        
			},
			**({'score':{
				'fig':{
					'set_size_inches':parameters['figsize'],
					'subplots_adjust':parameters['subplots_adjust'],					
					'tight_layout':{},
					'savefig':{'fname':parameters['path'],'bbox_inches':'tight'},
				},
				'ax':{
					'plot':[
						*[{
						 'x':parameters['score__x'],
						 'y':parameters['score__y'],
						 # 'yerr':parameters['score__y_error'],
						 'label':parameters['score__label'],
						 'linestyle':'-',
						 'linewidth':1,						 						 
						 'alpha':0.7,
						  }],
						],
					'axvline':[
						*[{
						 'x':parameters['score__x_best'],
						 'ymin':0,
						 'ymax':1,
						 'label':parameters['score__label_best'],
						 'linestyle':'--',
						 'linewidth':2,
						 'alpha':0.8,
						 # 'color':'__lines__',
						  }],
						],
					'set_xlabel':{'xlabel':r'${\lambda}_{}$'},
					'set_ylabel':{'ylabel':r'${\textrm{%s}}_{}$'%('Loss' if 'loss' in parameters['result_mean'] else 'Score')},                  
					'set_xscale':{'value':'log'},
					'set_yscale':{'value':'log'},  
					'set_xnbins':{'numticks':5},
					'set_ynbins':{'numticks':5},
					# "set_xmajor_formatter":{"ticker":"ScalarFormatter"},											
					# "set_ymajor_formatter":{"ticker":"ScalarFormatter"},											
					# "ticklabel_format":{"axis":"x","style":"sci","scilimits":[-1,2]},
					# "ticklabel_format":{"axis":"y","style":"sci","scilimits":[-1,2]},						         
					'legend':{'loc':'best','prop':{'size':15}}
					 },
				'style':{
					'layout':{'nrows':parameters['nrows'],'ncols':parameters['ncols'],'index':3},
				},                        
			}} if 'CV' in parameters['name'] else {}),
					
		}
		fig,axes = plot(settings=parameters['settings'],fig=parameters['fig'],axes=parameters['axes'])
		return fig,axes




if __name__ == '__main__':

	n = 100
	m = 5
	X = np.sort(np.random.rand(n,1),axis=0)
	# X = np.concatenate([np.ones((n,1)),*[np.linspace(0,1,n,endpoint=False)[:,None]]*(m-1)],axis=1)
	# X = np.concatenate([*[np.linspace(0,1,n,endpoint=False)[:,None]]*(m)],axis=1)
	coef_ = np.random.rand(m)
	# y = np.sin(X.dot(coef_))

	# func = lambda x: 1 - x + x**2 - x**3 + x**4
	# func = lambda x: x #+ x**2 - x**3 + x**4
	# func = lambda x: np.sin(x)
	# func = lambda x: x
	# yfunc = lambda x: np.concatenate([np.sin(x**i) for i in range(m)],axis=1)
	# Xfunc = lambda x: np.concatenate([x**i for i in range(m)],axis=1)
	# func = lambda x: np.sin(np.pi*X) #np.concatenate([np.sin(x[:,i:i+1]**i) for i in range(x.shape[1])],axis=1)

	yfunc = lambda x: np.sin(np.concatenate([x**i for i in range(1,m+1)],axis=1))
	Xfunc = lambda x: np.concatenate([x**i for i in range(1,m+1)],axis=1)
	X,y = Xfunc(X), yfunc(X).dot(coef_)
	# X = func(X + shuffle(X,axis=1,inplace=False))
	# X = X + shuffle(X,axis=1,inplace=False)


	args = []
	kwargs = {
		# 'estimator':'LassoCV',
		# 'estimator':'RidgeCV',
		# 'loss_func':'weighted',
		# 'score_func':'weighted',
		# 'criteria_func':None,
		# 'normalize':'l2',
		# 'fit_intercept':False,
		'n_jobs':1,
		# 'stats':{},
		# 'max_iter':10000,
		# 'tol':1e-8,
		'cv':{'cv':'UniformFolder','n_splits':1,'random_state':1235,'test_size':0.3},
		# 'cv':{'cv':'KFolder','n_splits':n,'random_state':1235,'test_size':0.3},
		# 'cv':{'cv':'KFold','n_splits':n,'random_state':1235,'test_size':0.3},
		'alphas':np.logspace(-12,-5,100),
		'alpha_zero':True,
		# # 'loss_weights':{'l2':0.5,'l1':0.5},
		# 'prioritize':True,
		'loss_func':'weighted',	
		'score_weights':{'l2':1.0,},
		'loss_weights':{'l2':1.0,},
		'score_l2_inds':slice(None),
		'loss_l2_inds':slice(None),
		"complexity_min": m, 
		'verbose':1,

		# 'estimator_kwargs':{
		# 	'loss_func':'weighted',
		# 	'loss_weights':{'l2':1.0,},
		# 	'alphas':np.logspace(-15,-1,5),
		# 	# 'loss_weights':{'l2':0.5,'l1':0.5},
		# 	'score_func':'weighted',
		# 	'score_weights':{'l2':1.0,},			
		# 	# 'score_weights':{'l2':0.5,'l1':0.5},			
		# 	'alpha_':1e-5,
		# 	'max_iter':10000,
		# 	'tol':1e-8,
		# 	'prioritize':True,
		# 	# 'cv':{'cv':'UniformFolder','n_splits':1,'random_state':1235,'test_size':0.3},
		# 	'cv':{'cv':'KFolder','n_splits':10,'random_state':1235,'test_size':0.3},

		# },
	}
	parameters = {
		'fig':None,
		'axes':None,
	}

	globs = globals()
	locs = locals()
	variables = {**globs,**locs}

	# _estimators = ['OLS','Ridge','Lasso']	
	# _estimators = ['OLS','Ridge','Lasso','RidgeCV','LassoCV']
	_estimators = ['RidgeCV']
	# _estimators = ['LassoCV']
	# _estimators = ['OLS']
	# _estimators = ['Stepwise']*1
	estimators = {}
	for i,name in enumerate(_estimators):
		for n_jobs in [1]:
			kwargs['estimator'] = {'Stepwise': ['OLS','RidgeCV','Ridge'][i+1],'RidgeCV':'Ridge','LassoCV':'Lasso'}.get(name)
			kwargs['n_jobs'] =  n_jobs
			estimators[name] = variables[name](*args,**kwargs)
			scale_X = estimators[name].get_normalize()(X)
			scale_y = estimators[name].get_normalize()(y)
			parameters['fig'],parameters['axes'] = estimators[name].plot(X,y,*args,**kwargs,**parameters)


====================================================================================================
mechanoChemML\src\graph_functions.py
====================================================================================================
#!/usr/bin/env python

# Import python modules
import os,sys,copy,warnings,functools,itertools
from natsort import natsorted
import numpy as np
import scipy as sp
import scipy.stats,scipy.signal
import pandas as pd

# import multiprocess as mp
# import multithreading as mt
# import multiprocessing as mp

warnings.simplefilter("ignore", (UserWarning,DeprecationWarning,FutureWarning))

# Global Variables
DELIMITER='__'

# Import user modules
from .load_dump import load,dump,path_split,path_join
from .dictionary import _set,_get,_pop,_has,_update,_permute,_clone,_find,_replace
from .graph_utilities import invert,add,outer,broadcast,convert,norm,repeat,where,delete,rank,isin,issparse,issingular,isfullrank,isarray,isnone
from .graph_utilities import iscallable,array_series,series_array,masked_index,wrapper,nonzeros,isiterable,subarray,round,explicit_zeros,catch,icombinations
from .graph_utilities import zeros,ones,arange
from .graph_utilities import refinement,neighbourhood,adjacency_matrix,similarity_matrix,combinations,ncombinations,nearest_neighbourhood,boundaries,stencils,twin
from .graph_utilities import add,subtract,prod,multiply,sqrt
from .graph_utilities import ge,gt,lt,le,eq
from .texify import scinotation
#from .estimator import OLS

# Logging
import logging
log = 'info'
logger = logging.getLogger(__name__)
#logger.setLevel(getattr(logging,log.upper()))


# Define dataset class
class dataset(pd.DataFrame):
	'''
	Class of dataset, with numerical data, metadata, and settings
	Args:
		key (str): Label for dataset
		data (panda.DataFrame): Dataframe class for numerical data
		metadata (dict): dictionary of metadata
		settings (dict): dictionary of dataset model settings
	'''
	def __init__(self,key,data,metadata,settings):
		'''
		Initialize class
		'''
		self.set_data(data)
		self.set_metadata(metadata)
		self.set_settings(settings)

		return

	# def __getitem__(self,key):		
	# 	return self.get_data()[key]

	# def __setitem__(self,key,value):
	# 	self.get_data()[key] = values
	# 	return

	def get(self,string):
		return getattr(self,'get_%s'%(string))()

	def set(self,string,*args,**kwargs):
		getattr(self,'set_%s'%(string))(*args,**kwargs)
		return		

	def set_data(self,data):
		super(dataset,self).__init__(data=data)
		return

	def get_data(self):
		return self.values

	def set_metadata(self,metadata):
		self.metadata = metadata
		return

	def get_metadata(self):
		return self.metadata

	def set_settings(self,settings):
		self.settings = settings
		fields = {'model':'model'}
		for field in fields:
			if field in settings:
				self.set(fields[field],setttings[field])


	def get_settings(self):
		return self.settings

	def set_model(self,model):
		self.model = model

	def get_model(self):
		return self.model




# Define nodes and edges and perform pre-processing of graph
def structure(data,metadata,settings,verbose=False):

	locs = locals()
	locs.update(settings['structure'])
	fields = [
		'directed','rename','round','drop_duplicates','groupby_filter','groupby_equal',
		'index','conditions','samples','functions','filters','symmetries',
		'scale','processed','refinement',]
	defaults = {'directed':False,'index':None,'refinement':None,'scale':{},'processed':False,}

	while not all([metadata[key].get('processed') for key in data]):
		keys = list(data)
		for key in keys:
			if metadata[key].get('processed'):
				continue

			for field in fields:

				try:
					value = locs[field][key]
					if value is None and field in defaults:
						locs[field][key] = defaults[field]
						value = defaults[field]
				except:
					value = locs.get(field)
					if value is None and field in defaults:
						locs[field] = defaults[field]
						value = defaults[field]
				if value is None:
					if field in defaults:
						metadata[key][field] = value
					continue

				elif field == 'directed':
					metadata[key][field] = value

				elif field == 'rename':
					data[key].rename(columns=value,inplace=True)

				elif field == 'round':
					labels = value['labels']
					decs = value['decimals']
					data[key][labels] = data[key][labels].round(decs)

				elif field == 'drop_duplicates':
					data[key] = data[key].drop_duplicates(subset=value['subset'],keep='first')

				elif field == 'groupby_filter':
					try:
						by = value['by']
						df_groupby = data[key].groupby(by)
						_functions = {
							'min_size':lambda func,by,df_groupby: lambda group: group.shape[0] == min(df_groupby.size()),
							'max_size':lambda func,by,df_groupby: lambda group: group.shape[0] == max(df_groupby.size()),
							'min':lambda func,by,df,df_groupby: lambda group: (group[by] == df[by].min()).all(),
							'max':lambda func,by,df,df_groupby: lambda group: (group[by] == df[by].max()).all(),
							'default': lambda func,by,df,df_groupby: lambda group: func(group,df_groupby)}
						func = value['func']
						func = _functions.get(func,_functions['default'])(func,by,data[key],df_groupby)
						data[key] = df_groupby.filter(func=func).copy()
					except:
						pass

				elif field == 'groupby_equal':
					try:
						df_groupby = data[key].groupby(value['by'])				
						ilocs = value['ilocs']
						labels = value['labels']
						if labels is None:
							labels = [labels]
							values = df_groupby.nth(ilocs).values
						else:
							if not isinstance(labels,list):
								labels = [labels]
							values = df_groupby.nth(ilocs)[labels].values
						assert equal_axis(values,0), "Node %d: not equal in all groups %s"%(ilocs,','.join(labels))
					except:
						pass

				elif field == 'index':
					_option = 'functions'
					if locs.get(_option) is not None:
						try:
							_variable = locs[field][key]
						except:
							_variable = locs[field]
						for _var in _variable:
							if value == _var['labels'] or value in _var['labels']:
								data[key][value] = _variable[value](data[key])
					try:

						data[key] = data[key].groupby(value).mean().reset_index()
						data[key].sort_values(value, axis='index', ascending=True, inplace=True, kind='quicksort')
					#     data[key] = data[key].reindex(index=sorter(data[key][value].values,data[key][value].values))
						data[key].set_index(value,drop=False, append=False, inplace=True)
					except:
						pass
					metadata[key][field] = value

				elif field == 'conditions':
					_functions = ['isin','eq','ne','gt','ge','lt','le']
					boolean = conjunction(*[getattr(data[key][condition['labels']],condition['func'])(value['other']) 
											for condition in value if condition['func'] in _functions])
					if boolean is not None:
						data[key].drop(data[key].loc[~boolean].index,inplace=True)


				elif field == 'samples':
					n = data[key].shape[0]
					if isinstance(value,(int,np.integer,float,np.float)):
						inds = np.random.choice(arange(n),size=int(n*(1-value)),replace=False)
						inds = data[key].index[inds]
					elif isinstance(value,np.ndarray):
						inds = delete(arange(n),value[value<n])
						inds = data[key].index[inds]
					elif isinstance(value,(list,slice)):
						inds = data[key].index[value]
					else:
						inds = []

					data[key].drop(inds,axis=0,inplace=True)


				elif field == 'functions':
					for function in value:
						labels = function['labels']
						try:
							data[key][labels] = function['func'](data[key])
						except:
							pass


				elif field == 'filters':
		
					for f in value:
						labels = f['labels']
						if f['type'] in ['rolling']:
							if labels is None:
								labels = list(data[key].columns)
							passes = iscallable(f['passes'],data[key])
							for n in range(passes):
								rolling_kwargs = iscallable(f['rolling_kwargs'],data[key],n)
								mean_kwargs = iscallable(f['mean_kwargs'],data[key],n)
								data[key][labels] = data[key][labels].rolling(**rolling_kwargs).mean(**mean_kwargs)
						elif f['type'] in ['filtering']:
							for label in labels:
								data[key][label],_,_ = filtering(data[key][label].values,**f['filtering_kwargs'])

						n = data[key].shape[0]
						inds = f.get('dropinds')
						if inds is not None:
							if isinstance(inds,(int,np.integer,float,np.float)):
								inds = np.random.choice(arange(n),size=int(n*(1-inds)),replace=False)
								inds = data[key].index[inds]
							elif isinstance(inds,np.ndarray):
								inds = delete(arange(n),inds[inds<n])
								inds = data[key].index[inds]
							elif isinstance(inds,(list,slice)):
								inds = data[key].index[inds]
							else:
								inds = []
							data[key].drop(inds,axis=0,inplace=True)								

				elif field == 'symmetries':
					df_ = [df.copy()]
					for symmetry in value:
						df_.append(data[key].copy())
						df_[-1].rename(columns=symmetry['labels'],inplace=True)
					data[key] = pd.concat(df_,ignore_index=True,axis=0)

				elif field == 'scale':
					_labels = data[key].select_dtypes(exclude=(object,int)).columns.values.tolist()
					if value.get('labels') is True:
						labels = _labels
					else:
						labels = value.get('labels',[])
					labels = [label for label in labels if label in _labels]

					if len(labels)>0:
						scales = data[key][labels].abs().max(axis=0).values
						scales[scales==0] = 1
						data[key][labels] /= scales
						scales = {k:v for k,v in zip(labels,scales)}
					else:
						scales = {}
					metadata[key][field] = scales
				

				elif field == 'processed':
					metadata[key][field] = True					


				elif field == 'refinement':

					if metadata[key]['type'] in ['refined']:
						continue



					# Get indices of refined meshes, with training and testing sets for each mesh refinement
					n = data[key].shape[0]
					powers = refinement(int(n**(1/value['p'])),value['p'],value['base'],powers=value['powers'],string=key,boundary=False)

					# Get list of settings keys for updating settings dictionary with new dataset meshes
					_settings = value['settings']
					_settings = [] if _settings is None else _settings 
					_settings.extend([_setting for _setting in ['sys__files','terms','model','terms__parameters','fit__info','plot__settings'] 
									if _setting not in _settings])


					# Update data, metadata, settings with new datasets and keys based on refinement keys	
					for j,power in enumerate(powers):

						# Update powers for training and testing data with new _key, where _key = key__k__i where data size has been refined n -> n/k 
						# and i represents the offset for the datasets, either indices {0,k,2k,...,n} (training) or {k/2,3k/2,...,n-k/2} (testing)
						for i,_key in enumerate(powers[power]):

							data[_key] = data[key].iloc[powers[power][_key]].copy()
							_replace(metadata,key,_key,_append=True,_values=True,_copy=True)				
							metadata[_key]['processed'] = False
							metadata[_key]['type'] = 'refined'
							metadata[_key]['refinement'] = powers[power][_key] 

						# Update each of _settings based on new keys from refinement
						for _setting in _settings:

							setting = _get(settings,_setting,_split=DELIMITER)							
							
							# Update specific (possibly key-dependent) settings

							# Update system label for saving data,metadata
							if _setting in ['sys__files']:
								# Add _key to any key dependent settings with the setting associated with key
								for i,_key in enumerate(powers[power]):

									_replace(setting,key,_key,_append=(value['keep'] or i < (len(powers[power])-1) or j < (len(powers)-1)),_values=True,_copy=True)				

									# Update certain parameters based on whether training and testing data

									# For data,metadata:
									#   Append _key postfix to file names to prevent overwriting

									params = {param:setting[param].replace(param,'%s%s'%(param,_key.replace(key,''))) if isinstance(setting[param],str) else setting[param][_key].replace(param,'%s%s'%(param,_key.replace(key,''))) 
												for param in ['data','metadata']}
									update = {param: True for param in params}

									for param in params:
									
										if not update[param] or param not in setting:
											continue

										# Make settings key-dependent
										if (not isinstance(setting[param],dict) or _key not in setting[param]):
											setting[param] = {k: copy.deepcopy(setting[param]) for k in data}											


										setting[param].update({_key:copy.deepcopy(params[param])})

							# Update terms and model and terms__parameters that may be key-dependent
							elif _setting in ['terms','model','terms__parameters']:
				
								# Add _key to any key dependent settings with the setting associated with key
								for i,_key in enumerate(powers[power]):

									_replace(setting,key,_key,_append=(value['keep'] or i < (len(powers[power])-1) or j < (len(powers)-1)),_values=True,_copy=True)				

									# Update certain parameters based on whether training and testing data

									# For iloc:
									# 	True if training data (allows all ilocs to be used for fitting)
									#	None if testing data (allows closest iloc to be found in training data for predicting)

									params = {'iloc':{0:True,1:None}.get(i,None)}
									update = {'iloc':{0:False,1:True}.get(i,False)}
									for param in params:
										if not update[param] or param not in setting:
											continue

										# Make settings key-dependent
										if (not isinstance(setting[param],dict) or _key not in setting[param]):
											setting[param] = {k: copy.deepcopy(setting[param]) for k in data}											


										setting[param].update({_key:copy.deepcopy(params[param])})
										
							# Update fit type based on training (fit) or testing (interpolate) datasets
							elif _setting in ['fit__info']:

								params = dict(zip(['fit','interpolate'],powers[power]))
								for param in params:
									if param not in setting:
										setting[param] = {}
									setting[param].update({
										params[param]:{
										'type':{'fit':'fit','interpolate':'fit'}.get(param,'fit'),
										'key':params[{'fit':'fit','interpolate':'fit'}.get(param,'fit')],
										'keys':[params[param]],								
										} for i,_key in enumerate(powers[power])
									})
									if not value['keep'] and key in setting[param]:
										setting[param].pop(key)


							# Update plot (possibly key-dependent) [lot settings]
							elif _setting in ['plot__settings']:
								for i,_key in enumerate(powers[power]):
									_replace(setting,key,_key,_append=(value['keep'] or i < (len(powers[power])-1) or j < (len(powers)-1)),_values=True,_copy=True)
							
							# Update other settings and add new _key settings
							else:
								for i,_key in enumerate(powers[power]):
									_replace(setting,key,_key,_append=(value['keep'] or i < (len(powers[power])-1) or j < (len(powers)-1)),_values=True,_copy=True)

					if len(powers)>0 and not value['keep']:
						data.pop(key);
						metadata.pop(key);

						continue	



	return



# Save data
def save(settings,paths,**objs):
	for name in objs:
		obj = objs[name]
		files = {key: settings['sys']['files'][name].get(key,name) if isinstance(settings['sys']['files'][name],dict) else settings['sys']['files'][name]
				 for key in paths}
		exts = {key: settings['sys']['ext'][name].get(key,'pickle') if isinstance(settings['sys']['ext'][name],dict) else settings['sys']['ext'][name]
				 for key in paths}
		wrs = {key: settings['sys']['write'][name].get(key,'w') if isinstance(settings['sys']['write'][name],dict) else settings['sys']['write'][name]
		 for key in paths}
		kwargs = {key: settings['sys']['kwargs']['dump'].get(key,settings['sys']['kwargs']['dump']) if isinstance(settings['sys']['kwargs']['dump'],dict) else settings['sys']['kwargs']['dump']
				 for key in paths}
		for key in paths:
			path = path_join(paths[key],files[key],ext=exts[key])
			try:
				dump(obj[key],path,wr=wrs[key],**kwargs[key])				
			except Exception as e:
				pass



def analysis(data,metadata,settings,models,texify=None,verbose=False):
	'''
	Analyse results of model
	Args:
		data (dict): Dictionary of pandas dataframes datasets
		metadata (dict): Dictionary of metadata for each dataset
		settings (dict): Dictionary of library settings
		texify (Texify,callable,None): Texify function or callable for rendering strings in Latex
		verbose (str): Print out additional information
	'''
	def replace(string,replacements,conditions=None):
		'''
		Replace substrings in string
		Args:	
			string (str): String to be updated
			replacements (dict): Dictionary of replacements
			conditions (list,None): List of conditions on substrings to perform replacements
		Returns:
			string with replacements
		'''
		for replace in replacements:
			if conditions is None or any([condition in string for condition in conditions]):
				string = string.replace(replace,replacements[replace])
		return string


	labels = list(models)
	keys = list(data)
	types = settings['fit']['types']



	if texify is None:
		texify = lambda string: str(string).replace(DELIMITER,' ').replace('$','')

	for ilabel,label in enumerate(labels):
		dims = arange(max([len(metadata[key]['rhs_lhs'].get(label,{}).get('lhs',[])) for key in data]))[settings['plot'].get('dims',slice(None))]
		for idim,dim in enumerate(dims):	
			for key in keys:
				r = metadata[key]['rhs_lhs'][label]['rhs']
				l = metadata[key]['rhs_lhs'][label]['lhs']
				if verbose == 2:
					logger.log(verbose,'LABEL: %s',label)
					logger.log(verbose,'RHS: %r'%r)
					logger.log(verbose,'LHS: %r\n'%l)
				for ityped,typed in enumerate(types):
					label_dim = DELIMITER.join([label,typed,'%d'%(dim)])							
					if (typed not in metadata[key]) or (label_dim not in metadata[key][typed]):
						continue
					
					stats = metadata[key][typed][label_dim]['stats']			

					iloc = {
						'indiv':metadata[key]['iloc'][dim%len(metadata[key]['iloc'])] if metadata[key]['iloc'] not in [None] else [metadata[key]['iloc']],
						'multi':metadata[key]['iloc'][dim%len(metadata[key]['iloc'])] if metadata[key]['iloc'] not in [None] else [metadata[key]['iloc']]
						}[settings['model']['approach']]
					Niterations = stats['iterations'].max()
					iterations = [None,*list(range(10-1,0,-1))]
					iterations = [Niterations-(Niterations if i is None else i) for i in iterations if i is None or i<=Niterations]
					iterations = list(sorted(list(set(iterations)),key=lambda i: iterations.index(i)))
					slices = slice(-1,-10-1,-1)
					



					# Latex model with symbolic coefficients
					values = {}



					for i in iterations:

						values['model_%d'%(i)] = r'%s = %s'%(
													texify(l[dim].replace('partial','delta')).replace('partial','delta'),
													r'\\'.join([r' + '.join(['%s%s'%(texify(o.replace('partial','delta')
																if 'taylorseries' in o else DELIMITER.join(['coefficient',str(None),o.replace('partial','delta')]) if 'constant' not in o else '').replace('partial','delta'),
																texify(o.replace('partial','delta')).replace('partial','delta') if 'taylorseries' not in o else '') 
																for o in stats['ordering'][slices] if stats['coef_'][o][i] != 0.0]) 
																])
													)			


						values['model_%d'%(i)] = r'$%s$'%(values['model_%d'%(i)].replace('$','').replace('\n','').replace(' + -',' - '))



					# values['elbow'] = np.array([*(np.abs((((stats['loss'][:-1]-stats['loss'][1:])/stats['loss'][:-1])))>0.04),0],dtype=bool)


					# Latex model with numeric coefficients					
					for i in iterations:
						values['coef_%d'%(i)] = r'%s = %s'%(
													texify(l[dim].replace('partial','delta')).replace('partial','delta'),
													r'\\'.join([r' + '.join(['%s%s'%(texify(stats['coef_'][o][i]) if stats['coef_'][o][i] != 1 else '',texify(o.replace('partial','delta')).replace('partial','delta')) 
																for o in stats['ordering'][slices] if stats['coef_'][o][i] != 0.0]) 
																])
													)
						values['coef_%d'%(i)] = r'$%s$'%(values['coef_%d'%(i)].replace('$','').replace('\n','').replace(' + -',' - '))

					fields = {'model':[*['model_%d'%(i) for i in iterations],*['coef_%d'%(i) for i in iterations]]}

					if verbose in ['info','warning','error','critical']:
						logger.log(verbose,'Fit: %s %s'%(key,l[dim]))
						logger.log(verbose,'Loss: %s\n'%(stats['loss'][::-1]))
						for i,k in enumerate(values):
							if all([k not in fields[field] for field in fields]):
								logger.log(verbose,'%s: %r%s'%(k.capitalize(),values[k],'' if i<(len(values)-1) else '\n\n\n\n'))

					if settings['boolean']['dump']:
						for field in fields:
							file = settings['sys']['files'][field]
							file = file%(field,DELIMITER.join([l[dim],'iloc',str(iloc)])) if file.count('%')==2 else file%(l[dim]) if file.count('%')==1 else file
							path = path_join(metadata[key]['directory']['dump'],file,ext=settings['sys']['ext'][field])
							for i,f in enumerate(fields[field]):
								wr = settings['sys']['write'][field] if i==0 else 'a'
								dump(values[f],path,wr=wr,**settings['sys']['kwargs']['dump'])
								dump('\n',path,wr='a',**settings['sys']['kwargs']['dump'])




					# Analyze derivatives and error
					field = 'analysis'
					subfield = 'parameters'
					parameters = settings.get(field,{}).get(subfield)

					if typed not in ['interpolate','predict'] or parameters is None:
						continue


					iteration = 0

					N = data[key].shape[0]
					p = parameters['p']
					n = int(N**(1/p))
					q = parameters['q']
					r = parameters['accuracy']
					unique = parameters['unique']
					order = parameters['order']
					K = parameters['K']
					h = np.mean(stats['distances'])/(p**(1/2))
					L = h*(2*n)
					inputs = parameters['inputs']
					outputs = parameters['outputs']
					refine = stats['refinement_inherited']
					manifold = {i:''.join([r'{%s^{%s}}'%(inputs[k] if j>0 else '',str(j) if j>1 else '') for k,j in enumerate(u)]) 
								for i,u in enumerate(combinations(p,r,unique=unique))}

					rhs = stats['rhs'][dim]
					lhs = stats['lhs'][dim]

					rhs_pred = list(stats['coef_'])
					lhs_pred = '%s%s%d'%(label_dim,DELIMITER,stats['size_'].max()-iteration)


					y = data[key][lhs].values

					if lhs_pred in data[key]:
						y_pred = data[key][lhs_pred].values
					else:
						y_pred = np.array([])

					X = data[key][inputs].values


					field = 'derivative'
					if parameters.get(field) is not None:
						derivative = parameters[field][refine][:,:,dim]
						derivative_pred = data[stats['key']][rhs].values
					else:
						derivative = np.array([])
						derivative_pred = np.array([])

					gamma = np.array([stats['coef_'][r][iteration] for r in stats['coef_']]).T

					field = 'indices'
					if parameters.get(field) is not None:
						indices = parameters[field]
					else:
						indices = np.array([])
					
					field = 'alpha'
					if parameters.get(field) is not None:
						alpha = parameters[field]
						alpha = alpha[:,dim]
						# alpha = alpha.ravel().tolist() if len(set(alpha.ravel().tolist()))>1 else (alpha.ravel()[0] if not int(alpha.ravel()[0])==alpha.ravel()[0] else int(alpha.ravel()[0]))					
					else:
						alpha = np.array([])

					# slices = boundaries(X,size=settings['model']['neighbourhood'],excluded=[0])
					slices = slice(None)


					def localerror(y,y_pred): 
						try:
							out = np.abs(y - y_pred)
						except:
							out = np.array([])
						return out
					def error(y,y_pred,axis=None,ord=2,slices=slice(None)): 
						try:
							out = norm(localerror(y,y_pred)[slices],axis=axis,ord=ord)/((max(1,y[slices].size))**(1./ord))
						except:
							out = np.array([])
						return out

					name = parameters['funcstring']
					params = {
						'h':h,
						'n':n,
						'p':p,
						'q':q,
						'r':r,
						'K':K,
						'order':order,
						'L':L,
						'manifold':manifold,
						'funcstring': parameters['funcstring'],
						'modelstring': parameters['modelstring'],
						'operator':parameters['operator'],
						'indices':parameters['indices'],
						'error': error(y,y_pred,ord=2,slices=slices),
						'localerror': localerror(y,y_pred),
						'gamma':gamma,
						'alpha':alpha,
						'derivativeerror': error(derivative,derivative_pred,axis=0,ord=2,slices=slices).T,
						'localderivativeerror':localerror(derivative,derivative_pred).T,
						'derivative':derivative.T,
						 }



					field = 'analysis'
					file = settings['sys']['files'][field]
					file = file%('%s_p%d_n%d_K%d_k%d'%(stats['lhs'][0],p,parameters['N'],K,order))
					path = path_join(metadata[key]['directory']['dump'],file,ext=settings['sys']['ext'][field])
					rw = settings['sys']['read'][field] 
					wr = settings['sys']['write'][field] 




					values = load(path,wr=rw,default={})


					if name in values:
						values[name].append(params)
					else:
						values[name] = [params]
					
					dump(values,path,wr=wr)



					continue
					# Plot Operators

					import matplotlib
					import matplotlib.pyplot as plt
					matplotlib.use('TkAgg')
					matplotlib.rcParams['text.usetex'] = True
					matplotlib.rcParams['font.size'] = 40


					slices = slice(None)


					if p == 1:
						fig,ax = plt.subplots()
					else:
						fig,ax = plt.figure(),plt.axes(projection='3d')

					
					if p == 1:
						O = [[0],[1],[2]]
						O = [[1]]
					else:
						_O_ = [0 for i in range(p)]
						O = [[1,0],[0,1],[1,1]]
						# O = [[1,0],[0,1]]
						O = [[1,0],[0,1],[1,1],[2,0],[0,2]]#[2,1],[1,2],[3,0],[0,3]]
						O = [[1,1]]#,[0,1],[1,1],[2,0],[0,2]]#[2,1],[1,2],[3,0],[0,3]]



					plots = []
					for o in O:

						if o not in indices:
							continue

						i = indices.index(o)	
						j = inputs.index(variable)

						x = X[slices]

						partial = derivative[slices,i]
						delta = derivative_pred[slices,i]

						partiallabel = r'$\frac{\partial^{%s}u}{\partial x^{%s}}$'%(str(sum(o)) if sum(o)>1 else '',str(''.join([r'{%s}_{%s}^{%s}'%(r'\mu' if len(o)>1 else u,l if len(o)>1 else '',u if len(o)>1 and u>1 else '') for l,u in enumerate(o) if u>0])) if (len(o)==1 and sum(o)>1) or (len(o)>1) else str(o[0]) if o[0]>1 else '') if i>0 else r'$u_{}$'
						deltalabel = (r'$\frac{\delta^{%s}u}{\delta x^{%s}}$'%(str(sum(o)) if (sum(o)>1) else '',str(''.join([r'{%s}_{%s}^{%s}'%(r'\mu' if len(o)>1 else u,l if len(o)>1 else '',u if len(o)>1 and u>1 else '') for l,u in enumerate(o) if u>0])) if (len(o)==1 and sum(o)>1) or (len(o)>1) else str(o[0]) if (o[0]>1) else '')) if i>0  else r'$u_{}$'


						if p == 1:

							plotter = 'plot'

							shape = {'plot':(-1)}[plotter]

							x,z,w = x[:,0].reshape(shape),partial.reshape(shape),delta.reshape(shape)

							if o == [0]:
								func = lambda x: 1 + x + x**2 + x**3 + x**4 + x**5 + x**6 + x**7 + x**8
							elif o == [1]:
								func = lambda x: 1 + 2*x + 3*x**2 + 4*x**3 + 5*x**4 + 6*x**5 + 7*x**6 + 8*x**7
							elif o == [2]:
								func = lambda x: 2 + 6*x + 12*x**2 + 20*x**3 + 30*x**4 + 42*x**5 + 56*x**6

							# plots.append(ax.plot(x,w,label=r'$\frac{\delta^{%s}u}{\delta x^{%s}}$'%(str(i) if i>1 else '',str(i) if i>1 else '') if i>0 else r'$u_{}$'))
							# plots.append(ax.plot(x,z,label=r'$\frac{\partial^{%s}u}{\partial x^{%s}}$'%(str(i) if i>1 else '',str(i) if i>1 else '') if i>0 else r'$u_{}$'))
							plots.append(getattr(ax,plotter)(x,np.abs(func(x)-w),label=r'%s - %s'%(deltalabel,partiallabel)))



						else:
							plotter = 'plot_surface'
							m = int(np.sqrt(x.shape[0]))
							shape = {'plot_surface':(m,-1),'plot_trisurf':(-1)}[plotter]

							x,y,z,w = x[:,0].reshape(shape),x[:,1].reshape(shape),partial.reshape(shape),delta.reshape(shape)

							
							if o == [0,0]:
								func = lambda x,y: x**3 + y**3 + x**2*y + x*y**2 + x**2 + y**2 + x*y + x + y
							elif o == [1,0]:
								func = lambda x,y: 3*x**2 + 2*x*y + y**2 + 2*x + y + 1
							elif o == [0,1]:
								func = lambda x,y: 3*y**2 + 2*x*y + x**2 + 2*y + x + 1
							elif o == [1,1]:
								func = lambda x,y: 2*x + 2*y + 1
							elif o == [2,0]:
								func = lambda x,y: 6*x + 2*y + 2
							elif o == [0,2]:
								func = lambda x,y: 6*y + 2*x + 2
							elif o == [3,0]:
								func = lambda x,y: 6
							elif o == [0,3]:
								func = lambda x,y: 6
							elif o == [2,1]:
								func = lambda x,y: 2
							elif o == [1,2]:
								func = lambda x,y: 2

							# ax.plot(x,partial,label=r'$\frac{\partial^{%s}u}{\partial x^{%s}}$'%(str(i) if i>1 else '',str(i) if i>1 else '') if i>0 else r'$u_{}$')
							# ax.plot(x,delta,label=r'$\frac{\delta^{%s}u}{\delta x^{%s}}$'%(str(i) if i>1 else '',str(i) if i>1 else '') if i>0 else r'$u_{}$')
							# plots.append(ax.plot_trisurf(x[:,0],x[:,1],np.abs(delta-partial),label=r'$\frac{\delta^{%s}u}{\delta x^{%s}}$'%(str(i) if i>1 else '',str(i) if i>1 else '') if i>0 else r'$u_{}$'))
							# plots.append(ax.plot_surface(x,y,z,label=r'$\frac{\delta^{%s}u}{\delta x^{%s}}$'%(str(i) if i>1 else '',str(i) if i>1 else '') if i>0 else r'$u_{}$'))
							# plots.append(getattr(ax,plotter)(x,y,w,label=deltalabel))
							# plots.append(getattr(ax,plotter)(x,y,z,label=partiallabel))
							# plots.append(getattr(ax,plotter)(x,y,w-z,label=r'%s - %s'%(deltalabel,partiallabel)))
							# plots.append(getattr(ax,plotter)(x,y,z-func(x,y),label=r'%s - %s'%(deltalabel,partiallabel)))
							plots.append(getattr(ax,plotter)(x,y,func(x,y),label=r'%s - %s'%(deltalabel,partiallabel)))


					for plot in plots:
						try:
							plot._facecolors2d = plot._facecolor3d 
							plot._edgecolors2d = plot._edgecolor3d
							ax._facecolors2d = ax._facecolor
						except:
							pass

					ax.legend(loc=(1.55,0.15),prop={'size':20})
					ax.set_title(('\n').join([
							r'$u_{} = \sum_{q=0}^{%d}\sum_{\mu = \{\mu_{1}\cdots\mu_{p=%d}\} : |\mu| = q} \alpha_{q_{\mu_{1}\cdots\mu_{p}}} x^{\mu_{1}\cdots\mu_{p}}$'%(K,p),
							r'$\alpha_q = %s$'%('[%s]'%(', '.join([r'%s~%s'%(scinotation(a,scilimits=[-2,1]),''.join([r'{x}^{{%s}^{%s}}'%(str(l) if len(j)>1 else str(u),str(u) if u>1 and len(j)>1 else '') if u>0 else '' for l,u in enumerate(j)])) + ('\\' if t==(len(indices)//2) else '')
							 for t,(j,a) in enumerate(zip(indices,alpha)) if a!= 0])) if isinstance(alpha,list) else str(alpha))
							]),pad=50)

					if p == 1:
						ax.set_xlabel(r'$x_{}$')
						ax.set_ylabel(r'$u_{}$')
						ax.set_yscale('log')

						ax.grid(True,alpha=0.6)
						
						fig.set_size_inches(10,10)
						fig.tight_layout()

						file = DELIMITER.join(['.'.join(path.split('.')[:-1]),'derivatives'])
						ext = 'pdf'
						path = '%s.%s'%(file,ext)
						fig.savefig(path,bbox_inches='tight')
					
					else:
						ax.set_xlabel(r'$x_{}$',labelpad=50)
						ax.set_ylabel(r'$y_{}$',labelpad=50)
						ax.set_zlabel(r'$u_{}$',labelpad=50)
						ax.set_zscale('linear')
						ax.grid(True,alpha=0.6)
						
						fig.set_size_inches(10,10)
						# fig.tight_layout()
						# fig.show()
						# plt.pause(100)
						# file = '.'.join(path.split('.')[:-1])
						# ext = 'pdf'
						# path = '%s.%s'%(file,ext)
						# fig.savefig(path,bbox_inches='tight')						




# Metrics
def metrics():

	def euclidean(x,y,ord=2,axis=-1):
		out = similarity_matrix(x,y,metric=ord,sparsity=None,directed=False)
		return out 

	locs = locals()
	funcs = {k:f for k,f in locs.items() if callable(f) and not k.startswith('_')}	

	return funcs


# Adjacencies
def adjacencies():
	'''
	Returns dictionary with functions for adjacency matrices, based on weights function and conditions for adjacency matrix depending on connectivity
	Args:
		data (ndarray): data to compute adjacency matrix of shape (n,p)
		parameters (dict): parameters for adjacency conditions
	Returns:
		weights (callable): callable weight function that accepts ndarray as data
		conditions (int,tuple,list,None): integer for k nearest neighbours, or tuple of (open) bounds on nearest neighbours, list of functions to make adjacency elements non-zero		
		out (int,float,ndarray,None): Return type of adjacency matrix of either out value, or weights value if None
		a (ndarray): adjacency matrix of shape (n,n)
	'''

	def _wrapper(func):
		@functools.wraps(func)
		def _func(data,parameters):
			weights,conditions = func(data,parameters)
			n = parameters['n']
			tol,atol,rtol = parameters['tol'],parameters['atol'],parameters['rtol']
			kind = parameters['kind']
			format = parameters['format']
			dtype = parameters['dtype']
			verbose = parameters['verbose']
			
			adjacency = adjacency_matrix(n=n,weights=weights,conditions=conditions,
							tol=tol,atol=atol,rtol=rtol,kind=kind,diagonal=False,format=format,dtype=dtype,verbose=verbose)
			
			return adjacency,weights
		return _func


	@_wrapper
	def default(data,parameters):
		def weights(data,parameters):
			metric = parameters['metric']
			format = parameters['format']
			dtype = parameters['dtype']
			chunk = parameters['chunk']
			n_jobs = parameters['n_jobs']
			verbose = parameters['verbose']				
			weights = similarity_matrix(data,data,metric=metric,directed=False,format=format,dtype=dtype,verbose=verbose,chunk=chunk,n_jobs=n_jobs)
			return weights
		def conditions(data,parameters):
			condition = None
			return condition
		returns = (weights(data,parameters),conditions(data,parameters))
		return returns

	@_wrapper
	def forward_all(data,parameters):
		def weights(data,parameters):
			p = parameters['p']
			n = parameters['n']
			sparsity = n
			metric = parameters['metric']			
			chunk = parameters['chunk']			
			format = parameters['format']
			dtype = parameters['dtype']
			n_jobs = parameters['n_jobs']
			verbose = parameters['verbose']
			weights = similarity_matrix(data,data,metric=metric,directed=True,sparsity=sparsity,format=format,dtype=dtype,verbose=verbose,chunk=chunk,n_jobs=n_jobs)
			if issparse(weights):				
				fmt = 'csr'
				format = weights.getformat()
				weights = weights.asformat(fmt)
				mask = lt(weights,0)
				for i in range(n):
					if mask[i].indices.size == weights[i].indices.size-1:
						weights[i] = weights[i].multiply(-1)
				weights = weights.asformat(format)
			else:
				mask = where(prod(lt(weights,0),axis=1))				
				weights[mask] *= -1
			weights += 0
			weights = explicit_zeros(weights,indices=[np.array([i]) for i in range(n)])			
			return weights
		def conditions(data,parameters):
			def condition(weights,argsort,counts):
				return gt(weights,0)
			return condition
		returns = (weights(data,parameters),conditions(data,parameters))
		return returns

	@_wrapper
	def forward_nearest(data,parameters):
		def weights(data,parameters):
			p = parameters['p']
			n = parameters['n']
			sparsity = 2*p+1
			metric = parameters['metric']			
			chunk = parameters['chunk']			
			format = parameters['format']
			dtype = parameters['dtype']
			n_jobs = parameters['n_jobs']
			verbose = parameters['verbose']			
			weights = similarity_matrix(data,data,metric=metric,directed=True,sparsity=sparsity,format=format,dtype=dtype,verbose=verbose,chunk=chunk,n_jobs=n_jobs)
			if issparse(weights):				
				fmt = 'csr'
				format = weights.getformat()
				weights = weights.asformat(fmt)
				mask = lt(weights,0)
				for i in range(n):
					if mask[i].indices.size == weights[i].indices.size-1:
						weights[i] = weights[i].multiply(-1)
				weights = weights.asformat(format)
			else:
				mask = where(prod(lt(weights,0),axis=1))				
				weights[mask] *= -1
			weights += 0
			weights = explicit_zeros(weights,indices=[np.array([i]) for i in range(n)])			
			return weights
		def conditions(data,parameters):
			def condition(weights,argsort,counts):
				return multiply(gt(weights,0),eq(argsort,1))
			return condition
		returns = (weights(data,parameters),conditions(data,parameters))
		return returns

	@_wrapper
	def backward_all(data,parameters):
		def weights(data,parameters):
			p = parameters['p']
			n = parameters['n']
			sparsity = n			
			metric = parameters['metric']			
			chunk = parameters['chunk']			
			format = parameters['format']
			dtype = parameters['dtype']
			n_jobs = parameters['n_jobs']
			verbose = parameters['verbose']			
			weights = similarity_matrix(data,data,metric=metric,directed=True,sparsity=sparsity,format=format,dtype=dtype,verbose=verbose,chunk=chunk,n_jobs=n_jobs)
			mask = where(prod(ge(weights,0),axis=1))
			if issparse(weights):				
				fmt = 'csr'
				format = weights.getformat()
				weights = weights.asformat(fmt)
				mask = gt(weights,0)
				for i in range(n):
					if mask[i].indices.size == weights[i].indices.size-1:
						weights[i] = weights[i].multiply(-1)
				weights = weights.asformat(format)
			else:
				mask = where(prod(gt(weights,0),axis=1))				
				weights[mask] *= -1
			weights += 0
			weights = explicit_zeros(weights,indices=[np.array([i]) for i in range(n)])			
			return weights
		def conditions(data,parameters):
			def condition(weights,argsort,counts):
				return lt(weights,0)
			return condition
		returns = (weights(data,parameters),conditions(data,parameters))
		return returns

	@_wrapper
	def backward_nearest(data,parameters):
		def weights(data,parameters):
			p = parameters['p']
			n = parameters['n']
			sparsity = 2*p+1
			metric = parameters['metric']			
			format = parameters['format']
			dtype = parameters['dtype']
			chunk = parameters['chunk']
			n_jobs = parameters['n_jobs']
			verbose = parameters['verbose']			

			weights = similarity_matrix(data,data,metric=metric,directed=True,sparsity=sparsity,format=format,dtype=dtype,verbose=verbose,chunk=chunk,n_jobs=n_jobs)
			
			if issparse(weights):				
				fmt = 'csr'
				format = weights.getformat()
				weights = weights.asformat(fmt)
				mask = gt(weights,0)
				for i in range(n):
					if mask[i].indices.size == weights[i].indices.size-1:
						weights[i] = weights[i].multiply(-1)
				weights = weights.asformat(format)
			else:
				mask = where(prod(gt(weights,0),axis=1))
				weights[mask] *= -1
			weights += 0
			weights = explicit_zeros(weights,indices=[np.array([i]) for i in range(n)])
			return weights
		def conditions(data,parameters):
			def condition(weights,argsort,counts):
				return multiply(lt(weights,0),eq(argsort,1))
			return condition
		returns = (weights(data,parameters),conditions(data,parameters))
		return returns

	@_wrapper
	def nearest(data,parameters):
		def weights(data,parameters):
			p = parameters['p']
			n = parameters['n']
			accuracy = parameters['accuracy']
			metric = parameters['metric']			
			chunk = parameters['chunk']
			format = parameters['format']
			dtype = parameters['dtype']		
			unique = parameters['unique']
			neighbourhood = parameters.get('neighbourhood',ncombinations(p,accuracy,unique=unique)-1)			
			sparsity = parameters.get('sparsity',2*neighbourhood*p)
			n_jobs = parameters['n_jobs']
			verbose = parameters['verbose']
			weights = similarity_matrix(data,data,metric=metric,sparsity=sparsity,directed=False,chunk=chunk,format=format,dtype=dtype,verbose=verbose,n_jobs=n_jobs)
			return weights	
		def conditions(data,parameters):
			n,p = data.shape[:2]
			n = min(parameters['n'],n)
			p = min(parameters['p'],p)
			chunk = parameters['chunk']
			accuracy = parameters['accuracy']
			unique = parameters['unique']			
			size = parameters.get('neighbourhood',ncombinations(p,accuracy,unique=unique)-1)
			strict = parameters['strict']
			atol = parameters['atol']
			rtol = parameters['rtol']
			kind = parameters['kind']
			argsortbounds = [None,None,None]
			weightbounds = [None,None,[0]]

			# condition = nearest_neighbourhood(size=size,
			# 								strict=strict,
			# 								atol=atol,rtol=rtol,kind=kind,
			# 								argsortbounds=argsortbounds,weightbounds=weightbounds)
			# def condition(weights,argsort,counts):
			# 	out = weights.astype(bool)
			# 	out.eliminate_zeros()
			# 	return out

			condition = None

			return condition
		returns = (weights(data,parameters),conditions(data,parameters))
		return returns


	@_wrapper
	def complete(data,parameters):
		def weights(data,parameters):
			metric = parameters['metric']			
			format = parameters['format']
			dtype = parameters['dtype']
			chunk = parameters['chunk']		
			n_jobs = parameters['n_jobs']
			verbose = parameters['verbose']			
			weights = similarity_matrix(data,data,metric=metric,directed=False,format=format,dtype=dtype,verbose=verbose,chunk=chunk,n_jobs=n_jobs)
			return weights
		def conditions(data,parameters):
			condition = None
			return condition
		returns = (weights(data,parameters),conditions(data,parameters))
		return returns


	@_wrapper
	def custom(data,parameters):
		returns = tuple((parameters[f](data,parameters) for f in ['weights','conditions']))
		return returns



	locs = locals()
	funcs = {k: f for k,f in locs.items() if callable(f) and not k.startswith('_')}	
	funcs[None] = default
	return funcs



# Operators
def operations():
	
	def _wrapper(func):
		@functools.wraps(func)		
		def _func(x,y,weights,adjacency):

			# Update adjacency to have identical sparsity structure as weights
			adjacency = twin(adjacency,weights)

			out = func(x,y,weights,adjacency)
			if issparse(out):
				out = out.A
			return out
		return _func

	def _dy(x,y,weights,adjacency):
		out = subtract(y,y,where=adjacency)
		return out

	def _dx(x,y,weights,adjacency):
		out = subtract(x,x,where=adjacency)
		return out
	
	@_wrapper
	def constant(x,y,weights,adjacency):
		return y

	@_wrapper	
	def partial(x,y,weights,adjacency):
		out = add(multiply(_dy(x,y,weights,adjacency),_dx(x,y,weights,adjacency),weights),axis=1,size=True)
		return out

	@_wrapper	
	def divergence(x,y,weights,adjacency):
		out = add(multiply(_dy(x,y,weights,adjacency),sqrt(weights)),axis=1,size=True)
		return out

	@_wrapper	
	def laplacian(x,y,weights,adjacency):
		out = add(multiply(_dy(x,y,weights,adjacency),weights),axis=1,size=True)
		return out

	locs = locals()
	funcs = {k:f for k,f in locs.items() if callable(f) and not k.startswith('_')}	

	return funcs
	


# Operator weight - i.e. weight calculation
def weightings():

	def _wrapper(func):
		@functools.wraps(func)
		def _func(data,dimension,accuracy,adjacency,manifold,parameters):

			def update(exception,*args,**kwargs):
				kwargs['parameters']['sparsity'] *= parameters['catch_factor']
				return

			exceptions = (AssertionError,)
			raises = (ValueError,TypeError)
			iterations = parameters['catch_iterations']



			@catch(update,exceptions,raises,iterations)
			def results(data=None,adjacency=None,parameters={}):

				# Get adjacency
				if callable(adjacency):
					adj,wgts = adjacency(data,parameters)
				elif adjacency is None or isinstance(adjacency,str):
					adj,wgts = adjacencies()[adjacency](data,parameters)
				else:
					adj,wgts = adjacency
				if not (isnone(adj) or isarray(adj)):
					raise ValueError("Error - adjacency matrix is not array or None")

				# Get weights
				weights = func(data,adj,wgts,parameters)	

				return weights,adj


			# Update parameters
			assert data.ndim == 2, "Error - incorrect weights data shape %r"%(tuple(data.shape))

			parameters['n'] = data.shape[0]
			parameters['p'] = data.shape[1]
			parameters['dimension'] = dimension
			parameters['accuracy'] = accuracy
			parameters['manifold'] = manifold
			parameters['neighbourhood'] = min(parameters['neighbourhood'],
											ncombinations(parameters['p'],parameters['accuracy'],
															unique=parameters['unique'])-1)	
			constants = {}
			for const in constants:
				value = parameters.get(const,constants[const])
				if callable(value):
					parameters[const] = value(**parameters)
				else:
					parameters[const] = value


			# Get weights and adjacency matrices, catching exceptions and updating parameters until no exceptions
			weights,adjacency = results(data=data,adjacency=adjacency,parameters=parameters)

			return weights,adjacency

		return _func


	@_wrapper
	def decay(data,adjacency,weights,parameters):
		parameters['r'] = metrics()['euclidean'](data,data)
		parameters['R'] = parameters.get('R',np.max(parameters['r'])/2)
		parameters['sigma'] = parameters.get('sigma',0.01*parameters['R'])
		parameters['epsilon'] = parameters.get('epsilon',parameters['p']/2.0)
		parameters['z'] = parameters.get('z',parameters['R']*invert(parameters['sigma'],constant=1.0))
		parameters['W'] = 1
		parameters['C'] = 1

		weights = parameters['C']*func((parameters['r']*invert(parameters['sigma'],constant=0.0)))
		return weights

	@_wrapper	
	def diff(data,adjacency,weights,parameters):
		func = lambda x,eps: invert(x**(2.0+eps),constant=0.0)
		parameters['r'] = metrics()['euclidean'](data,data)
		parameters['R'] = parameters.get('R',np.max(parameters['r'])/2)
		parameters['sigma'] = parameters.get('sigma',0.01*parameters['R'])
		parameters['epsilon'] = parameters.get('epsilon',parameters['p']/2.0)
		parameters['z'] = parameters.get('z',parameters['R']*invert(parameters['sigma'],constant=1.0))
		parameters['W'] = 1
		parameters['C'] = 1
		parameters['sigma'] = 1
		weights = parameters['C']*func((parameters['r']*invert(parameters['sigma'],constant=0.0)))
		return weights

	@_wrapper
	def gauss(data,adjacency,weights,parameters):  
		func = lambda x: np.exp(-(1/2)*(x**2))
		parameters['r'] = metrics()['euclidean'](data,data)
		parameters['R'] = parameters.get('R',np.max(parameters['r'])/2)
		parameters['sigma'] = parameters.get('sigma',0.01*parameters['R'])
		parameters['epsilon'] = parameters.get('epsilon',parameters['p']/2.0)
		parameters['z'] = parameters.get('z',parameters['R']*invert(parameters['sigma'],constant=1.0))
		parameters['W'] = sp.integrate.quad(func,0,parameters['z'],args=(parameters['p']))[0]
		parameters['C'] = ((parameters['z']**parameters['p'])*invert((parameters['sigma']**2)*(parameters['W']),constant=1.0))
		weights = parameters['C']*func(parameters['r']*invert(parameters['sigma'],constant=0.0))
		return weights

	@_wrapper
	def poly(data,adjacency,weights,parameters):
		func = lambda x,eps: invert(x**(2.0+eps),constant=0.0)
		parameters['r'] = metrics()['euclidean'](data,data,where=adjacency)
		parameters['R'] = parameters.get('R',np.max(parameters['r'])/2)
		parameters['sigma'] = parameters.get('sigma',0.01*parameters['R'])
		parameters['epsilon'] = parameters.get('epsilon',parameters['p']/2.0)
		parameters['z'] = parameters.get('z',parameters['R']*invert(parameters['sigma'],constant=1.0))
		parameters['epsilon'] = parameters.get('epsilon',parameters['p']/2)
		parameters['W'] = (parameters['z']**(parameters['p']-parameters['epsilon']))*(1/(parameters['p']-parameters['epsilon']))
		parameters['C'] = ((parameters['z']**(parameters['p']))*invert((parameters['sigma']**2)*(parameters['W']),constant=1.0))
		weights = parameters['C']*func((parameters['r']*invert(parameters['sigma'],constant=0.0)))
		return weights

	@_wrapper
	def stencil(data,adjacency,weights,parameters):

		# Get order of stencil and dimension of derivative
		dimension = parameters['dimension']
		accuracy = parameters['accuracy']
		size = parameters['neighbourhood']

		# Get sort algorithm parameters
		tol,atol,rtol = parameters['tol'],parameters['atol'],parameters['rtol']
		kind = parameters['kind']

		# Get verbose
		verbose = parameters['verbose']

		# Basis of points
		basis = parameters['stencil']

		# Get weights for stencil
		weights = stencils(data,accuracy,size,basis,adjacency=adjacency,weights=weights,dimension=dimension,tol=tol,atol=atol,rtol=rtol,kind=kind,verbose=verbose)

		return weights

	@_wrapper
	def adjacency(data,adjacency,weights,parameters):

		# Get weights as adjacency matrix
		weights = adjacency

		return weights

	@_wrapper
	def default(data,adjacency,weights,parameters):

		# Get weights as adjacency matrix
		weights = adjacency

		return weights


	locs = locals()
	funcs = {k:f for k,f in locs.items() if callable(f) and not k.startswith('_')}	
	funcs[None] = default

	return funcs


# Operator labels
def labelings():
	def label(operation,function,variable,manifold,weight,adjacency,order,dimension,accuracy,**kwargs):
		return [DELIMITER.join([DELIMITER.join(operation[:j]),'%d'%j,function,DELIMITER.join(variable[:j]),
						   DELIMITER.join(weight[:j])]) 
				for j in range(1,order+1)] if order>0 else [function]

	def weighting(operation,function,variable,manifold,weight,adjacency,order,dimension,accuracy,**kwargs):
		return [DELIMITER.join([weight[j-1],manifold[j-1][dimension[j-1]],str(dimension[j-1])])
				for j in range(1,order+1)] if order > 0 else []
		# return [DELIMITER.join([weight[j-1],DELIMITER.join(manifold[j-1]),str(dimension[j-1])])
		# 		for j in range(1,order+1)] if order > 0 else []

	def adjacencies(operation,function,variable,manifold,weight,adjacency,order,dimension,accuracy,**kwargs):
		return [DELIMITER.join([adjacency[j-1],manifold[j-1][dimension[j-1]],str(dimension[j-1])])
				for j in range(1,order+1)] if order > 0 else []		
		# return [DELIMITER.join([adjacency[j-1],DELIMITER.join(manifold[j-1]),str(dimension[j-1])])
		# 		for j in range(1,order+1)] if order > 0 else []

	locs = locals()
	funcs = {k:f for k,f in locs.items() if callable(f) and not k.startswith('_')}	

	return funcs




# Operator function wrapper
def operators(funcs=None):


	if funcs is None:
		funcs = operations()


	# Operators
	def _wrapper(func):
		@functools.wraps(func)
		def _func(x,y,weights,dimension,accuracy,weight,adjacency,manifold,parameters={}):

			if callable(weight):
				pass
			elif weight is None or isinstance(weight,str):
				weight = weightings()[weight]
			else:
				_weight = weight
				weight = lambda weights,dimension,accuracy,adjacency,manifold,parameters,_weight=_weight: (_weight,adjacency)

			if not callable(weight):
				raise ValueError("Error - weight function is not callable")

			parameters = copy.deepcopy(parameters)			
			weights,adjacency = weight(weights,dimension,accuracy,adjacency,manifold,parameters) 

			d = func(x,y,weights,adjacency)

			return d,weights,adjacency

		return _func

	funcs = {func: _wrapper(funcs[func]) for func in funcs}		

	return funcs





# Compute Terms in model
def terms(data,metadata,settings,verbose=False,texify=None):
	'''
	Compute graph operators based on terms and append to data dataframes in place
	Args:
		data (dict): Dictionary of pandas dataframes datasets
		metadata (dict): Dictionary of metadata for each dataset
		settings (dict):
		verbose (str): Print out additional information
	'''
	def get(df,obj,*args,**kwargs):
		try:
			return df[obj].values
		except:
			return obj

	# Dictionaries of functions for different operators (i.e) partial for partial derivatives) and weights (i.e) poly for polynomial weights)
	operator = operators()

	# Setup and parameters are all present
	setup(data,metadata,settings,verbose=verbose)


	# Iterate through each dataset
	for key in data:

		# Get dataframe and list of terms dictionaries for each operator to compute
		df = data[key]
		terms = metadata[key]['terms']
		functions = metadata[key].pop('functions',[])

		if terms is None:
			continue


		# Private variables to reuse labels weights and adjacencies amongst operators and avoid recomputing operator data
		_is = {k:False for k in ['label','weighting','adjacencies']} # Booleans whether variable has been already computed
		_var = {k: None for k in ['label','weighting','adjacencies']} # Current type of each variable (i,e) partial)
		_vars = {k: {} for k in ['label','weighting','adjacencies']} # Dictionary for all values for each type of each variable


		# Iterate over terms and order of term to compute operators
		for term in terms:
			funcs = [term['function'],*term['label']]
			parameters = copy.deepcopy(metadata[key]['parameters'])

			for j in range(term['order']):

				_var['label'] = funcs[j+1]
				_is['label'] = _var['label'] in df
				if _is['label']:
					continue

				_var['weighting'] = term['weighting'][j]
				_is['weighting'] = _var['weighting'] in _vars['weighting']
				
				_var['adjacencies'] = term['adjacencies'][j]
				_is['adjacencies'] = _var['adjacencies'] in _vars['adjacencies']

				# Get values of term fields
				function = get(df,funcs[j])
				variable = get(df,term['variable'][j])
				variables = get(df,term['manifold'][j])
				manifold = term['manifold'][j]
				dimension = term['dimension'][j]
				accuracy = term['accuracy'][j]
				weight = _vars['weighting'][_var['weighting']] if _is['weighting'] else term['weight'][j]
				adj = _vars['adjacencies'][_var['adjacencies']] if _is['adjacencies'] else term['adjacency'][j]
				operation = operator[term['operation'][j]]

				logger.log(verbose,'Computing %s Operator %d with shape %r'%(_var['label'],dimension,variables.shape))

				df[_var['label']],_weight,_adjacencies = operation(variable,function,variables,dimension,accuracy,weight,adj,manifold,parameters)

				if not _is['weighting']:	
					_vars['weighting'][_var['weighting']] = _weight
					if parameters['store'].get('weighting'):
						metadata[key][_var['weighting']] = _weight
				if not _is['adjacencies']:
					_vars['adjacencies'][_var['adjacencies']] = _adjacencies
					if parameters['store'].get('adjacency'):
						metadata[key][_var['adjacencies']] = _adjacencies

		# Compute functions on operators
		for function in functions:
			labels = function['labels']
			if ((isinstance(labels,(list,tuple)) and any([l not in df for l in labels])) or (labels not in df)):
				try:
					df[labels] = function['func'](data[key])
				except:
					pass

	return



# Setup operator terms
def setup(data,metadata,settings,verbose=False):

	defaults = {
		'inputs':[],'outputs':[],'operations':[],'orders':range(1+1),'accuracies':[1],'constants':None,
		'weights':[None],'adjacencies':[None],'terms':[],'parameters':{},'unique':True,'kwargs':{}}


	field = 'terms'
	parameters = {}
	parameters.update({parameter: (defaults.get(parameter,settings[field].get(parameter)) if settings[field].get(parameter) is None else settings[field][parameter]) 
						for parameter in settings[field]})


	# Add parameters to metadata for each dataset, under keyword key in metadata dictionary
	for key in data:

		# Iterate through parameters and add parameter values to metadata, depending on value types
		for parameter in parameters:
			value = copy.deepcopy(parameters[parameter])
			try:
				metadata[key][parameter] = value[key]
			except:
				metadata[key][parameter] = value

			if isinstance(metadata[key][parameter],dict):
				for field in metadata[key][parameter]:
					value = copy.deepcopy(metadata[key][parameter][field])
					try:
						metadata[key][parameter][field] = value[key]
					except:
						metadata[key][parameter][field] = value


		# Handle terms parameter in metadata and check if fields are present and labelled
		parameter = 'terms'
		labelor = labelings()
		fields = ['label','weighting','adjacencies']	

		# Setup list of dictionaries of terms parameter in metadata parameters
		if metadata[key].get(parameter) is None:
			metadata[key][parameter] = {}
		_setup(data[key],**metadata[key])

		# Get label formatters for fields
		for value in metadata[key][parameter]:
			for field in fields:
				if field not in value:
					if field in labelor:
						value[field] = labelor[field](**{**metadata[key],**value})
					else:
						value[field] = metadata[key][field]

	



	return


# Get default terms
def _setup(data,inputs,outputs,operations,orders,accuracies,constants,weights,adjacencies,terms,unique,**kwargs):
	
	# Return if terms already contains variables
	if len(terms)>0:
		return

	# Check variables are correct type
	_terms = [
			 *[{'function':y,
				'variable': list(x),
				'manifold':[[*inputs]]*max(1,j),
				'weight':[w]*max(1,j),
				'adjacency':[a]*max(1,j),
				'accuracy': [accuracy]*max(j,1),		
				'dimension':[inputs.index(u) for u in x],	
				'order':j,
				'operation':[o]*max(1,j),
			   }
			   for j in orders
			   for o in operations
			   for w in weights
			   for a in adjacencies
			   for accuracy in accuracies
			   for x in icombinations(inputs,[j],unique=unique)
			   for y in outputs   
			   if (
				  (constants is None) or 
				  (not any([(v in constants.get(y,[])) for v in x]))
				   )
			  ],
			]

	terms.extend(_terms)

	return


====================================================================================================
mechanoChemML\src\graph_main.py
====================================================================================================
#!/usr/bin/env python

# Import python modules
import sys,os,glob,copy,itertools
from natsort import natsorted
import numpy as np
import pandas as pd
import numexpr as ne

# Global Variables
DELIMITER='__'
MAX_PROCESSES = 7
PARALLEL = 0

ne.set_vml_num_threads(MAX_PROCESSES)

from .graph_settings import set_settings, permute_settings#,get_settings,

from .graph_functions import structure,terms,save#,analysis

from .texify import Texify#,scinotation

from .dictionary import _set,_get,_pop,_has,_update,_permute

from .load_dump import setup,path_join #, load,dump,path_split
	


# Logging
import logging,logging.handlers
log = 'info'

rootlogger = logging.getLogger()
rootlogger.setLevel(getattr(logging,log.upper()))
stdlogger = logging.StreamHandler(sys.stdout)
stdlogger.setLevel(getattr(logging,log.upper()))
rootlogger.addHandler(stdlogger)	


logger = logging.getLogger(__name__)
logger.setLevel(getattr(logging,log.upper()))


def decorator_func(func):
	# Choose basic load, dump, plot, usetex settings
	load = 0
	dump = 1
	plot = 1
	usetex = 1
	display = 0
	verbose = 1
	
	def inner(*args, **kwargs):
		#Create dummy sys settings
		sys_settings = {
			'sys__directories__cwd__load':[kwargs['settings']['cwd']],
			'sys__directories__cwd__dump':[kwargs['settings']['cwd']],
			'sys__directories__src__load':[kwargs['settings']['cwd']],
			'sys__directories__src__dump':[kwargs['settings']['cwd']],
			'sys__directories__directories__load':[kwargs['settings']['directories_load']],
			'sys__directories__directories__dump':[kwargs['settings']['directories_dump']],
			'sys__files__files':[[kwargs['settings']['data_filename']]],			
			'sys__label':[None],
			'sys__read__data':['r'],
			'sys__read__metadata':['rb'],
			'sys__write__data':['w'],
			'sys__write__metadata':['wb'],
			'sys__labels':[[]], #To change output filenames
			}		
		#Create dummy structure settings
		structure_settings = {
			'structure__index':[None],
			'structure__seed':[1234556789],
			'structure__filters':[None],
			'structure__conditions':[None],
			'structure__refinement':[None],
			'structure__functions': kwargs['settings']['algebraic_operations']
			}
		#Create dummy flags
		boolean_settings = {
			'boolean__load':[load],
			'boolean__dump':[dump],
			'boolean__verbose':[verbose],
			'boolean__texify':[usetex],
			'boolean__display':[display],
			'boolean__plot':[plot],
			}
		#Create dummy model settings
		model_settings = {
			'model__order':kwargs['settings']['model_order'],
			'model__p':kwargs['settings']['model_p'],
			'model__basis':[None],
			'model__intercept_':[0],
			'model__inputs':[[]],
			'model__outputs':[[]],
			'model__selection':[[]],		
			'model__normalization':['l2'],
			'model__rhs_lhs': [{
				'model_label':{
					'lhs': [],
					'rhs': []}
					}]
			}
		terms_settings = {'terms__terms': kwargs['settings']['differential_operations']}
		settings = {**kwargs['settings'], **sys_settings, **model_settings, **boolean_settings, **structure_settings, **terms_settings}
		#print(settings)
		settings_grid = permute_settings(settings,_copy = True)
		func(data={}, metadata={} ,settings=settings_grid[0])
	
	return inner

@decorator_func
def main(data={},metadata={},settings={}):
	""" 
	Main program for graph theory library

	Args:
		data (dict): dictionary of {key:df} string keys and Pandas Dataframe datasets
		metadata (dict): dictionary of {key:{}} string keys and dictionary metadata about datasets
		settings (dict): settings in JSON format for library execution
	"""

	# Set Settings
	set_settings(settings,
				path=path_join(settings.get('sys',{}).get('src',{}).get('dump',''),
						  settings.get('sys',{}).get('settings')) if (
					(isinstance(settings.get('sys',{}).get('settings'),str)) and (
					not (settings.get('sys',{}).get('settings').startswith(settings.get('sys',{}).get('src',{}).get('dump',''))))) else (
					settings.get('sys',{}).get('settings')),
				_dump=True,_copy=False)


	# Import datasets
	if any([k in [None] for k in [data,metadata]]):
		data = {}
		metadata = {}
	if any([k in [{}] for k in [data,metadata]]):
		setup(data=data,metadata=metadata,
			  files=settings['sys']['files']['files'],
			  directories__load=settings['sys']['directories']['directories']['load'],# if ((not settings['boolean']['load']) or (not settings['boolean']['dump'])) else settings['sys']['directories']['directories']['dump'],
			  directories__dump=settings['sys']['directories']['directories']['dump'],
			  metafile=settings['sys']['files']['metadata'],
			  wr=settings['sys']['read']['files'],
			  flatten_exceptions=[],
			  **settings['sys']['kwargs']['load'])

	verbose = settings['sys']['verbose'] 
	models = {}

	# Set logger and texifying
	if settings['boolean']['log']:
		filelogger = logging.handlers.RotatingFileHandler(path_join(
				settings['sys']['directories']['cwd']['dump'],
				settings['sys']['files']['log'],
				ext=settings['sys']['ext']['log']))
		fileloggerformatter = logging.Formatter(
			fmt='%(asctime)s: %(message)s',
			datefmt='%Y-%m-%d %H:%M:%S')
		filelogger.setFormatter(fileloggerformatter)
		filelogger.setLevel(getattr(logging,log.upper()))
		if len(rootlogger.handlers) == 2:
			rootlogger.removeHandler(rootlogger.handlers[-1])
		rootlogger.addHandler(filelogger)
		


	logger.log(verbose,'Start')
	logger.log(verbose,'Set Settings')

	# Show Directories
	logger.log(verbose,'Imported Data: %s'%(settings['sys']['identity']) if len(data)>0 else "NO DATA")
	logger.log(verbose,'Datasets: %s'%('\n\t'.join(['',*[r'%s: %r'%(key,data[key].shape) for key in data]])))
	logger.log(verbose,'Load paths: %s'%('\n\t'.join(['',*settings['sys']['directories']['directories']['load']])))
	logger.log(verbose,'Dump paths: %s'%('\n\t'.join(['',*settings['sys']['directories']['directories']['dump']])))


	# Texify operation
	if settings['boolean']['texify']:
		tex = Texify(**settings['texify'])
		texify = tex.texify
	else:
		tex = None
		texify = None


	logger.log(verbose,'Setup Texify')

	# Define Structure of graph and perform pre-processing on datasets
	if settings['boolean']['structure']:
		structure(data,metadata,settings,verbose=settings['structure']['verbose'])
	logger.log(verbose,'Defined Structure')


	# Calculate terms
	if settings['boolean']['terms']:
		terms(data,metadata,settings,verbose=settings['terms']['verbose'],texify=texify)
	logger.log(verbose,'Calculated Operators')
	
	# Save Data
	if settings['boolean']['dump']:
		save(settings,paths={key: metadata[key]['directory']['dump'] for key in data},data=data,metadata=metadata)
	logger.log(verbose,'Saved Data') 
		
	Debug_Flag = False
	
	if Debug_Flag: 
		#Standalone fitting and saving models
	
		# Save Data
		if settings['boolean']['dump']:
			save(settings,paths={key: metadata[key]['directory']['dump'] for key in data},data=data,metadata=metadata)
		logger.log(verbose,'Saved Data')  


		# Calculate Model
		if settings['boolean']['model']:
			model(data,metadata,settings,models,verbose=settings['model']['verbose'])
		logger.log(verbose,'Setup Model')


		# Save Data
		if settings['boolean']['dump']:
			save(settings,paths={key: metadata[key]['directory']['dump'] for key in data},data=data,metadata=metadata)
		logger.log(verbose,'Saved Data') 


		# Fit Data     
		if settings['boolean']['fit']:
			for label in models:	
				fit(data,metadata,
					{key: metadata[key]['rhs_lhs'].get(label,{}).get('rhs') for key in data},
					{key: metadata[key]['rhs_lhs'].get(label,{}).get('lhs') for key in data},
					label,
					settings['fit']['info'],
					models[label],
					settings['fit']['estimator'],
					{
						**settings['fit']['kwargs'],
						**settings['model'],
						**{'modelparams':settings['analysis']}
					},				
					verbose=settings['fit']['verbose']
					)
		logger.log(verbose,'Fit Data')   


		# Save Data
		if settings['boolean']['dump']:
			save(settings,paths={key: metadata[key]['directory']['dump'] for key in data},data=data,metadata=metadata)
		logger.log(verbose,'Saved Data')  

		# Analyse Fit Results and Save Texified Model
		if settings['boolean']['analysis']:
			analysis(data,metadata,settings,models,texify,verbose=settings['analysis']['verbose'])
		logger.log(verbose,'Analysed Results')  

		# Plot Fit Results
		if settings['boolean']['plot']:
			plotter(data,metadata,settings,models,texify,verbose=settings['plot']['verbose'])
		logger.log(verbose,'Plotted Data')    

		logger.log(verbose,'Done\n')
	 

	
		return
	else: 
		return



====================================================================================================
mechanoChemML\src\graph_settings.py
====================================================================================================
#!/usr/bin/env python

# Import python modules
import sys,os,glob,copy,itertools
from natsort import natsorted
import numpy as np
import pandas as pd


# Global Variables
DELIMITER='__'

# Import user modules
from .dictionary import _set,_get,_pop,_has,_update,_permute,_clone
from .load_dump import load,dump,path_split,path_join
from .graph_utilities import basis_size,ncombinations



# Logging
import logging,logging.handlers
log = 'info'

logger = logging.getLogger(__name__)
#logger.setLevel(getattr(logging,log.upper()))


def _identify(labels,values,prefix=None,postfix=None): 
	'''
	Get formatted identity string based on labels and values, with prefix and postfix amendments
	
	Args:
		labels (list): list of keys to get values for formatting into identity string
		values (dict): dictionary of values to be formatted with label keys in labels
		prefix (str): prefix string to be included at beginning of label
		postfix (str): postfix string to be included at end of label
	
	Returns:
		String with DELIMITER separated label_value strings
	'''
	def isnumber(string):
		try:
			string = int(number)
			string = '%d'%(string)
		except:
			try:
				string = float(number)
				string = '%0.1f'%(string)
			except:
				pass
		return string

	def stringify(obj,exceptions=[]):
		
		if isinstance(obj,dict):
			obj = '_'.join(['_'.join([k,str(obj[k])]) for k in obj])

		string = isnumber(str(obj))
		checks = {**{s:'_' for s in [os.path.sep]},
				  **{s:'-' for s in [', ']},
				  **{s:'_' for s in ['.']},
				  **{s:'' for s in ['[',']','(',')']},
				  **{s:'-' for s in [' ']}
				  }
		for e in exceptions:
			checks.pop(e);
		for s in checks:
			if not string.endswith(s):
				string = string.replace(s,checks[s])
			else:
				string = string[:-len(s)].replace(s,checks[s])
		return string

	delimeter=DELIMITER
	setter='_'
	ammendments = ['%s'%(stringify(a,exceptions=(['.'] if i==1 else [])) if a is not None else '') 
					for i,a in enumerate([prefix,postfix])]
	body = (delimeter.join(['%%s%s%%s'%(setter)]*(len(labels))))%(
				sum(zip([stringify(l[-1] if isinstance(l,list) else l.split(DELIMITER)[-1]) for l in labels],
						[stringify(_get(values,l,_split=DELIMITER)) for l in labels]),()))

	strings = [ammendments[0],body,ammendments[1]]
	strings = [string for string in strings if len(string)>0]
	if len(strings) > 1:
		strings = [*strings[:1],('' if strings[-1].startswith('.') else delimeter).join(strings[1:])]

	if len(strings) > 1:
		string = ('' if strings[-1].startswith('.') else delimeter).join(strings)
	else:
		string = delimeter.join(strings)

	return string


 # Default settings, can be overwritten in the main.py of the data folder
def _get_settings():
	'''
	Fixed default settings for graph theory library

	Returns:
		Dictionary of fixed default settings
	'''
	settings_grid = _permute({
		# System settings, inputs and outputs
		'sys__verbose':[True],		
		'sys__directories__cwd__load':['.'],
		'sys__directories__cwd__dump':[None],
		'sys__directories__src__load':[None],
		'sys__directories__src__dump':[None],		
		'sys__directories__directories__load':[['']],
		'sys__directories__directories__dump':[['']],
		
		'sys__files__files':[['data.csv']],
		'sys__files__settings':['settings.json'],

		'sys__ext__files':['csv'],
		'sys__ext__data':['csv'],
		'sys__ext__metadata':['pickle'],
		'sys__ext__plot':['pdf'],
		'sys__ext__settings':['json'],
		'sys__ext__model':['tex'],
		'sys__ext__analysis':['pickle'],
		'sys__ext__log':['log'],
		'sys__ext__mplstyle':['mplstyle'],

		'sys__read__files':['r'],
		'sys__read__data':['rb'],
		'sys__read__metadata':['rb'],
		'sys__read__plot':['r'],
		'sys__read__settings':['r'],
		'sys__read__model':['r'],
		'sys__read__analysis':['rb'],
		'sys__read__log':['r'],
		'sys__read__mplstyle':['r'],

		'sys__write__files':['w'],
		'sys__write__data':['w'],
		'sys__write__metadata':['wb'],
		'sys__write__plot':['w'],
		'sys__write__settings':['w'],
		'sys__write__model':['w'],
		'sys__write__analysis':['wb'],
		'sys__write__log':['w'],
		'sys__write__mplstyle':['w'],

		'sys__kwargs__load':[{}],
		'sys__kwargs__dump':[{}],
		
		'sys__label':[""],

		# Labels for output filenames
		'sys__labels':[['sys__label','model__basis','model__order']],	

		# Booleans to turn on/off code functionality
		'boolean__verbose':[True],		
		'boolean__load':[0],
		'boolean__dump':[1],
		'boolean__fit':[1],
		'boolean__plot':[1],
		'boolean__structure':[1],
		'boolean__terms':[1],
		'boolean__model':[1],
		'boolean__log': [1],
		'boolean__texify': [1],		
		'boolean__analysis':[1],
		'boolean__drop_duplicates':[0],					

		# Options for defining graph structure
		'structure__verbose':[True],
		'structure__seed':[0],
		'structure__index':[None],							
		'structure__directed':[True],
		'structure__label':['test'],
		'structure__conditions':[None],
		'structure__samples':[None],
		'structure__filters':[None],
		'structure__functions':[None],
		'structure__symmetries':[None],
		'structure__refinement':[None],
		'structure__scale':[None],
		'structure__rename':[None],
		'structure__drop_duplicates':[None],
		'structure__round':[None],
		'structure__groupby_filter':[None],
		'structure__groupby_equal':[None],
		'structure__kwargs':[{}],

		# Terms for model fitting
		'terms__verbose':[True],		
		'terms__inputs':[None],
		'terms__outputs':[None],
		'terms__constants':[None],
		'terms__orders':[None],
		'terms__weights':[None],
		'terms__adjacencies':[None],
		'terms__operations':[None],
		'terms__terms':[None],
		'terms__parameters':[{}],
		'terms__functions':[None],
		'terms__kwargs':[{}],
		
		# Operator basis used for fitting
		'model__verbose':[True],		
		'model__inputs':[['%s%d'%(x,i) for x in ['x'] for i in range(5)]],
		'model__outputs':[['%s%d'%(x,i) for x in ['y'] for i in range(3)]],	
		'model__constants':[None],	
		'model__basis':['taylorseries'],
		'model__order':[None],
		'model__size':[None],
		'model__p':[None],
		'model__iloc':[[0]],
		'model__accuracy':[None],
		'model__weights':[['stencil']],
		'model__adjacency':[['nearest']],
		'model__operations':[['partial']],
		'model__stencil':['vandermonde'],
		'model__metric':[2],
		'model__strict':[False],
		'model__format':['csr'],
		'model__dtype':['float'],
		'model__catch_iterations':[5],
		'model__catch_factor':[2],
		'model__chunk':[0.2],
		'model__unique':[True],
		'model__tol':[None],
		'model__atol':[None],
		'model__rtol':[None],
		'model__kind':['mergesort'],
		'model__n_jobs':[1],
		'model__samples':[None],
		'model__parameter':[None],
		'model__store':[{}],
		'model__normalization':['l2'],
		'model__scheme':['None'],
		'model__approach':['indiv'],
		'model__dtype':['float64'],
		'model__type':['function'],
		'model__selection':[None],
		'model__intercept_':[False],
		'model__expand':[False],
		'model__lhs':[{}],
		'model__rhs':[{}],
		'model__rhs_lhs':[{}],
		'model__kwargs':[{}],

		  
		# Regression operations and settings
		'fit__verbose':[True],		
		'fit__estimator':['Stepwise'],
		'fit__info':[None],	
		'fit__types':[['fit','predict','update','method','interpolate']],
		'fit__kwargs':[{}],
		'fit__kwargs__estimator':['OLS'],
		'fit__kwargs__loss_func':['rmse'],
		'fit__kwargs__score_func':['rmse'],
		'fit__kwargs__fit_intercept':[False],
		'fit__kwargs__njobs':[1],
		'fit__kwargs__parallel':[None],
		'fit__kwargs__method':['cheapest'],
		'fit__kwargs__threshold':[1e20],#1e-1
		'fit__kwargs__included':[None],
		'fit__kwargs__iterations':[[]],
		'fit__kwargs__complexity_max':[None],
		'fit__kwargs__complexity_min':[None],


		# Model Analysis
		'analysis__verbose':[True],
		'analysis__kwargs':[None],

		# Plots the results of the fits
		'plot__verbose':[True],
		'plot__names': [{
			**{
				'Loss':r'\textrm{Stepwise Loss}',
				'BestFit':r'\textrm{Fits}',				
				'Coef':r'\gamma ~\textrm{Coefficients}',
				'Error':r'\textrm{Model Error}',				
			},
			**{k: r'\textrm{%s}'%(k) 
				for k in ['Variables','Operators','Terms']},
			}],						
		'plot__groups':[{'fit':['Loss','BestFit','Coef','Error'],'plot':['Variables','Operators','Terms']}],
		'texify__usetex':[1],
		'texify__texargs':[{}],
		'texify__texstrings':[{}],
		})  
	return settings_grid[0]


# Settings that will be affected by the main input settings in the settings_grid
def _get_settings_dependent(settings,_keep=False):
	'''
	Settings dependent default settings for graph theory library

	Args:
		settings (dict): dictionary of settings to set settings-dependent default values
		_keep (bool): boolean of whether to keep original settings that may be altered during settings-dependent setting of settings

	Returns:
		Dictionary of settings with settings-dependent settings included
	'''
	settings_dependent = {

		**{'%s__verbose'%(k): (lambda settings: (
			{'notset':0,'debug':10,'info':20,'warning':30,'error':40,'critical':50}[settings['%s'%(k)].get('verbose').lower()] if (
			isinstance(settings['%s'%(k)].get('verbose'),str) and settings['boolean']['verbose']) else (
			{**{i:i for i in [10,20,30,40,50]},**{i:10*i for i in [2,3,4,5]},True:20,False:False}[settings['%s'%(k)].get('verbose')] if (				
			isinstance(settings['%s'%(k)].get('verbose'),(bool,int)) and settings['boolean']['verbose']) else (
			False))
			)) for k in ['structure','terms','model','fit','analysis','plot']},
		**{'%s__verbose'%(k): (lambda settings: (
			{'notset':0,'debug':10,'info':20,'warning':30,'error':40,'critical':50}[settings['%s'%(k)].get('verbose').lower()] if (
			isinstance(settings['%s'%(k)].get('verbose'),str) and settings['boolean']['verbose']) else (
			{**{i:i for i in [10,20,30,40,50]},**{i:10*i for i in [2,3,4,5]},True:20,False:False}[settings['%s'%(k)].get('verbose')] if (
			isinstance(settings['%s'%(k)].get('verbose'),(bool,int)) and settings['boolean']['verbose']) else (
			20 if settings['boolean']['verbose'] else False))
			)) for k in ['sys']},
		
		'sys__directories__cwd__load': lambda settings: (settings['sys']['directories']['cwd'].get('dump') if settings['boolean']['load'] or settings['sys']['directories']['cwd'].get('load') is None else settings['sys']['directories']['cwd'].get('load')),
		'sys__directories__cwd__dump': lambda settings: (settings['sys']['directories']['cwd'].get('load') if settings['sys']['directories']['cwd'].get('dump') is None else settings['sys']['directories']['cwd'].get('dump')),
		'sys__directories__src__dump': lambda settings: (settings['sys']['directories']['src'].get('dump') if (settings['sys']['directories']['src'].get('dump') is not None) 
														else settings['sys']['directories']['src'].get('load') if (settings['sys']['directories']['src'].get('load') is not None) 
														else (settings['sys']['directories']['cwd'].get('dump')) if (settings['sys']['directories']['cwd'].get('dump') is not None) 
														else (settings['sys']['directories']['cwd'].get('load'))),

		'sys__directories__src__load': lambda settings: (settings['sys']['directories']['src'].get('load') if (settings['sys']['directories']['src'].get('load') is not None) 
														else settings['sys']['directories']['src'].get('dump') if (settings['sys']['directories']['src'].get('dump') is not None) 
														else (settings['sys']['directories']['cwd'].get('load')) if (settings['sys']['directories']['cwd'].get('load') is not None) 
														else (settings['sys']['directories']['cwd'].get('dump'))),
		'sys__directories__directories__load': lambda settings: ([(path_join(settings['sys']['directories']['cwd']['load'],k) if not k.startswith(settings['sys']['directories']['cwd']['load']) else k) 
																	for k in (settings['sys']['directories']['directories']['load'] if not settings['boolean']['load'] else settings['sys']['directories']['directories']['dump'])]), 
		'sys__directories__directories__dump': lambda settings: ([(path_join(settings['sys']['directories']['cwd']['dump'],k) if not k.startswith(settings['sys']['directories']['cwd']['dump']) else k) 
																	for k in settings['sys']['directories']['directories']['dump']]),
		'sys__files__settings': lambda settings: (path_join(settings['sys']['directories']['src']['load'],settings['sys']['files']['settings']) if not settings['sys']['files']['settings'].startswith(settings['sys']['directories']['src']['load']) else settings['sys']['files']['settings']),

		'sys__read__files': lambda settings: (settings['sys']['read'].get('data','rb') if settings['boolean']['load'] else settings['sys']['read'].get('files','r')),


		**{'boolean__%s'%k: (lambda settings,k=k: ((not settings['boolean']['load']) and (settings['boolean'][k]))) for k in ['terms','fit']},


		'structure__seed': lambda settings: [settings['structure'].get('seed',0),np.random.seed(settings['structure'].get('seed',0))][0],
		'structure__drop_duplicates': lambda settings: (settings['structure'].get('drop_duplicates',{'drop':settings['model']['inputs']}) if settings['structure'].get('drop_duplicates') is not None else None),
		
		'structure__round': lambda settings: (settings['structure'].get('round',{'decimals':8,'labels':settings['model']['inputs']}) if settings['structure'].get('round') is not None else None),
		
		'structure__groupby_filter': lambda settings: (settings['structure'].get('groupby_filter',
										{
										# 'by':settings['model']['parameter'],
										'by':'BURNUP',
										'func':[i for i in settings['model']['inputs'] if i!=settings['model']['parameter']]
										}) if settings['structure'].get('groupby') is not None else None),
		
		
		'structure__groupby_equal': lambda settings: (settings['structure'].get('groubpy_equal',
															{'by':settings['model']['parameter'],'iloc':0,
															 'labels':[i for i in settings['model']['inputs'] if i!=settings['model']['parameter']]
															}) if settings['structure'].get('groupby') is not None else None),
		
		'structure__scale': lambda settings: (settings['structure'].get('scale',
															{'labels':[*settings['model']['inputs'],*settings['model']['outputs']]
															}) if settings['structure'].get('scale') is not None else None),

		'structure__refinement': lambda settings: (settings['structure'].get('refinement',{'base':2,'n':None,'p':None,'powers':[1,2,1],'settings':[],'keep':False}) if settings['structure'].get('refinement') is not None else None),


		'model__p': lambda settings: len(settings['model'].get('inputs',[])) if settings['model'].get('p') is None else settings['model']['p'],
		'model__manifold': lambda settings: settings['model'].get('inputs',[]) if settings['model'].get('manifold') is None else settings['model']['manifold'],
		'model__accuracy': lambda settings: settings['model']['accuracy'] if settings['model'].get('accuracy') is not None else settings['model']['order']+1,
		'model__neighbourhood': lambda settings: (settings['model']['neighbourhood'] if settings['model'].get('neighbourhood') is not None else {
					'vandermonde':ncombinations(settings['model']['p'],settings['model']['accuracy'],unique=settings['model']['unique'])-1
					}[settings['model']['stencil']]),
		'model__sparsity': lambda settings: (settings['model']['sparsity'] if settings['model'].get('sparsity') is not None else settings['model']['neighbourhood']*settings['model']['p']*settings['model']['order']),
		'model__adjacency': lambda settings: (settings['model']['adjacency'] if settings['model'].get('adjacency') is not None else None),

		# iloc can be passed as one of five types:
		# model fitting uses reference dataset df0 and given dataset df to form model in graph_fit and graph_models
		# 1) int : specific location in df0 to expand model and fit with data in df
		# 2) list[int]: several specific locations in df0 to expand model and fit with data in df
		# 3) float: proportion of locations in df0 to expand number, chooses randomly
		# 3) True: all allowed locations in ilocs of df0 
		# 4) None: Find closest point in ilocs of df0 for each point in df
		# 5) dict: dictionary with keys of datasets for different settings, and values of one of 1) to 4) iloc types above
		# Ensure iloc is either dictionary of {dataset:value} value, where value is 1) to 4) of above
		# If iloc type is an int,float,True then in graph_models, iloc parameter is preprocessed into list, depending on specific type and size of df0
		# If iloc is None, then kept as None and processed in graph_models when forming specific model/finding nearest points for fitting

		'model__iloc': lambda settings: (settings['model'].get('iloc',[0]) if not isinstance(settings['model'].get('iloc',[0]),dict) else (
										{k: settings['model']['iloc'][k] if not any([k in (settings['fit'].get('info',{}).get(t,{}) 
																					if settings['fit'].get('info',{}) is not None else {}) 
																			 		for t in settings['fit'].get('types',[]) if t in ['interpolate']])
																else None if (
																	not any([k in (settings['fit'].get('info',{}).get(t,{}) 
																			if settings['fit'].get('info',{}) is not None else {}) 
																			for t in settings['fit'].get('types',[]) if t not in ['interpolate']])) 
																else settings['model']['iloc'][k] 
																		if isinstance(settings['model']['iloc'][k],(list)) else (
																	settings['model']['iloc'][k])
																for k in settings['model']['iloc']})),

		'model__expand': lambda settings: settings['model'].get('expand',False) or settings['model']['basis'] in ['taylorseries'],
		'model__transform__features': lambda settings: {
								**{
								  **{k:{'order':None,'features':None,
										'dim':None,'intercept_':False,'samples':None,
								  		'iterate':False}
									 for k in [None,'taylorseries','default',
									 		   'linear','monomial','polynomial',
									 		   'chebyshev','legendre','hermite']},
								 }.get(settings['model']['basis'],{}),
								 **settings['model'].get('transform',{}).get('features',{})},
		'model__transform__normalization': lambda settings: {**{k:{'norm_func':k,'axis':0} for k in [None,'l2','uniform']}.get(settings['model']['normalization'],{}),
															 **settings['model'].get('transform',{}).get('normalization',{})},
		'model__transform__scheme': lambda settings: {**{str(k): {'scheme':k,'method':v} for k,v in {None:None,'euler':None}.items()}.get(settings['model']['scheme'],{}),
													  **settings['model'].get('transform',{}).get('scheme',{})},
		'model__transform__dtype': lambda settings: settings['model'].get('transform',{}).get('dtype',{str(k):k for k in ['float64']}.get(settings['model']['dtype'])),
		# 'model__lhs__order': lambda settings: (settings['model'].get('lhs',{}).get('order',settings['model']['order'])),
		# 'model__lhs__label': lambda settings: (settings['model'].get('lhs',{}).get('label',settings['model']['outputs'])),
		# 'model__lhs__function': lambda settings: (settings['model'].get('lhs',{}).get('function',settings['model']['outputs'])),
		# 'model__lhs__variable': lambda settings: (settings['model'].get('lhs',{}).get('variable',settings['model']['inputs'])),
		# 'model__lhs__operations': lambda settings: (settings['model'].get('lhs',{}).get('operations',settings['terms']['operations'])),
		# 'model__rhs__order': lambda settings: (settings['model'].get('rhs',{}).get('order',settings['model']['order'])),
		# 'model__rhs__label': lambda settings: (settings['model'].get('rhs',{}).get('label',settings['model']['outputs'])),
		# 'model__rhs__function': lambda settings: (settings['model'].get('rhs',{}).get('function',settings['model']['outputs'])),
		# 'model__rhs__variable': lambda settings: (settings['model'].get('rhs',{}).get('variable',settings['model']['inputs'])),
		# 'model__rhs__operations': lambda settings: (settings['model'].get('rhs',{}).get('operations',settings['terms']['operations'])),



		'terms__inputs': lambda settings: settings['model']['inputs'] if  settings['terms'].get('inputs') is None else settings['terms']['inputs'],
		'terms__outputs': lambda settings: settings['model']['outputs'] if  settings['terms'].get('outputs') is None else settings['terms']['outputs'],
		'terms__constants': lambda settings: settings['model']['constants'] if  settings['terms'].get('constants') is None else settings['terms']['constants'],
		'terms__orders': lambda settings: (list(range(settings['model']['order']+1)) if settings['terms'].get('orders') is None else list(range(settings['terms']['orders']+1)) if isinstance(settings['terms']['orders'],(int,np.integer)) else settings['terms']['orders']),
		'terms__accuracies': lambda settings: ([settings['model']['order']+1] if settings['terms'].get('accuracies',settings['model'].get('accuracies',settings['model'].get('accuracy',settings['model']['order']+1))) is None else [settings['terms'].get('accuracies',settings['model'].get('accuracies',settings['model'].get('accuracy',settings['model']['order']+1)))]
		 										if isinstance(settings['terms'].get('accuracies',settings['model'].get('accuracies',settings['model'].get('accuracy',settings['model']['order']+1))),(int,np.integer)) else settings['terms'].get('accuracies',settings['model'].get('accuracies',[settings['model'].get('accuracy',settings['model']['order']+1)]))),
		'terms__parameters': lambda settings: ({
								**settings['model'],
								**(settings['terms'].get('parameters') if settings['terms'].get('parameters') is not None else {}),
								}),		
		'terms__functions': lambda settings: ([
								*(settings['structure'].get('functions') if settings['structure'].get('functions') is not None else []),
								*(settings['terms'].get('functions') if settings['terms'].get('functions') is not None else []),
								]),
		'terms__weights': lambda settings: settings['model'].get('weights',[settings['model'].get('weight')]) if settings['terms'].get('weights') is None else settings['terms']['weights'],		
		'terms__adjacencies': lambda settings: settings['model'].get('adjacencies',[settings['model'].get('adjacency')]) if settings['terms'].get('adjacencies') is None else settings['terms']['adjacencies'],		
		'terms__operations': lambda settings: settings['model']['operations'] if  settings['terms'].get('operations') is None else settings['terms']['operations'],		
		'terms__unique': lambda settings: settings['model']['unique'] if  settings['terms'].get('unique') is None else settings['terms']['unique'],		
		'fit__kwargs__method': lambda settings: (settings['fit']['kwargs'].get('method','cheapest') if settings['fit']['estimator'] == 'Stepwise' else None),
		'fit__kwargs__included': lambda settings: ([*(settings['fit']['kwargs'].get('included',[]) if settings['fit']['kwargs'].get('included') is not None else [])]),
		'fit__kwargs__fixed': lambda settings: ({**({0: 1} if settings['model']['expand'] else {}),**settings['fit']['kwargs'].get('fixed',{})}),


		'plot__fig': lambda settings: ({k:settings['plot'].get('fig',{}).get(k,{}) for k in settings['plot'].get('fig',settings['plot']['names'])}),
		'plot__axes': lambda settings: ({k:settings['plot'].get('axes',{}).get(k,{}) for k in settings['plot']['fig']}),
		'plot__names': lambda settings: ({k: settings['plot'].get('names',{}).get(k,k)
										 for k in settings['plot']['fig']}),	
		
		'plot__dims': lambda settings: (settings['plot'].get('dims',slice(None))),
		'plot__rescale': lambda settings: ({k:settings['plot'].get('rescale',{}).get(k,False) if isinstance(settings['plot'].get('rescale'),dict) else settings['plot'].get('rescale',False)
							  				for k in settings['plot'].get('fig',settings['plot']['names'])}),
		'plot__retain__fig': lambda settings: ({k:settings['plot'].get('retain',{}).get('fig',{}).get(k,False) for k in settings['plot']['fig']}),
		'plot__retain__axes': lambda settings: ({k:settings['plot'].get('retain',{}).get('axes',{}).get(k,False) for k in settings['plot']['fig']}),
		'plot__retain__label': lambda settings: ({k:settings['plot'].get('retain',{}).get('label',{}).get(k,False) for k in settings['plot']['fig']}),
		'plot__retain__dim': lambda settings: ({k:settings['plot'].get('retain',{}).get('dim',{}).get(k,False) for k in settings['plot']['fig']}),
		'plot__retain__key': lambda settings: ({k:settings['plot'].get('retain',{}).get('key',{}).get(k,False) for k in settings['plot']['fig']}),

		'plot__file': lambda settings: (settings['plot'].get('file',{k: path_join(settings['sys']['directories']['src']['load'],'plot',ext=settings['sys']['ext']['settings'])
					 	for k in settings['plot']['fig']}) if isinstance(settings['plot'].get('file'),dict) else (
					 	{k: settings['plot'].get('file',path_join(settings['sys']['directories']['src']['load'],'plot',ext=settings['sys']['ext']['settings']))
					 	for k in settings['plot']['fig']})),
		'plot__mplstyle': lambda settings: (settings['plot'].get('mplstyle',{k: path_join(settings['sys']['directories']['src']['load'],'plot' if settings['texify'].get('usetex') else 'plot_notex',ext=settings['sys']['ext']['mplstyle'])
					 	for k in settings['plot']['fig']}) if isinstance(settings['plot'].get('mplstyle'),dict) else (
					 	{k: settings['plot'].get('mplstyle',path_join(settings['sys']['directories']['src']['load'],'plot' if settings['texify'].get('usetex') else 'plot_notex',ext=settings['sys']['ext']['mplstyle']))
					 	for k in settings['plot']['fig']})),					 		
		'plot__settings': lambda settings: ({k:{
							**{name:{
								'style':{'layout':{'nrows':2,'ncols':5},
										**settings['plot'].get('settings',{}).get(name,{}).get('style',{})},
								'other':{'fontsize':45,'constant':[],
										**settings['plot'].get('settings',{}).get(name,{}).get('other',{})},
								**{k: settings['plot'].get('settings',{}) .get(name,{})[k] for k in settings['plot'].get('settings',{}).get(name,{}) if k not in ['style','other']}}
								for name in ['Coef']},
							**{name:{
								'other':{
									'iterations': [None,100,30,20,10,5,3,1],
									'x':settings['model']['inputs'][:1],
									'fontsize':55,
									'constant':[],
									'data':True,
									'sort':False,
									**settings['plot'].get('settings',{}).get(name,{}).get('other',{})} ,
								**{k: settings['plot'].get('settings',{}).get(name,{})[k] for k in settings['plot'].get('settings',{}).get(name,{}) if k not in ['other']}}
								for name in ['BestFit','Error']},
							**{name:{
							 	'other':{'fontsize':100,'constant':[],**settings['plot'].get('settings',{}).get(name,{}).get('other',{})},
								**{k: settings['plot'].get('settings',{}) .get(name,{})[k] for k in settings['plot'].get('settings',{}).get(name,{}) if k not in ['other']}}
								for name in ['Loss']},
							**{name:{
								'other':{'subplots':True,
										 'x':[],
										 'y':[],
										 'fontsize':32,
										 'terms': [{'function':settings['model']['outputs'],
												    'variable':settings['model']['inputs']}],
										 'constant':[],
										  **settings['plot'].get('settings',{}).get(name,{}).get('other',{})},
	 							'style':{'layout':{
									'nrows':min(2,max(1,min(len(settings['plot'].get('settings',{}).get(name,{}).get('other',{}).get('x',[])),
		 											      len(settings['plot'].get('settings',{}).get(name,{}).get('other',{}).get('y',[]))))
		 										if settings['plot'].get('settings',{}).get(name,{}).get('other',{}).get('subplots') else 1),
		 							'ncols':min(5,max(1,min(len(settings['plot'].get('settings',{}).get(name,{}).get('other',{}).get('x',[])),
		 											      len(settings['plot'].get('settings',{}).get(name,{}).get('other',{}).get('y',[]))))
		 										if settings['plot'].get('settings',{}).get(name,{}).get('other',{}).get('subplots') else 1)},
		 							**settings['plot'].get('settings',{}).get(name,{}).get('style',{})},
								**{k: settings['plot'].get('settings',{}).get(name,{})[k] for k in settings['plot'].get('settings',{}).get(name,{}) if k not in ['other','style']}}
								for name in ['Variables']},
							**{name:{
								'other':{'subplots':True,
										 'x':[],
										 'y':[],
										 'fontsize':32,
										 'terms': [{'function':settings['model']['outputs'],
										 		    'variable':settings['model']['inputs']}],
										  'constant':[],
										  **settings['plot'].get('settings',{}).get(name,{}).get('other',{}),										  
										},
	 							'style':{'layout':{
									'nrows':min(4,max(1,min(len(settings['plot'].get('settings',{}).get(name,{}).get('other',{}).get('x',[])),
		 											      len(settings['plot'].get('settings',{}).get(name,{}).get('other',{}).get('y',[]))))
		 										if settings['plot'].get('settings',{}).get(name,{}).get('other',{}).get('subplots') else 1),
		 							'ncols':min(8,max(1,min(len(settings['plot'].get('settings',{}).get(name,{}).get('other',{}).get('x',[])),
		 											      len(settings['plot'].get('settings',{}).get(name,{}).get('other',{}).get('y',[]))))
		 										if settings['plot'].get('settings',{}).get(name,{}).get('other',{}).get('subplots') else 1)},
		 							**settings['plot'].get('settings',{}).get(name,{}).get('style',{})},
								**{k: settings['plot'].get('settings',{}).get(name,{})[k] for k in settings['plot'].get('settings',{}).get(name,{}) if k not in ['other','style']}}
								for name in ['Operators','Terms']},
							}[k] 
						 for k in settings['plot']['fig']}  if ((settings['plot'].get('settings') is None) or any([k in settings['plot'].get('settings',{}) or k in settings['plot']['names'] 
						 for k in settings['plot']['fig']])) else (
						 {l: {k:{
							**{name:{'style':{'layout':{'nrows':2,'ncols':5},
										**settings['plot'].get('settings',{}).get(l,{}).get(name,{}).get('style',{})},
									'other':{'fontsize':45,'constant':[],
											**settings['plot'].get('settings',{}).get(l,{}).get(name,{}).get('other',{})},
									**{k: settings['plot'].get('settings',{}).get(l,{}).get(name,{})[k] for k in settings['plot'].get('settings',{}).get(l,{}).get(name,{}) if k not in ['style','other']}}
								for name in ['Coef']},									
							**{name:{
								'other':{
									'iterations': [None,100,30,20,10,5,3,1],
									'x':settings['model']['inputs'][:1],
									'fontsize':55,
									'constant':[],
									'data': True,
									'sort':False,
									**settings['plot'].get('settings',{}).get(l,{}).get(name,{}).get('other',{})} ,
								**{k: settings['plot'].get('settings',{}).get(l,{}).get(name,{})[k] for k in settings['plot'].get('settings',{}).get(l,{}).get(name,{}) if k not in ['other']}}
								for name in ['BestFit','Error']},
							**{name:{
							 	'other':{
							 		'fontsize':100,'constant':[],
							 		**settings['plot'].get('settings',{}).get(l,{}).get(name,{}).get('other',{})},
								**{k: settings['plot'].get('settings',{}).get(l,{}).get(name,{})[k] for k in settings['plot'].get('settings',{}).get(l,{}).get(name,{}) if k not in ['other']}}
								for name in ['Loss']},								
							**{name:{
								'other':{'subplots':True,
										'x':[],
										'y':[],
	 								    'fontsize':32,
										'terms': [{'function':settings['model']['outputs'],
												    'variable':settings['model']['inputs']}],
										'constant':[],
										  **settings['plot'].get('settings',{}).get(name,{}).get('other',{}),	
										}, 								    
	 							'style':{'layout':{
									'nrows':min(2,max(1,min(len(settings['plot'].get('settings',{}).get(l,{}).get(name,{}).get('other',{}).get('x',[])),
		 											        len(settings['plot'].get('settings',{}).get(l,{}).get(name,{}).get('other',{}).get('y',[]))))
		 										if settings['plot'].get('settings',{}).get(l,{}).get(name,{}).get('other',{}).get('subplots') else 1),
		 							'ncols':min(5,max(1,min(len(settings['plot'].get('settings',{}).get(l,{}).get(name,{}).get('other',{}).get('x',[])),
		 											        len(settings['plot'].get('settings',{}).get(l,{}).get(name,{}).get('other',{}).get('y',[]))))
		 										if settings['plot'].get('settings',{}).get(l,{}).get(name,{}).get('other',{}).get('subplots') else 1)},
		 							**settings['plot'].get('settings',{}).get(l,{}).get(name,{}).get('style',{})},
								**{k: settings['plot'].get('settings',{}).get(l,{}).get(name,{})[k] 
									for k in settings['plot'].get('settings',{}).get(l,{}).get(name,{}) if k not in ['other','style']}}
								for name in ['Variables']},
							**{name:{
								'other':{'subplots':True,
										'x':[],
										'y':[],
	 								    'fontsize':32,'constant':[],**settings['plot'].get('settings',{}).get(l,{}).get(name,{}).get('other',{})},
	 							'style':{'layout':{
									'nrows':min(4,max(1,min(len(settings['plot'].get('settings',{}).get(l,{}).get(name,{}).get('other',{}).get('x',[])),
		 											        len(settings['plot'].get('settings',{}).get(l,{}).get(name,{}).get('other',{}).get('y',[]))))
		 										if settings['plot'].get('settings',{}).get(l,{}).get(name,{}).get('other',{}).get('subplots') else 1),
		 							'ncols':min(8,max(1,min(len(settings['plot'].get('settings',{}).get(l,{}).get(name,{}).get('other',{}).get('x',[])),
		 											        len(settings['plot'].get('settings',{}).get(l,{}).get(name,{}).get('other',{}).get('y',[]))))
		 										if settings['plot'].get('settings',{}).get(l,{}).get(name,{}).get('other',{}).get('subplots') else 1)},
		 							**settings['plot'].get('settings',{}).get(l,{}).get(name,{}).get('style',{})},
								**{k: settings['plot'].get('settings',{}).get(l,{}).get(name,{})[k] 
									for k in settings['plot'].get('settings',{}).get(l,{}).get(name,{}) if k not in ['other','style']}}
								for name in ['Operators','Terms']},								
							}[k] 
						 for k in settings['plot']['fig']}
						 for l in settings['plot'].get('settings',{})})),
		'texify__texargs__order': lambda settings: (settings['model']['order'] if settings['model']['basis'] in ['taylorseries','default',None,'polynomial','monomial'] else settings['texify'].get('texargs',{}).get('order')),
		'texify__texargs__basis': lambda settings: (settings['model']['order'] if settings['model']['basis'] in [None,'default','linear','monomial','polynomial','chebyshev','legendre','hermite']  else settings['texify'].get('texargs',{}).get('order')),
		'texify__texargs__iloc': lambda settings: (settings['texify'].get('texargs',{}).get('iloc',settings['model']['iloc'])),
		'texify__texargs__unique': lambda settings: (settings['texify'].get('texargs',{}).get('unique',settings['model']['unique'])),
		'texify__texargs__weights': lambda settings: (settings['texify'].get('texargs',{}).get('weights',settings['model'].get('weights',['diff','frobenius','gauss','poly','decay','stencil']))),
		'texify__texargs__inputs': lambda settings: (settings['texify'].get('texargs',{}).get('inputs',{'%s'%(x):r'{%s_{%d}}'%(u,i) for u in ['x'] for i,x in enumerate(settings['model']['inputs'])})),
		'texify__texargs__outputs': lambda settings: (settings['texify'].get('texargs',{}).get('outputs',{'%s'%(x):r'{%s_{%d}}'%(u,i) for u in ['y'] for i,x in enumerate(settings['model']['outputs'])})),
		'texify__texargs__groups': lambda settings: (settings['texify'].get('texargs',{}).get('groups',({**{'%s'%(x):r'{%s}'%(x) for u in ['x'] for i,x in enumerate(settings['model']['inputs'])}}))),
		'texify__texargs__constants': lambda settings: settings['model'].get('constants',{}),
		'texify__texargs__bases': lambda settings: settings['texify'].get('texargs',{}).get('bases',{None:1,'default':1,'monomial':1,'polynomial':1,'taylorseries':1,'derivative':1,
																								   'chebyshev':1,'legendre':1,'hermite':1}),
		'texify__texstrings': lambda settings: ({**settings['texify'].get('texstrings',{})}),
		'sys__label': lambda settings: (settings['sys']['labeler'](settings) if callable(settings['sys'].get('labeler')) else settings['sys'].get('label','template')),
		'sys__identity': lambda settings: _identify(settings['sys']['labels'],settings),
		**{'sys__files__%s'%(k):(lambda settings,k=k: (settings['sys'].get('files',{}).get(k) if (not settings['boolean']['load']) else (
						[_identify(settings['sys']['labels'],settings,'data','.%s'%settings['sys']['ext']['data'])])))	
				for k in ['files']},
		**{'sys__files__%s'%(k): (lambda settings,k=k: (settings['sys'].get('files',{}).get(k,path_join('%s%s%s'%(k,DELIMITER if len(settings['sys']['identity'])>0 else '',settings['sys']['identity']),ext=settings['sys']['ext'][k]))
												if (not settings['boolean']['load']) else  (
										_identify(settings['sys']['labels'],settings,k,'.%s'%settings['sys']['ext'][k]))))
										for k in ['metadata','data']},									
		**{'sys__files__%s'%(k):(lambda settings,k=k: (settings['sys'].get('files',{}).get(k) if (settings['sys'].get('files',{}).get(k) is not None) else  (
										_identify(settings['sys']['labels'],settings,'%s%s%%s'%(k,DELIMITER),'.%s'%settings['sys']['ext'][k]))))
										for k in ['model']},
		**{'sys__files__%s'%(k):(lambda settings,k=k: (settings['sys'].get('files',{}).get(k) if (settings['sys'].get('files',{}).get(k) is not None) else  (
										_identify(settings['sys']['labels'],settings,'%s%s%%s'%(k,DELIMITER),'.%s'%settings['sys']['ext'][k]))))
										for k in ['analysis']},
		**{'sys__files__%s'%(k): (lambda settings,k=k: (settings['sys'].get('files',{}).get(k,path_join('%s%s%s'%(k,DELIMITER if len(settings['sys']['identity'])>0 else '',settings['sys']['identity']),ext=settings['sys']['ext'][k]))
												if (not settings['boolean']['load']) else  (
										_identify(settings['sys']['labels'],settings,k,'.%s'%settings['sys']['ext'][k]))))
										for k in ['log']},										
		**{'sys__files__%s'%(k):(lambda settings,k=k: (settings['sys'].get('files',{}).get(k) if (settings['sys'].get('files',{}).get(k) is not None) else  (
										_identify(settings['sys']['labels'],settings,'%s%s%%s%s%%s'%(k,DELIMITER,DELIMITER),'.%s'%settings['sys']['ext'][k]))))
										for k in ['plot']},		

		}



	_settings = {k: copy.deepcopy(settings_dependent[k]) if k in settings_dependent else settings[k] for k in settings}
	for key,value in settings_dependent.items():
		_set(_settings,key,value(_settings),_split=DELIMITER,_copy=False)
	
	if _keep:
		for key in copy.deepcopy(_settings):
			if _has(settings,key,_split=DELIMITER):
				_set(_settings,key,_get(settings,key,_split=DELIMITER),_split=DELIMITER,_copy=False)
	
	return _settings


def set_settings(settings,path='settings.json',_copy=False,_dump=False):
	''' 
	Set dictionary of settings

	Args:
		settings (dict,str): dictionary of settings, either with delimiter-separated strings or nested dictionary entries; or string path to dictionary to be loaded; to be modified in place
		path (str): string of path to save dictionary in JSON format (='settings.json')
		_copy (bool): boolean to copy data in settings dictionary (=False)
		_dump (bool): boolean to save dictionary to path (=False)
	'''	

	# Check if settings are passed as atring to be loaded as dictionary
	if not isinstance(settings,dict):
		settings = load(settings,default={})

	assert isinstance(settings,dict), "Settings not Dict"

	# Check if settings are present in path, in case of empty settings dictionary
	if settings == {}:
		settings.update(load(path,default={}))


	# Copy settings to _settings, to be modified before replacing settings
	_settings = {}
	_clone(settings,_settings)

	# Get keys of _settings and sort by place in nested structure (based on number of DELIMITER)
	keys = list(_settings)
	keys = sorted(keys,key=lambda key: (key,-len(key.split(DELIMITER))))

	# Replace DELIMITER separated keys with proper nested dictionary entries
	for key in keys:
		value = _pop(settings,key,_split=False,_copy=_copy)
		_set(settings,key,value,_split=DELIMITER,_copy=_copy)


	# Get fixed default settings
	_settings = _get_settings()

	# Set settings based on DELIMITER separated string keys in _settings
	for key,value in _settings.items():
		if not _has(settings,key,_split=DELIMITER):
			_set(settings,key,value,_split=DELIMITER,_copy=False,_reset=False)


	# Set settings-dependent settings
	for key,value in _get_settings_dependent(settings).items():
		_set(settings,key,value,_split=DELIMITER,_copy=False,_reset=False)

	# Save settings to path if _dump is True
	if _dump:
		dump(settings,path)
	return


def get_settings(settings,path='settings.json',_dump=False):
	'''	
	Get dictionary of settings

	Args:
		settings (dict,str): dictionary of settings, either with delimiter-separated strings or nested dictionary entries; or string path to dictionary to be loaded; to be modified in place
		path (str): string of path to save dictionary in JSON format (='settings.json')
		_dump (bool): boolean to save dictionary to path (=False)
	'''	


	assert isinstance(settings,dict), "Settings not Dict"

	if _dump:
		dump(settings,path)

	return settings


def permute_settings(settings,path='settings.json',_copy=False,_dump=False,_groups=None,_set_settings=True):
	""" 
	Permute dictionary of settings

	Args:
		settings (dict): dictionary of settings with values of lists of values to be permuted
		path (str): string of path to save dictionary in JSON format (='settings.json')
		_dump (bool): boolean to save dictionary to path (=False)
		_copy (bool): boolean to copy data in settings dictionary (=False)
		_groups (list): list of groups of dictionary keys to group when permuting
		_set_settings (bool): boolean to update settings with default values after permutations

	Returns:
		List of settings with all permutations in original settings
	"""	
	_path = lambda i,N,path=path: '%s%s.%s'%(path_split(path,file=True,directory_file=True),'%s%d'%(DELIMITER,i) if N>1 else '',path_split(path,ext=True))
	_settings = _permute(settings,_copy=_copy,_groups=_groups)



	if _set_settings:
		N = len(_settings)
		for i in range(N):
			set_settings(_settings[i],_path(i,N),_dump=_dump,_copy=_copy)
	return _settings





====================================================================================================
mechanoChemML\src\graph_utilities.py
====================================================================================================
#!/usr/bin/env python

# Import python modules
import os,sys,copy,warnings,functools,itertools,inspect,timeit
from natsort import natsorted
import numpy as np
import scipy as sp
import scipy.stats,scipy.signal
import pandas as pd
import sparse as sparray
# import numba as nb



# import multiprocess as mp
# import multithreading as mt
import joblib
import multiprocessing as multiprocessing
import multiprocessing.dummy as multithreading

# warnings.simplefilter("ignore", (UserWarning,DeprecationWarning,FutureWarning))
# warnings.simplefilter("ignore", (sp.sparse.SparseEfficiencyWarning))
# warnings.filterwarnings('error',category=sp.sparse.SparseEfficiencyWarning)

DELIMITER='__'
MAX_PROCESSES = 8

# Logging
import logging
log = 'info'
logger = logging.getLogger(__name__)
#logger.setLevel(getattr(logging,log.upper()))		
from progress.bar import Bar





# Sparse array class
class sparsearray(sparray.COO):
	def __init__(self,shape,data=None,coords=None,dtype=None,fill_value=None):
		self.shape = shape
		self.ndim = len(self.data)		
		self.data = data if data is not None else np.array([])
		self.coords = coords if coords is not None else (np.array([]),)*self.ndim
		self.dtype = dtype if dtype is not None else self.data.dtype
		self.fill_value = fill_value if fill_value is not None else 0
		
		self.nnz = self.data.size

		return


# Quasi array class
class ndarray(object):
	def __init__(self,data,row,col):
		self.data = data
		self.row = row
		self.col = col
	def eliminate_zeros(self):
		inds = where(self.data==0)
		self.data = delete(self.data,inds)
		arr.row = delete(self.row,inds)
		arr.col = delete(self.col,inds)
		return


def timing(verbose):
	''' 
	Timing function wrapper
	
	'''
	def decorator(func):
		@functools.wraps(func)
		def wrapper(*args,**kwargs):
			if verbose:
				time = 0
				time = timeit.default_timer() - time
				value = func(*args,**kwargs)
				time = timeit.default_timer() - time
				logger.log(verbose,'%r: %r s'%(repr(func),time))
			else:
				value = func(*args,**kwargs)				
			return value
		return wrapper
	return decorator


# Wrapper class for function, with
# class args and kwargs assigned after
# classed args and kwrags
class wrapper(object):
	def __init__(self,_func,*args,**kwargs):
		self.func = _func
		self.args = args
		self.kwargs = kwargs
		functools.update_wrapper(self, _func)
		return

	def __call__(self,*args,**kwargs):
		args = [*args,*self.args]
		kwargs = {**self.kwargs,**kwargs}
		return self.func(*args,**kwargs)

	def __repr__(self):
		return self.func.__repr__()

	def __str__(self):
		return self.func.__str__()


# Decorator with optional additional arguments
def decorator(*ags,**kwds):
	def wrapper(func):
		@functools.wraps(func)
		def function(*args,**kwargs):
			args = list(args)
			args.extend(ags)
			kwargs.update(kwds)
			return func(*args,**kwargs)
		return function
	return wrapper


# Context manager
class context(object):
	def __init__(self,func,*args,**kwargs):
		self.obj = func(*args,**kwargs)		
	def __enter__(self):
		return self.obj
	def __exit__(self, type, value, traceback):
		self.obj.__exit__(type,value,traceback)

# Empty Context manager
class emptycontext(object):
	def __init__(self,func,*args,**kwargs):
		self.obj = func	
	def __call__(self,*args,**kwargs):
		return self
	def __enter__(self,*args,**kwargs):
		return self.obj
	def __exit__(self, type, value, traceback):
		try:
			self.obj.__exit__(type,value,traceback)
		except:
			pass
		return


# Null Context manager
class nullcontext(object):
	def __init__(self,*args,**kwargs):
		return
	def __call__(self,*args,**kwargs):
		return self
	def __enter__(self,*args,**kwargs):
		return self
	def __exit__(self, type, value, traceback):
		return


def nullfunc(*args,**kwargs):
	return


class nullclass(object):
	pass


# Call function with proper signature
def call(cls,func,*args,**kwargs):
	try:
		func = getattr(cls,func,func)	
	except:
		pass
	assert callable(func), "Error - cls.func or func not callable"

	params = inspect.signature(func).parameters.values()
	arguments = []
	keywords = {}    

	for param in params:
		name = param.name
		default = param.default
		kind = str(param.kind)
		if kind in ['VAR_POSITIONAL']:
			if name in kwargs:
				keywords[name] = kwargs.get(name,default)
			arguments.extend(args)
		elif kind in ['VAR_KEYWORD']:
			keywords.update(kwargs)
		elif kind not in ['POSITIONAL_OR_KEYWORD'] and default is param.empty:
			pass
		else:
			keywords[name] = kwargs.get(name,default)

	return func(*arguments,**keywords)


def empty(obj,*attrs):
	class Empty(obj.__class__):
		def __init__(self): pass
	newobj = Empty()
	newobj.__class__ = obj.__class__
	for attr in inspect.getmembers(obj):
		attr = attr[0]
		if attr in attrs:
			setattr(newobj,attr,copy.deepcopy(getattr(obj,attr)))
	newobj.__dict__.update({attr: obj.__dict__.get(attr) 
						   for attr in attrs if not getattr(newobj,attr,False)})
	return newobj


# Pool class, similar to multiprocessing.Pool
class Pooler(object):
	def __init__(self,processes,pool=None,initializer=None,initargs=(),maxtasksperchild=None,context=None):
		self.set_processes(processes)
		self.set_pool(pool)
		self.set_initializer(initializer)
		self.set_initargs(initargs)
		self.set_maxtasksperchild(maxtasksperchild)
		self.set_context(context)
		return
	@timing(False)	
	def __call__(self,module,func,iterable,args=(),kwds={},callback_args=(),callback_kwds={},callback=nullfunc,error_callback=nullfunc):
		with self.get_pool(
			processes=self.get_processes(),
			initializer=self.get_initializer(),
			initargs=self.get_initargs(),
			maxtasksperchild=self.get_maxtasksperchild(),
			context=self.get_context()) as pool:

			self.set_iterable(iterable)
			jobs = (getattr(pool,module)(
					func=wrapper(func,*args,**{**kwds,**i}),
					**(dict(callback=wrapper(callback,*callback_args,**{**callback_kwds,**i}),
					error_callback=wrapper(error_callback,*callback_args,**{**callback_kwds,**i}))
					if 'async' in module else dict()))
					for i in self.get_iterable())
						

			start = timeit.default_timer()	
			pool.close()
			pool.join()
			end = timeit.default_timer()

			if not self.get_null():
				logger.log(self.get_verbose(),"processes: %d, time: %0.3e"%(self.get_processes(),end-start))							

		return

	def set_pool(self,pool):
		attr = 'pool'  
		self.set_null()
		if self.get_null():
			value = nullPool
		elif value in [False]:
			value = nullPool
		elif pool in [None,True]:
			value = Pool 
		elif callable(pool):
			value = emptycontext(pool)	
		else:
			value = pool		
		setattr(self,attr,value)
		return 
	def get_pool(self,default=None):
		attr = 'pool'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value

	def set_processes(self,processes):
		attr = 'processes'
		default = 1
		processes = default if processes is None else processes
		processes = min(processes,MAX_PROCESSES-1)
		setattr(self,attr,processes)
		self.set_null()
		return
	def get_processes(self,default=None):
		attr = 'processes'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value

	def set_initializer(self,initializer):
		attr = 'initializer'
		value = initializer
		setattr(self,attr,value)
		return
	def get_initializer(self,default=None):
		attr = 'initializer'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value

	def set_initargs(self,initargs):
		attr = 'initargs'
		value = initargs
		setattr(self,attr,value)
		self.initargs = initargs
		return
	def get_initargs(self,default=None):
		attr = 'initargs'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value

	def set_maxtasksperchild(self,maxtasksperchild):
		attr = 'maxtasksperchild'
		value = maxtasksperchild
		setattr(self,attr,value)
		self.initargs = initargs
		return
	def get_maxtasksperchild(self,default=None):
		attr = 'maxtasksperchild'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value

	def set_context(self,context):
		attr = 'context'
		value = context
		setattr(self,attr,value)
		self.initargs = initargs
		return
	def get_context(self):
		attr = 'context'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value

	def set_verbose(self,verbose):  
		attr = 'verbose'
		if verbose is None:
			value = False 
		else:
			value = verbose
		setattr(self,attr,value)
		return 
	def get_verbose(self,default=None):
		attr = 'verbose'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value

	def set_iterable(self,iterable):
		attr = 'iterable'
		if isinstance(iterable,dict):
			keys = list(iterable)
			value = (dict(zip(keys,values)) 
							for values in itertools.product(*[iterable[key] 
															for key in keys]))
		elif isinstance(iterable,int):
			value = ({'i':i} for i in range(iterable))
		else:
			value = iterable
		setattr(self,attr,value)
		return
	def get_iterable(self,default=None):
		attr = 'iterable'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value		

	def set_null(self):
		attr = 'null'
		min_processes = 2
		value = self.get_processes() < min_processes
		setattr(self,attr,value)
		return
	def get_null(self,default=None):
		attr = 'null'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value	

# nullPool class, similar to multiprocessing.Pool
class Pool(multiprocessing.pool.Pool):
	pass

# nullPool class, similar to multiprocessing.Pool
class nullPool(object):
	def __init__(self,processes=None,initializer=None,initargs=(),maxtasksperchild=None,context=None):
		return
	@timing(False)	
	def apply(self,func,args=(),kwds={}):
		return func(*args,**kwds)
	@timing(False)
	def apply_async(self,func,args=(),kwds={},callback=nullfunc,error_callback=nullfunc):
		try:
			callback(func(*args,**kwds))
		except:
			error_callback(func(*args,**kwds))
		return
	@timing(False)	
	def map(self,func,iterable,chunksize=None):
		return list(map(func,iterable))
	@timing(False)		
	def map_async(self,func,iterable,chunksize=None,callback=nullfunc,error_callback=nullfunc):
		try:
			map(callback,list(map(func,iterable)))
		except:
			map(error_callback,list(map(func,iterable)))
		return 
	@timing(False)		
	def imap(self,func,iterable,chunksize=None):
		return list(map(func,iterable))
	@timing(False)	
	def imap_unordered(self,func,iterable,chunksize=None):
		return list(map(func,iterable))
	@timing(False)	
	def starmap(self,func,iterable,chunksize=None):
		return list(map(func,iterable))
	@timing(False)	
	def starmap_async(self,func,iterable,chunksize=None,callback=nullfunc,error_callback=nullfunc):
		try:
			map(callback,list(map(func,iterable)))
		except:
			map(error_callback,list(map(func,iterable)))
		return 		
	def close(self):
		pass
	def join(self):
		pass
	
	def set_processes(self,processes):
		attr = 'processes'
		default = 1
		processes = default if processes is None else processes
		processes = min(processes,MAX_PROCESSES-1)
		setattr(self,attr,processes)
		self.set_null()
		return
	def get_processes(self,default=None):
		attr = 'processes'
		if not hasattr(self,attr):
			self.set_processes(default)
		return getattr(self,attr)

	def set_verbose(self,verbose):  
		attr = 'verbose'
		if verbose is None:
			value = False 
		else:
			value = verbose
		setattr(self,attr,value)
		return 
	def get_verbose(self,default=None):
		attr = 'verbose'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value

	def set_iterable(self,iterable):
		attr = 'iterable'
		if isinstance(iterable,dict):
			keys = list(iterable)
			value = (dict(zip(keys,values)) 
							for values in itertools.product(*[iterable[key] 
															for key in keys]))
		elif isinstance(iterable,int):
			value = ({'i':i} for i in range(iterable))
		else:
			value = iterable
		setattr(self,attr,value)
		return
	def get_iterable(self,default=None):
		attr = 'iterable'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value	

	def set_null(self):
		attr = 'null'
		min_processes = 2
		value = self.get_processes() < min_processes
		setattr(self,attr,value)
		return
	def get_null(self,default=None):
		attr = 'null'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value	


# Parallelize iterations, similar to joblib
class Parallelize(object):
	def __init__(self,n_jobs,backend=None,parallel=None,delayed=None,prefer=None,verbose=False):
		self.set_n_jobs(n_jobs)
		self.set_backend(backend)
		self.set_parallel(parallel)
		self.set_delayed(delayed)
		self.set_prefer(prefer)
		self.set_verbose(verbose)
		return
	@timing(False)
	def __call__(self,func,iterable,values,*args,**kwargs):
		with self.get_parallel()(n_jobs=self.get_n_jobs(),backend=self.get_backend(),prefer=self.get_prefer()) as parallel:           
			self.set_iterable(iterable)
			jobs = (self.get_delayed()(func)(*args,**{**kwargs,**i}) 
					for i in self.get_iterable())

			start = timeit.default_timer()	
			values.extend(parallel(jobs))
			end = timeit.default_timer()

			if not self.get_null():
				logger.log(self.get_verbose(),"n_jobs: %d, time: %0.3e"%(self.get_n_jobs(),end-start))
		return 
	def __enter__(self,*args,**kwargs):
		return self
	def __exit__(self, type, value, traceback):
		return 

	def set_n_jobs(self,n_jobs):  
		attr = 'n_jobs'
		if n_jobs is None:
			n_jobs = 1  
		value = max(1,min(joblib.effective_n_jobs(n_jobs),MAX_PROCESSES-1))
		setattr(self,attr,value)
		self.set_null()		
		return 
	def get_n_jobs(self,default=None):
		attr = 'n_jobs'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value	

	def set_backend(self,backend):  
		attr = 'backend'
		if backend is None:
			value = 'loky'  
		else:
			value = backend
		setattr(self,attr,value)
		return 
	def get_backend(self,default=None):
		attr = 'backend'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value	

	def set_parallel(self,parallel):
		attr = 'parallel'  
		self.set_null()
		if self.get_null():
			value = nullParallel
		elif parallel in [False]:
			value = nullParallel
		elif parallel in [None,True]:
			value = Parallel 
		elif callable(parallel):
			value = emptycontext(parallel)	
		else:
			value = parallel
		setattr(self,attr,value)
		return 
	def get_parallel(self,default=None):
		attr = 'parallel'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value	

	def set_delayed(self,delayed):
		attr = 'delayed'  
		if delayed is None:
			value = Delayed 
		else:
			value = delayed
		setattr(self,attr,value)
		return 
	def get_delayed(self,default=None):
		attr = 'delayed'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value	

	def set_prefer(self,prefer):  
		attr = 'prefer'
		if prefer is None:
			value = None 
		else:
			value = prefer
		setattr(self,attr,value)
		return 
	def get_prefer(self,default=None):
		attr = 'prefer'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value			

	def set_verbose(self,verbose):  
		attr = 'verbose'
		if verbose is None:
			value = False 
		else:
			value = verbose
		setattr(self,attr,value)
		return 
	def get_verbose(self,default=None):
		attr = 'verbose'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value			

	def set_iterable(self,iterable):
		attr = 'iterable'
		if isinstance(iterable,dict):
			keys = list(iterable)
			value = (dict(zip(keys,values)) 
							for values in itertools.product(*[iterable[key] 
															for key in keys]))
		elif isinstance(iterable,int):
			value = ({'i':i} for i in range(iterable))
		else:
			value = iterable
		setattr(self,attr,value)
		return
	def get_iterable(self,default=None):
		attr = 'iterable'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value	

	def set_null(self):
		attr = 'null'
		min_n_jobs = 2
		value = self.get_n_jobs() < min_n_jobs
		setattr(self,attr,value)
		return
	def get_null(self,default=None):
		attr = 'null'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value			


# Parallel class using joblib
class Parallel(joblib.Parallel):
	pass


# null Parallel class using joblib
class nullParallel(joblib.Parallel):
	@timing(False)
	def __call__(self,jobs):
		return [func(*args,**kwargs) for func,args,kwargs in jobs]


# Delayed function call for parallelization using joblib
class Delayed(object):
	def __init__(self,function, check_pickle=None):
		self.function = joblib.delayed(function)
		return
	def __call__(self,*args,**kwargs):
		return self.function(*args,**kwargs)



# TODO: Modify to match class API of concurrent futures 
# (can be also similar to structure of __call__ and get_exucator_pool in place of get_pool, with a Future() and nullFuture() class as the context)
# Futures class, similar to concurrent.futures
class Futures(object):
	def __init__(self,processes,pool=None,initializer=None,initargs=(),maxtasksperchild=None,context=None):
		self.set_processes(processes)
		self.set_pool(pool)
		self.set_initializer(initializer)
		self.set_initargs(initargs)
		self.set_maxtasksperchild(maxtasksperchild)
		self.set_context(context)
		return
	@timing(False)	
	def __call__(self,module,func,iterable,args=(),kwds={},callback_args=(),callback_kwds={},callback=nullfunc,error_callback=nullfunc):
		with self.get_pool(
			processes=self.get_processes(),
			initializer=self.get_initializer(),
			initargs=self.get_initargs(),
			maxtasksperchild=self.get_maxtasksperchild(),
			context=self.get_context()) as pool:

			self.set_iterable(iterable)
			jobs = (getattr(pool,module)(
					func=wrapper(func,*args,**{**kwds,**i}),
					**(dict(callback=wrapper(callback,*callback_args,**{**callback_kwds,**i}),
					error_callback=wrapper(error_callback,*callback_args,**{**callback_kwds,**i}))
					if 'async' in module else dict()))
					for i in self.get_iterable())
						

			start = timeit.default_timer()	
			pool.close()
			pool.join()
			end = timeit.default_timer()

			logger.log(self.get_verbose(),"processes: %d, time: %0.3e"%(self.get_processes(),end-start))							

		return

	def set_pool(self,pool):
		attr = 'pool'  
		self.set_null()
		if self.get_null():
			value = nullPool
		elif value in [False]:
			value = nullPool
		elif pool in [None,True]:
			value = Pool 
		elif callable(pool):
			value = emptycontext(pool)	
		else:
			value = pool		
		setattr(self,attr,value)
		return 
	def get_pool(self,default=None):
		attr = 'pool'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value

	def set_processes(self,processes):
		attr = 'processes'
		default = 1
		processes = default if processes is None else processes
		processes = min(processes,MAX_PROCESSES-1)
		setattr(self,attr,processes)
		self.set_null()
		return
	def get_processes(self,default=None):
		attr = 'processes'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value

	def set_initializer(self,initializer):
		attr = 'initializer'
		value = initializer
		setattr(self,attr,value)
		return
	def get_initializer(self,default=None):
		attr = 'initializer'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value

	def set_initargs(self,initargs):
		attr = 'initargs'
		value = initargs
		setattr(self,attr,value)
		self.initargs = initargs
		return
	def get_initargs(self,default=None):
		attr = 'initargs'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value

	def set_maxtasksperchild(self,maxtasksperchild):
		attr = 'maxtasksperchild'
		value = maxtasksperchild
		setattr(self,attr,value)
		self.initargs = initargs
		return
	def get_maxtasksperchild(self,default=None):
		attr = 'maxtasksperchild'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value

	def set_context(self,context):
		attr = 'context'
		value = context
		setattr(self,attr,value)
		self.initargs = initargs
		return
	def get_context(self):
		attr = 'context'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value

	def set_verbose(self,verbose):  
		attr = 'verbose'
		if verbose is None:
			value = False 
		else:
			value = verbose
		setattr(self,attr,value)
		return 
	def get_verbose(self,default=None):
		attr = 'verbose'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value

	def set_iterable(self,iterable):
		attr = 'iterable'
		if isinstance(iterable,dict):
			keys = list(iterable)
			value = (dict(zip(keys,values)) 
							for values in itertools.product(*[iterable[key] 
															for key in keys]))
		elif isinstance(iterable,int):
			value = ({'i':i} for i in range(iterable))
		else:
			value = iterable
		setattr(self,attr,value)
		return
	def get_iterable(self,default=None):
		attr = 'iterable'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value		

	def set_null(self):
		attr = 'null'
		min_processes = 2
		value = self.get_processes() < min_processes
		setattr(self,attr,value)
		return
	def get_null(self,default=None):
		attr = 'null'
		if not hasattr(self,attr):
			getattr(self,'set_%s'%(attr))(default)
		value = getattr(self,attr) 
		return value




def catch(update,exceptions,raises,iterations=1000):
	'''
	Wrapper to loop through function and catch exceptions, updating args and kwargs until no exceptions
	
	Args:
		update (callable): function with signature update(exception,*args,**kwargs) to update *args and **kwargs after exceptions
		exceptions (tuple): exceptions that invoke updating of *args and **kwargs
		raises (tuple): exceptions that raise exception and do not update *args and **kwargs
		iterations (int): maximum number of iterations before exiting
	Returns:
		func (callable): wrapped function for catching exceptions
	'''
	def wrap(func):
		@functools.wraps(func)
		def wrapper(*args,**kwargs):
			result = None
			exception = Exception
			iteration = 0
			while (exception is not None) and (iteration < iterations):
				try:
					result = func(*args,**kwargs)
					exception = None
				except Exception as e:
					exception = e
					if isinstance(exception,exceptions):
						update(exception,*args,**kwargs)
					elif isinstance(exception,raises):
						raise exception
				iteration += 1
			if exception is not None:
				raise exception
			return result
		return wrapper
	return wrap



def getmethod(obj,attr,default=None):
	if default is None:
		def default(*args,**kwargs): 
			return obj
	value = (getattr(obj,attr) if (getattr(obj,attr,None) is not None) else default)
	return value

def getattribute(obj,attr,default=None): 
	value = (getattr(obj,attr) if (getattr(obj,attr,None) is not None) else default)
	return value

def setattribute(obj,attr,value):
	setattr(obj,attr,value)
	return 

def chunkfunc(index,data,shape,chunk,sparsity,where,function,wrapper,serialize,format,dtype,n_jobs,verbose=False):
	'''
	Chunk operation on N arrays
	
	Args:
		index (list[int]): list of N integers for indices of each chunk
		shape (list[int]): list of N integers for length of each data array
		chunk (list[int]): list of N integers for size of each chunk for each data array
		sparsity (int,float,list): Sparsity of chunk. If int, sparsity number of smallest elements along that axis of the chunk, if float, all elements with abs(chunk[axis]) < sparsity		
		where (ndarray,sparse_matrix): array of shape shape, of where to compute chunks
		data (list[array]): list of N data arrays		
		function (callable): callable function that has signature function(data,slices,shape,where)
							 for sliced arrays based on index and chunk
		wrapper (callable): callable function on chunked return values and slices from function with signature wrapper(shape,values)		
		serialize (list): list of integers of arrays that are processed in serial
		format (str): output format (sparse types 'csr','csc', etc. or 'array' for dense numpy array)		
		dtype (str,data-type): data type of output
		n_jobs (int): Number of parallel jobs
		verbose (bool,int): Print out details of chunking							 
	Returns:
		value: Return value of function
		bounds (list): list of [start,stop,step] bounds of chunk	
	'''

	def loop(index,data,shape,chunk,where,format,dtype,verbose):
		# Slices for index and chunk size
		bounds = [[index[i]*chunk[i],min(shape[i],(index[i]+1)*chunk[i]),1] for i in range(N)]
		slices = [slice(*bounds[i]) for i in range(N)]
		lengths = [min(chunk[i],shape[i]-index[i]*chunk[i]) for i in range(N)]

		# Compute chunk value
		value = function(data,slices,shape,where)
		# value = asdtype(asformat(value,format),dtype)
		value = asdtype(value,dtype)

		# Print out chunking details		
		# logger.log(verbose,'Chunk %s with shape %r'%(','.join(['%d/%d'%(index[i],shape[i]//chunk[i]) for i in range(N)]),chunk))

		return value,bounds

	# Number of arrays
	N = min(len(index),len(shape),len(chunk),len(data))

	# Serialize any part of chunk for index that is null
	serialize = [] if serialize is None else serialize
	indexes = (ind for ind in itertools.product(*[range(int(shape[i]//chunk[i]+1)) if i in serialize else [index[i]] for i in range(N)]))

	values = []
	for index in indexes:
		value = loop(index,data,shape,chunk,where,format,dtype,verbose)
		values.append(value)

	value = wrapper(values,shape,chunk,sparsity,where,format,dtype)

	return value



def chunkify(data,shape,chunk,sparsity,where,function,wrapper=None,index=None,serialize=None,format=None,dtype=None,n_jobs=1,verbose=False):
	'''
	Chunk operation on N arrays
	
	Args:
		data (list[array]): list of N data arrays of lengths
		shape (list[int]): list of N integers for length of each data array		
		chunk (int, list[int]): list of N integers for size of each chunk for each data array
		sparsity (int,float,list): Sparsity of chunk. If int, sparsity number of smallest elements along that axis of the chunk, if float, all elements with abs(chunk[axis]) < sparsity		
		where (ndarray,sparse_matrix): array of shape shape, of where to compute chunks
		function (callable): callable function that has signature function(data,slices,shape,where)
							 for sliced arrays based on index and chunk
		wrapper (list,callable): callable function on chunked return values and slices from function,
			with signature wrapper(values,shape,chunk,sparsity,where,format,dtype). List if nested chunkify calls for parallel/serial calls
		index (list[int]): list of N integers for indices of each chunk		
		serialize (list): list of integers of arrays that are processed in serial, or list of lists if nested if nested chunkify calls for parallel/serial calls
		format (str): output format (sparse types 'csr','csc', etc. or 'array' for dense numpy array)				
		dtype (str,data-type): data type of output
		n_jobs (int): Number of parallel jobs
		verbose (bool,int): Print out details of chunking
	Returns:
		values: Return value of wrapper
		slices: Slices of arrays
		
	'''
	
	# Serial Wrapper
	def wrapper_serial(values,shape,chunk,sparsity,where,format,dtype):

		# values = [(bounds_0,values_0),...,(bounds_n-1,values_n-1)]
		# where bounds are bounds of N dimensional data, and 
		# bounds_j = [[start_j_0,stop_j_0,step_j_0]...,[start_j_n-1,stop_j_n-1,step_j_n-1]], 
		# with slice lengths lengths_j = [lengths_j_0,....,lengths_j_N-1] and total size size_j = prod(lengths_j)
		# values_j = [value_j_0,...,value_j_size_j-1] of length size_j
		
		# Get number of values, where 
		n = len(values)			

		# Get whether sparse values
		sparse = issparse(where)

		# Get bounds = [[bounds_0_0,...,bounds_0_N-1],...,[bounds_n-1_0,...,bounds_n-1_N-1]]
		bounds = [values[i][1] for i in range(n)]

		# Get out = [[values_0_0,...,values_0_length_0-1],...,[values_n-1_0,...,values_n-1_length_n_1-1]]
		out = [values[i][0] for i in range(n)]
		values = out

		# Get array of bounds and values
		out = concatenate(out,axis=-1)

		bounds = asndarray(bounds)
		bounds = [[bounds[:,i,0].min(),bounds[:,i,1].max(),bounds[:,i,2].max()] for i in range(bounds.shape[1])]

		# Sparsify array
		out = sparsify(out,sparsity,axis=0,format=format)

		return out,bounds

	# Parallel Wrapper
	def wrapper_parallel(values,shape,chunk,sparsity,where,format,dtype):

		# values = [(bounds_0,values_0),...,(bounds_n-1,values_n-1)]
		# where bounds are bounds of N dimensional data, and 
		# bounds_j = [[start_j_0,stop_j_0,step_j_0]...,[start_j_n-1,stop_j_n-1,step_j_n-1]], 
		# with slice lengths lengths_j = [lengths_j_0,....,lengths_j_N-1] and total size size_j = prod(lengths_j)
		# values_j = [value_j_0,...,value_j_size_j-1] of length size_j
		
		# Get number of values, where 
		n = len(values)			

		# Get whether sparse values
		sparse = issparse(where)

		# Get bounds = [[bounds_0_0,...,bounds_0_N-1],...,[bounds_n-1_0,...,bounds_n-1_N-1]]
		bounds = [values[i][1] for i in range(n)]

		# Get values = [[values_0_0,...,values_0_length_0-1],...,[values_n-1_0,...,values_n-1_length_n_1-1]]
		values = [values[i][0] for i in range(n)]

		# Get array of bounds and values
		out = concatenate(values,axis=0)
		out = asformat(out,format=format)

		return out



	# Number of arrays
	N = min(len(data),len(shape))

	# Chunk sizes
	if not isinstance(chunk,list):
		chunk = [chunk,chunk]
	chunk = [max(1,min(shape[i],shape[i] if chunk[i] is None else int(shape[i]*chunk[i]) if isinstance(chunk[i],(float,np.float64)) else chunk[i])) 
			 for i in range(N)]


	# Default wrapper (concatenation of array chunks)
	wrapper = [wrapper_parallel,wrapper_serial] if wrapper is None else wrapper

	# Get wrapper and serial (list if serial is list for nested chunkify calls)
	wrapper = [wrapper] if not isinstance(wrapper,list) else wrapper
	serialize = [] if serialize is None else serialize
	
	# Check if chunkify is a parent loop (wrapper has multiple items for nested calls)
	# and if chunkify is a child loop (index is not None), and adjust wrapper accordingly
	parent = len(wrapper[1:]) > 1
	child = index is not None
	

	# Parallel
	parallel = Parallelize(n_jobs=n_jobs,verbose=verbose)

	# Loop
	func = chunkify if parent else chunkfunc
	wrappers = wrapper[1:] if parent else wrapper[1]
	wrapper = wrapper[0]
	iterable = ({'index':index} for index in itertools.product(*[range(int(shape[i]//chunk[i]+1)) if i not in serialize else [index[i] if child else -1] for i in range(N)]))
	values = []
	args = []
	kwargs = {
		'data':data,'shape':shape,'chunk':chunk,'sparsity':sparsity,'where':where,
		'function':function,'wrapper':wrappers,'serialize':serialize,
		'format':format,'dtype':dtype,'n_jobs':n_jobs,'verbose':verbose
		}

	parallel(func,iterable,values,*args,**kwargs)


	# Wrapped value
	value = wrapper(values,shape,chunk,sparsity,where,format,dtype)

	return value


# Return tuple of returns
def returnargs(returns):
	if isinstance(returns,tuple) and len(returns) == 1:
		return returns[0]
	else:
		return returns


# Get range from slice
def slice_range(slices,format=None,zero=False):
	def get(obj,default):
		return obj if obj is not None else default
	if isinstance(slices,slice):
		if not zero:
			ranges = range(get(slices.start,0),slices.stop,get(slices.step,1))
		else:
			ranges = range(0,slices.stop - get(slices.start,0),get(slices.step,1))
	else:
		ranges = slices
	ranges = format(ranges) if ranges is not None else ranges
	return ranges

# Get slices index with indices, where slice has maximum value of n
def slice_index(slices,indices,n):
	if isinstance(slices,slice):
		defaults = (0,n,1)
		slices = slices.indices(n)
		slices = tuple([s if s is not None else d for s,d in zip(slices,defaults)])
		arr = np.array(list(range(*slices)))
	else:
		arr = np.array(slices)
	arr = arr[indices]
	return arr


# Set diagonal of array in place
def fill_diagonal(arr,values,offset=0):
	'''
	Set diagonal of ndarray with values in place
	
	Args:
		arr (ndarray): Array to be set in place with shape (...,n,...m,...) at axis1 and axis2
		values (ndarray) Array of values of size (n-abs(offset))*(m-abs(offset))
		offset (int): Offset from diagonal of array
		
	'''
	assert arr.ndim == 2, "Error - ndim != 2 and array is not matrix"
	n,m = arr.shape
	arr.ravel()[max(offset,-m*offset):max(0,(m-offset))*m:m+1] = values.ravel()
	return


def sortarr(arr,axis=0):
	'''
	Sort array along axis, with key by all elements along other axes
	
	Args:
		arr (ndarray): array to be sorted
		axis (int): axis to sort along
	Returns
		arr (ndarray): sorted array
	'''
	shape = list(arr.shape)
	size = shape.pop(axis)
	arr = [a for a in np.swapaxes(arr,axis,0).reshape((size,-1))]
	arr = np.swapaxes(np.array(list(sorted(arr,key=lambda a: tuple(a)))).reshape((size,*shape)),axis,0)
	return arr


def within(arr,indices,axis,bounds):
	'''
	Return arr where unique indices along axis are all within bounds (open bounds)
	
	Args:
		arr (ndarray): array to be searched of shape (n0,n1,...naxis,...,nndim)
		indices (int,list,ndarray): indices along axis to search
		axis (int): axis to search
		bounds (list): upper and lower bounds that arr must be within
	Returns:
		mask (ndarray): mask of elements within bounds of shape (n0,n1,...naxis-1,naxis+1...,nndim)
		
	'''
	if not isinstance(indices,(list,np.ndarray)):
		indices = [indices]
	bounds = [np.array(b) for b in bounds]
	
	arr = take(arr,indices,axis)
	mask = ((arr>bounds[0]) & (arr<bounds[1])).all(axis=axis)
	return mask


def similarity_matrix(x,y,adjacency=None,metric=2,sparsity=None,directed=False,chunk=None,format='csr',dtype=None,eliminate_zeros=False,kind='mergesort',n_jobs=None,verbose=False):
	'''
	Get comparison of ndarrays y-x, either by euclidean p-norm for p>0 or positive sign if p=0 or negative sign if p=-1
	
	Args:
		x (ndarray): Array to be measured of shape (n,(d))
		y (ndarray): Array to be measured of shape (m,(d))
		adjacency (ndarray,sparse_matrix): array of shape (n,m) of where to compute differences between elements		
		metric (int,float,str,callable): Type of metric, or order of euclidean norm over all axis. 
			If metric>0, then euclidean metric-norm of y-x, if metric=0, then sign of y-x, if metric<0, then signed metric-norm of y-x. 
			If callable, is function which accepts broadcasted x and y of shapes (n,(d),m,(d)) and returns similarity metric of shape (n,m)		
		sparsity (int,float,list): Sparsity of similarity matrix. If int, sparsity number of smallest elements along that axis of the similarity matrix, if float, all elements with \vert similarity matrix[axis]\vert < sparsity				
		directed (bool): Whether data is directed between and x and y 		
		chunk (int,float,list,None): Size of chunks along first axes of x,y to perform distance calculation
		format (str): Matrix format (sparse types 'csr','csc', etc. or 'array' for dense numpy array)		
		dtype (str,data-type): data type of output		
		eliminate_zeros (bool): eliminate elements with explicit zero similarity
		kind (str): Sort algorithm
		n_jobs (int): Number of parallel jobs
		verbose (bool,int): Print out details of computation		
	Returns:
		out (sp.sparse.format_matrix): p-norm distance between pairwise points in x and y of shape	
	'''

	# Get size of arrays and chunk sizes

	def formatter(dtype,format):
		def decorator(func):
			@functools.wraps(func)
			def wrapper(*args,**kwargs):
				fmt = 'csr'
				out = func(*args,**kwargs)
				out = out.astype(dtype)
				out = out.asformat(format) if issparse(out) else getattr(sp.sparse,'%s_matrix'%(fmt))(out).asformat(format)
				return out


	n,m = x.shape[0],y.shape[0]	
	shape = (n,m)

	# Get sparsity
	sparse = issparse(adjacency)
	format = adjacency.getformat() if (sparse and (format is None)) else 'array' if format is None else format

	# Update norm order depending on directedness of graph
	if directed:	
		if isinstance(metric,(int,np.integer,float,np.float64)):
			metric = -abs(metric)
		elif isinstance(metric,str):
			metric = '_'.join([metric,'directed'])
		else:
			metric = metric
	else:
		metric = metric
	metric = 'euclidean' if metric is None else metric

	# Comparisons
	if adjacency is None:
		def func(x,y,where=None):
			def func_x(x,y):
				out = broadcast(x,axes=[x.ndim//2 + dim for dim in range(y.ndim//2)],shape=y.shape[y.ndim//2:],newaxis=False)
				return out
			def func_y(x,y):
				out = broadcast(y,axes=[dim for dim in range(x.ndim//2)],shape=x.shape[:x.ndim//2],newaxis=False)
				return out
			out = [outer(func,x,y,where=where) for func in [func_x,func_y]]
			return out
	else:
		def func(x,y,where=None):
			def func_x(x,y):
				out = x
				return out
			def func_y(x,y):
				out = y
				return out
			out = [outer(func,x,y,where=where) for func in [func_x,func_y]]
			return out			

	# metric function takes arrays with shape (n,m,(d)) and returns comparison with shape (n,m)
	if callable(metric):
		_metric = metric
		def function(data,slices,shape,where):
			return _metric(*func(data[0][slices[0]],data[1][slices[1]],where=where[slices[0],:][:,slices[1]] if where is not None else None))
	elif metric == 'euclidean':
		ord = 2
		dtype = 'float'
		def function(data,slices,shape,where):
			out = None
			N = min(len(data),len(slices),len(shape))
			full = not any([data[i].shape[0] == 0 for i in range(N)])
			if full:
				out = norm(getdiag(data[0][slices[0]][...,None,None] - data[1][slices[1]][None,None,...],axis1=1,axis2=-1),ord=ord,axis=-1)
				out = where[slices[0],:][:,slices[1]].multiply(out) if where is not None else out
			return out
	elif metric == 0:
		# Sign of cos(x,y) for all higher dimensions
		ord = 2
		dtype = 'int'
		def function(data,slices,shape,where):
			return normsigndiff(*func(data[0][slices[0]],data[1][slices[1]],where=where[slices[0],:][:,slices[1]] if where is not None else None),
								axis=-1,ord=ord,normed=False,signed=True)
	elif metric > 0:        
		# Euclidean p norm of x - y for all higher dimensions
		ord = abs(metric)
		dtype = 'float'
		def function(data,slices,shape,where):
			return normsigndiff(*func(data[0][slices[0]],data[1][slices[1]],where=where[slices[0],:][:,slices[1]] if where is not None else None),
								axis=-1,ord=ord,normed=True,signed=False)
	elif metric < 0:
		# Euclidean p norm and Sign of cos(x,y) for all higher dimensions
		ord = abs(metric)        
		dtype = 'float'
		def function(data,slices,shape,where):
			return normsigndiff(*func(data[0][slices[0]],data[1][slices[1]],where=where[slices[0],:][:,slices[1]] if where is not None else None),
								axis=-1,ord=ord,normed=True,signed=True)
	

	# Comparisons array
	# broadcast function returns x with shape (n,(d),1,...,1) to (n,(d),m,(d)) or y with shape (1,...,1,m,(d)) to (n,(d),m,(d))
	# outer returns x or y with shape (n,m,(d))
	# metric returns comparison with shape (n,m)
	
	N = 2
	data = [x,y]
	shape = [len(data[i]) for i in range(N)]
	where = adjacency
	index = None
	serialize = [1]
	wrapper = None

	# Loop
	out = chunkify(data,shape,chunk,sparsity,where,function,wrapper,index,serialize,format,dtype,n_jobs,verbose)

	return out




def adjacency_matrix(n,weights=None,conditions=None,format=None,dtype=int,tol=None,atol=None,rtol=None,kind='mergesort',diagonal=False,return_argsort=False,return_counts=False,verbose=False):
	'''
	Create adjacency matrix for n vertices with weights depending on condition
	
	Args:
		n (int): size of adjacency matrix rows
		weights (ndarray,sparse_matrix,None): weights matrix of vertices, possibly sparse
		conditions (int,tuple,callable,None): integer for k nearest neighbours, or tuple of (open) bounds on nearest neighbours, or callable function with weights,argsort,counts arguments to make adjacency elements non-zero
		format (str): Matrix format (sparse types 'csr','csc', etc. or 'array' for dense numpy array)		
		dtype (str,data-type): data type of output		
		tol (float): Tolerance for rank and singularity of matrix
		atol (float): Absolute tolerance of difference between unique elements
		rtol (float): Relative tolerance of difference between unique elements
		kind (str): Sort algorithm                
		diagonal (bool): explicitly include diagonal of adjacency
		return_argsort (bool): Return sort order of elements equal within tolerance
		return_counts (bool): Return number of occurrences of unique elements within tolerance
		verbose (bool,int): Print out details of adjacency calculation							 
	Returns:
		adjacency (ndarray,sparse_matrix,sparse_array): adjacency matrix with shape (n,n), possibly sparse
		argsort (ndarray): Sort order of elements that are equal within tolerance along higher dimensions
		counts (list): list of ndarrays of counts of unique elements if return_counts is True				
	'''


	# Size of adjacency matrix
	if n is None and weights is not None:
		n = weights.shape[0]
	elif n is None and (weights is None):
		raise "Error - adjacency size and weights arguments are None"
		return



	# Default weights as nearest neighbours in vertices
	if weights is None:
		offsets = [0,1,-1]
		diags = [0,1,1]
		weights = sp.sparse.diags(diags,offsets,shape=(n,n),dtype=float)


	# Get weights shape
	n,m = weights.shape

	# Get if weights are sparse
	sparse = issparse(weights)

	# Default conditions as nearest neighbours
	isint = isinstance(conditions,(int,np.integer))
	istuple = isinstance(conditions,tuple)
	isnone = conditions is None

	if isnone:
		def conditions(weights,argsort,counts):
			out = weights.astype(bool)
			out.eliminate_zeros()
			return out
	elif isint or istuple:
		argsortbounds = [0-1 if (isnone or isint or conditions[0] is None) else conditions[0],
						 n if (isnone or conditions[1] is None) else conditions+1 if (isint) else conditions[1],
						 []]
		def conditions(weights,argsort,counts):
			sparse = issparse(weights) or issparse(argsort)
			condition = [gt(argsort,argsortbounds[0]) ,lt(argsort,argsortbounds[1])]
			condition = multiply(*condition)
			return condition

	# Sort weights
	if not isnone:
		argsort,counts = unique_argsort(np.abs(weights),return_counts=True,atol=atol,rtol=rtol,signed=False,kind=kind)
	else:
		argsort,counts = None,None


	# Create adjacency matrix
	adjacency = conditions(weights,argsort,counts)


	# Set adjacency format and dtype
	adjacency = asformat(adjacency,format)
	adjacency = asdtype(adjacency,dtype)


	# Explicitly include diagonal in adjacency
	if diagonal:
		adjacency = setdiag(adjacency,diagonal)		

	# Returns
	returns = ()

	returns += (adjacency,)

	if return_argsort:
		returns += (argsort,)
	if return_counts:
		returns += (counts,)

	return returnargs(returns)

			  
def unique_argsort(arr,return_counts=False,atol=None,rtol=None,signed=False,kind='mergesort'):             
	'''
	Get sort indices of unique elements within absolute and relative tolerances
	
	Args:
		arr (ndarray,sparse_matrix,sparse_array): Array of shape(n,m) to be searched, possibly sparse
		return_counts (bool): Return number of occurrences of unique elements within tolerance		
		atol (float): Absolute tolerance of difference between unique elements
		rtol (float): Relative tolerance of difference between unique elements
		signed (bool): Sort by absolute value and then return argsort by sign of grouped unique elements		
		kind (str): Sort algorithm
	Returns:
		argsort (ndarray,sparse_matrix,sparse_array): Sort order of elements that are equal within tolerance along higher dimensions, possibly sparse
		counts (list): list of ndarrays of counts of unique elements if return_counts is True	
	'''


	# Get shape of arr
	ndim = arr.ndim
	if ndim == 1:
		arr = arr.reshape((1,-1))
	n,m = arr.shape

	# Check if arr is sparse
	sparse = issparse(arr)
	if sparse:
		format = arr.getformat()
		argsort = arr.astype(int,copy=True)
		counts = [[] for i in range(n)]
	else:
		format = 'array'
		argsort = zeros((n,m),dtype=int)
		counts = [[] for i in range(n)]

	# Sort array along axis dimension
	for i in range(n):
		result = unique_tol(arr[i],return_unique=False,return_counts=return_counts,return_argsort=True,atol=atol,rtol=rtol,signed=signed,kind=kind)
		if return_counts:
			counts[i] = result[0]
			result = result[1]

		argsort[i] = result

	if ndim == 1:
		argsort = argsort[-1]
		counts = counts[-1]

	returns = ()

	returns += (argsort,)

	if return_counts:
		returns += (counts,)

	return returnargs(returns)



def unique_tol(arr,
			  return_unique=True,return_index=False,return_counts=False,return_argsort=False,return_sets=False,
			  atol=None,rtol=None,
			  signed=False,
			  kind='mergesort'):             
	'''
	Get unique elements within absolute and relative tolerances
	
	Args:
		arr (ndarray,sparse_matrix,sparse_array): Array to be searched, possibly sparse
		return_unique (bool): Return unique elements
		return_index (bool): Return indices of unique elements
		return_counts (bool): Return number of occurrences of unique elements within tolerance
		return_argsort (bool): Return sort order of elements equal within tolerance
		return_sets (bool): Return indices grouped by their argsort
		atol (float): Absolute tolerance of difference between unique elements
		rtol (float): Relative tolerance of difference between unique elements
		signed (bool): Sort by absolute value and then return argsort by sign of grouped unique elements
		kind (str): Sort algorithm                
	Returns:
		unique (ndarray): Unique elements of array
		indices (ndarray): Indices of unique elements if return_index is True
		counts (ndarray): Counts of unique elements if return_counts is True
		argsort (ndarray,sparse_matrix,sparse_array): Sort order of elements that are equal within tolerance if return_argsort is True
		sets (list): Sets of indices grouped by their sort order
	Example:
		arr = [-4,2,4,5,1,-2,-2] 
		has absolute value sorting rank (if signed is False)
		argsort = [2,1,2,3,0,1,1]
		and signed sorting rank
		argsort = [4,3,5,6,0,1,2] 
		since negative numbers are sorted before positive numbers of equal absolute value.	
	'''



	# Check if arr is sparse
	sparse = issparse(arr)
	if sparse:
		format = arr.getformat()
		size,dtype = arr.shape[-1],arr.dtype 
		inds = arr.indices
		indptr = arr.indptr
		arr = arr.data.ravel()
	else:
		format = 'array'
		size,dtype = arr.shape[-1],arr.dtype 		
		inds = arange(size)
		arr = arr.ravel()

	# Get absolute value of array if signed is True
	if not signed:
		_arr = arr
	else:
		_arr = np.abs(arr)

	# Get mask of unique elements
	n = arr.size
	mask = np.empty(n,dtype=bool)

	# Sort array
	indices = _arr.argsort(kind=kind)

	# Get difference between nearest elements
	diff = np.diff(_arr[indices])

	# Get indices where adjacent sorted elements differ by greater than tolerances
	mask[:1] = True
	mask[1:] = ~isclose(diff,rtol=rtol,atol=atol)

	# Get return values
	returns = ()

	# Get unique elements
	if return_unique:
		unique = _arr[indices][mask]
		returns += (unique,)

	# Get index of unique elements
	if return_index:
		index = inds[indices[mask]]
		returns += (index,)

	# Get counts of unique elements
	# Get argsort order of equal elements within tolerance
	# Get indices grouped by their argsort
	if return_counts or return_argsort or return_sets:
		counts = np.diff(concatenate(np.nonzero(mask) + ([n],)))

		if return_counts:
			returns += (counts,)

		counts = [0,*cumsum(counts)]		
		argsort = zeros(n,dtype=int)
		sets = []
		for j in range(len(counts)-1):
			if not signed:
				i = j
			else:
				i = counts[j] + arr[indices[counts[j]:counts[j+1]]].argsort(kind=kind).argsort(kind=kind)
			sets.append(indices[counts[j]:counts[j+1]])
			argsort[sets[-1]] = i


		if return_argsort:
			if sparse:
				argsort = getattr(sp.sparse,'%s_matrix'%(format))((argsort,inds,indptr),shape=(1,size),dtype=int)

			returns += (argsort,)

		if return_sets:
			sets = [inds[s].tolist() for s in sets]
			returns += (sets,)

	return returnargs(returns)



def unique_int(arr,kind='mergesort'):
	'''
	Get indices of unique integers in integer array
	
	Args:
		arr (array): dimensional array of size n, flattened if multidimensional
		kind (str): Sort algorithm
	Returns:
		indices (dict): dictionary of unique elements and arrays of indices of each unique element
	'''

	# Get array shape an dtype
	arr = arr.ravel()
	n,dtype = arr.size,arr.dtype

	# if n == 0:
	# 	return 

	assert dtype in [int,np.integer], "Error - Array is not integer dtype"

	# Get unique elements and their indices of sorted array
	argsort = arr.argsort(kind=kind)
	unique,inds = np.unique(arr[argsort],return_index=True)
	inds = np.insert(inds,obj=inds.size,values=n)
	
	# Get indices of each unique element based on indices of unique sorted elements
	indices = {unique[i]: np.sort(argsort[inds[i]:inds[i+1]]) for i in range(inds.size-1)}

	return indices




def neighbourhood(x,y,size,basis,order=None,adjacency=None,indices=None,metric=2,variable=None,unique=True,diagonal=False,argsortbounds=None,weightbounds=None,
					strict=False,tol=None,atol=None,rtol=None,chunk=None,format=None,dtype=None,kind='mergesort',
					verbose=False,n_jobs=None,return_weights=False):
	'''
	Get indices of neighbourhood of indices of array y that are nearest to x
	
	Args:
		x (ndarray,DataFrame): Array to be measured of shape (n,(d))
		y (ndarray,DataFrame): Array to be measured of shape (m,(d))
		size (int): Size of neighbourhood
		basis (str,callable): function to yield basis, or string in ['vandermonde']
		order (int): order of basis matrix for linearly independent points
		adjacency (ndarray,sparse_matrix): array of shape (n,m) of where to compute differences between elements				
		indices (array,None): Allowed indices of y to compare to x
		metric (int,float,str,callable): Type of metric, or order of euclidean norm over all axis. 
		ord (int,float): Order of euclidean norm
		variable (str,list): dataframe column labels to be used for data	
		unique (bool): Include unique basis terms q = choose(p+n,n) else q = (p^(n+1)-1)/(p-1)				
		diagonal (bool): explicitly include diagonal of adjacency		
		argsortbounds (list): Minimum and maximum ranked distance between neighbors (open boundaries), and disallowed values, between -1 and m+1, where None is replaced by -1 or m+1
		weightbounds (list): Minimum and maximum weight distance between neighbors (open boundaries), and disallowed values, between -np.inf and np.inf, where None is replaced by -np.inf or np.inf
		strict (bool): Whether size of neighborhood is exactly size, or rounded to minimum set of nearest neighbors within bounds
		tol (float): Tolerance for rank and singularity of basis matrix of points
		atol (float): Absolute tolerance of difference between unique elements
		rtol (float): Relative tolerance of difference between unique elements
		chunk (int,float,None): Size of chunks along 0th axis to perform distance calculation		
		format (str): Matrix format (sparse types 'csr','csc', etc. or 'array' for dense numpy array)		
		dtype (str,data-type): data type of output		
		kind (str): Sort algorithm        
		verbose (bool,int): Print out details of neighbourhood
		n_jobs (int): Number of parallel jobs		
		return_weights (bool): Return weights of neighbourhood points in y that are nearest to x        
	Returns:
		indices (list,ndarray): Indices of neighborhood of points in y (x) that are closest to each x (y). ndarray if each x (y) has same size of neighborhood.
		weights (list,ndarray): If return_weights is True, Weights of neighborhood between closest y (x) values to each x (y). ndarray if each x has same size of neighborhood.
	Example:
		Arguments of:
		x = [[1,0],[4,-4],[5,2],[2,-3]]
		y = [[0,0],[4,3],[3,-2]]        
		indices = None
		size = 2
		ord = 2
		argsortbounds = (0,2,None)
		
		returns the indices of y that is closest to each point in x.
		This example will return
		indices = [0,2,1,2] since x[0] is closest to y[0], x[1] is closest to y[2] etc.
	'''
	
	# Get sparsity
	exists = isarray(adjacency)
	sparse = issparse(adjacency)

	# Get array if dataframe
	isvariable = variable is not None
	if isdataframe(x):
		if isvariable:
			x = x[variable]
		x = x.to_numpy()
	if isdataframe(y):
		if isvariable:
			y = y[variable]
		y = y.to_numpy()


	# Get number of elements in x and y
	n,m = x.shape[0],y.shape[0]
	p,d = x.shape[1:],y.shape[1:]
	p = prod(p,dtype=int)


	# Get size and order of neighbourhood
	if order is None:
		if size is None:
			size = n
		order = 1
	else:
		if size is None:
			size = ncombinations(p,order,unique=unique)
	if size in [1]:
		order = 0
	sparsity = min(m,4*p*size*max(order,1))

	# Get allowed indices of y to compare to x
	if indices is None:
		indices = arange(m)
	indices = np.array(indices)

	y = y[indices]

	adjacency = adjacency[:,indices] if exists else adjacency


	# Get euclidean distance weight function
	weights = similarity_matrix(x,y,adjacency=adjacency,metric=metric,sparsity=sparsity,directed=True,chunk=chunk,kind=kind,format=format,dtype=dtype,n_jobs=n_jobs)

	# Set conditions to choose nearest neighbours such that neighborhood is at least size
	conditions = nearest_neighbourhood(size=size,strict=strict,atol=atol,rtol=rtol,argsortbounds=argsortbounds,weightbounds=weightbounds)


	# Get adjacency matrix, based on indices of nearest neighbours as per weights, as per conditions on neighborhood bounds and size and strict
	adjacency = adjacency_matrix(n,weights,conditions,tol=tol,atol=atol,rtol=rtol,format=format,dtype=dtype)

	# Get indices of linearly independent
	inds = linearly_independent(x,y,order,size,basis,
		adjacency=adjacency,dimension=None,ones=True,unique=unique,
		tol=tol,atol=atol,rtol=rtol,kind=kind,diagonal=diagonal,verbose=verbose,return_differences=False)

	# Get sparsity
	sparse = sparse or issparse(weights)

	# Get indices nearest neighbours in y for each x
	# Get weights of nearest neighbours
	if sparse:
		indices = [indices[i] for k,i in enumerate(inds)]
		weights = [weights[k,i].A.ravel() for k,i in enumerate(inds)]
	else:
		indices = [indices[i] for k,i in enumerate(inds)]
		weights = [weights[k,i] for k,i in enumerate(inds)]

	# Sort indices by absolute value of weights
	argsort = [np.abs(w).argsort(kind=kind) for i,w in zip(indices,weights)]
	indices = [i[a] for a,i,w in zip(argsort,indices,weights)]
	weights = [w[a] for a,i,w in zip(argsort,indices,weights)]

	# Format as ndarray if each x has same size of neighborhood
	squeeze = len(set([i.size for i in indices])) == 1
	if squeeze:
		indices = np.array(indices).squeeze()
		weights = np.array(weights).squeeze()


	# Get return values
	returns = ()
	returns += (indices,)
	if return_weights:
		returns += (weights,)

	return returnargs(returns)




def nearest_neighbourhood(size,strict=False,argsortbounds=None,weightbounds=None,atol=None,rtol=None,kind='mergesort'):
	'''
	Wrapper function to return conditions for nearest neighborhood of size, with neighbors not along dimension where x,y are equal
	
	Args:
		size (int): minimum size of neighborhood
		strict (bool): whether neighborhood must be exactly of size
		argsortbounds (list): Minimum and maximum ranked distance between neighbors (open boundaries), and disallowed values, between -1 and m+1, where None is replaced by -1 or m+1
		weightbounds (list): Minimum and maximum weight distance between neighbors (open boundaries), and disallowed values, between -np.inf and np.inf, where None is replaced by -np.inf or np.inf		
		atol (float): Absolute tolerance of difference between unique elements
		rtol (float): Relative tolerance of difference between unique elements
		kind (str): Sort algorithm        
	Returns:
		conditions (callable): function for adjacency function, which accepts weights, argsort,counts as arguments
	'''

	q = size

	def conditions(weights,argsort,counts):


		# Mask function based on weights,argsort,counts,z and bounds that returns allowed indices
		def ismask(i,m,weights,argsort,counts,size,sums,argsortbounds,weightbounds):
			
			sparse = issparse(weights) or issparse(argsort)


			_mask = [
				gt(argsort,argsortbounds[0]) if argsortbounds[0] > 0 else 1,
				lt(argsort,argsortbounds[1]) if argsortbounds[1] < m else 1,
				gt(weights,weightbounds[0]) if weightbounds[0] > -np.inf else 1,
				lt(weights,weightbounds[1]) if weightbounds[1] < np.inf else 1,
				isin(weights,weightbounds[2],invert=True) if weightbounds[2].size > 0 else 1
				]
			_mask = multiply(*_mask) if not all([i is 1 for i in _mask]) else 1
			
			mask = [_mask] if _mask is not 1 else 1

			indices = argsort.indices if sparse else arange(m)
			indices = where(multiply(*mask)) if mask is not 1 else indices


			if indices.size < size:
				nulllength = 0
				lengths = (sums - nulllength)>=(size)
				length = where(lengths)[0] if lengths.any() else sums.max()

				mask = [le(argsort,length), _mask]
				indices = where(multiply(*mask))

			return indices


		# Check if arr is sparse
		sparse = issparse(weights) or issparse(argsort)
		format = weights.getformat() if sparse else 'array'


		# Get conditions shape and data
		n,m = weights.shape
		shape = (n,m)
		size = min(q,m)
		data,indices,indptr = [],[],[0]
		fmt = 'csr'
		value = True


		# Get bounds of neighborhood ranked distances
		isargsortbounds = argsortbounds is not None
		isweightbounds = weightbounds is not None
		if not isargsortbounds:
			_argsortbounds = [-1,m,np.array([],dtype=int)]
		else:
			_argsortbounds = [min(m,max(-1,argsortbounds[0])) if isinstance(argsortbounds[0],(int,np.integer)) else -1,
						 max(0,min(m,argsortbounds[1])) if isinstance(argsortbounds[1],(int,np.integer)) else m,
						 np.array(argsortbounds[2],dtype=int).reshape(-1)
							if len(argsortbounds)>2 and argsortbounds[2] is not None else np.array([],dtype=int).reshape(-1)]

		if not isweightbounds:
			_weightbounds = [-np.inf,np.inf,np.array([])]
		else:
			_weightbounds = [min(np.inf,max(-np.inf,weightbounds[0])) if isinstance(weightbounds[0],(int,np.integer,float,np.float64)) else -np.inf,
						max(-np.inf,min(np.inf,weightbounds[1])) if isinstance(weightbounds[1],(int,np.integer,float,np.float64)) else np.inf,
						np.array(weightbounds[2]).reshape(-1)
							if len(weightbounds)>2 and weightbounds[2] is not None else np.array([],dtype=float).reshape(-1)]



		excluded = [argsort[i,where(isin(weights[i],_weightbounds[2]))] for i in range(n)]
		excluded = [np.unique([*_argsortbounds[2],*(excluded[i].data if sparse else excluded[i])]) for i in range(n)]
		nullsums = [np.array([add(counts[i][excluded[i][excluded[i]<=j]]) for j,c in enumerate(counts[i])]) if excluded[i].size>0 else 0 for i in range(n)]
		sums = [(cumsum(counts[i]) - nullsums[i]) for i in range(n)]

		sizes = -ones(n,dtype=int)
		_sizes = -ones(n,dtype=int)

		iteration = 0
		while (iteration==0) or (iteration < m) and ((sizes<(size)).any() and strict) and ((sizes==-1).any() or (((strict) and (sizes<sizes.max())).any() and ((sizes != _sizes).all()))):
			size = max(size,np.min([0,*sizes[sizes>size]]))

			for i in range(n):
				abounds = copy.deepcopy(_argsortbounds)
				wbounds = copy.deepcopy(_weightbounds)

				if (sizes[i] >= size):
					continue

				inds = ismask(i,m,weights[i],argsort[i],counts[i],size,sums[i],abounds,wbounds)

				while ((strict and (inds.size < size)) or (inds.size == 0)) and ((abounds[0]>=-1) and (abounds[1] <= m)):
					abounds[0] -= 1
					abounds[1] += 1 
					inds = ismask(i,m,weights[i],argsort[i],counts[i],size,sums[i],abounds,wbounds)

				if (strict and (inds.size>size)):
					mask = lt(unique_tol(weights[i,inds],return_unique=False,return_argsort=True,signed=True,atol=atol,rtol=rtol,kind=kind),size)
					inds = inds[mask.data if sparse else mask]

				sizes[i] = inds.size

				data.extend([value]*sizes[i])
				indices.extend(inds)
				indptr.append(indptr[-1]+sizes[i])

			iteration += 1


		condition = getattr(sp.sparse,'%s_matrix'%(fmt))((data,indices,indptr),shape=shape)
		condition = condition.asformat(format)

		return condition


	return conditions




def explicit_zeros(arr,indices,shape=None,format=None,eliminate_zeros=False):
	'''
	Ensure array has explicit zeros at indices 
	
	Args:
		arr (array,sparse_matrix,sparse_array): array to be made with explicit zeros with shape (n,m)
		indices (list): list of arrays indices with possible explicit zeros in arr
		shape (iterable): shape of full array
		format (str): Format of output sparse_matrix
		eliminate_zeros (bool): eliminate explicit zeros
	Returns:
		out (sparse_matrix): sparse_matrix with explicit zeros	
	'''

	# Get array shape and temporary format of array
	n,m = arr.shape
	shape = (n,m) if shape is None else shape


	# Convert arr to class with fmt data,row,col attributes for appending to datum,row,col and handle explicit zeros
	if isndarray(arr):
		fmt = 'coo'
		data = arr.ravel()
		row = repeat(arange(n),repeats=m,axis=0).ravel()
		col = repeat(arange(m)[None,:],repeats=n,axis=0).ravel()
		out = getattr(sp.sparse,'%s_matrix'%(fmt))((data,(row,col)),shape=shape)
		format = 'array' if format is None else format
	elif issparsearray(arr):
		format = arr.getformat() if format is None else format

		fmt = 'coo'	
		out = arr.tocsr().asformat(fmt)
		data,row,col = out.data.tolist(),out.row.tolist(),out.col.tolist()
		
		fmt = 'csr'	
		out = arr.asformat(fmt)

		for i in range(n):					
			data.extend(out[i,indices[i]].A[0])
			row.extend(i*ones(indices[i].size,dtype=int))
			col.extend(indices[i])
		fmt = 'coo'
		out = getattr(sp.sparse,'%s_matrix'%(fmt))((data,(row,col)),shape=shape)
		format = fmt if format is None else format
	elif issparsematrix(arr):
		format = arr.getformat() if format is None else format

		fmt = 'coo'	
		out = arr.asformat(fmt)
		data,row,col = out.data.tolist(),out.row.tolist(),out.col.tolist()
		
		fmt = 'csr'	
		out = arr.asformat(fmt)

		for i in range(n):					
			data.extend(out[i,indices[i]].A[0])
			row.extend(i*ones(indices[i].size,dtype=int))
			col.extend(indices[i])
		fmt = 'coo'		
		out = getattr(sp.sparse,'%s_matrix'%(fmt))((data,(row,col)),shape=shape)
		format = fmt if format is None else format


	if eliminate_zeros:
		out.eliminate_zeros()

	out = out.asformat(format)

	return out

def sparsify(arr,sparsity,axis=0,func=None,format=None):
	'''
	Sparsify array with only sparsity number of elements or absolute value of elements less than sparsity along axes other than axis
	
	Args:
		arr (array,sparse_matrix): array to make sparse
		sparsity (int,float): integer or float to define sparsity, or float to define maximum absolute value of elements
		axis (int): axis along which to make sparse
		func (callable): func(arr,sparsity) to sort elements and choose sparsity, returning indices of sparse array.
			defaults to sorting by minimum absolute value
		format (str): Matrix format (sparse types 'csr','csc', etc. or 'array' for dense numpy array)				
	Returns:
		out (array,sparse_matrix): sparsified array
	'''

	# @nb.njit(fastmath=True,cache=True,parallel=True)	
	def _sparsify(data,indices,indptr,sparsity,func):
		_data,_indices,_indptr = [],[],[0]
		# for i in nb.prange(indptr.size-1):
		for i in np.arange(indptr.size-1):
			inds = func(data[indptr[i]:indptr[i+1]],sparsity)
			_data.extend(data[indptr[i]:indptr[i+1]][inds])
			_indices.extend(indices[indptr[i]:indptr[i+1]][inds])
			_indptr.append(_indptr[i]+inds.size)
		return _data,_indices,_indptr


	# If sparsity is None, return arr
	if sparsity is None:
		return arr


	# Get type of arr
	shape = arr.shape
	sparse = issparse(arr)

	# Get arr data
	if sparse:
		format = arr.getformat() if format is None else format
		fmt = 'csr' if axis in [0,None] else 'csc'
		arr = arr.asformat(fmt)
		indices = arr.indices
		indptr = arr.indptr
		arr = arr.data
	else:
		format = 'array' if format is None else format
		fmt = 'csr' if axis in [0,None] else 'csc'		
		indices = tile(arange(shape[1-axis]),shape[axis])
		indptr = shape[1-axis]*arange(shape[axis]+1)
		arr = arr.ravel()



	# Get func
	if func is None:

		# @nb.njit(fastmath=True,cache=True)
		def func(arr,sparsity):
			if int(sparsity) == float(sparsity):
				indices = np.argsort(np.abs(arr))[:sparsity]
			else:
				indices = np.where(np.abs(arr)<sparsity)[0]
			return indices
				


	# Sparsify arr
	arr,indices,indptr = _sparsify(arr,indices,indptr,sparsity,func)

	out = getattr(sp.sparse,'%s_matrix'%(fmt))((arr,indices,indptr),shape=shape)
	out = out.asformat(format)
	return out


def twin(arr,like):
	'''
	Make array arr have same sparse structure as like
	Sparsify array with only sparsity number of elements in rows or absolute value of elements less than sparsity
	
	Args:
		arr (array,sparse_matrix): array of shape (n,m) to match sparsity to
		like (array,sparse_matrix): reference array of shape (n,m) of sparsity
	Returns:
		out (array,sparse_matrix): array with data of arr and sparsity of like	
	'''

	# Get sparsity
	sparse = issparse(arr)
	likesparse = issparse(like)
	shape = arr.shape
	format = arr.getformat() if sparse else 'array'

	assert (arr.ndim==2) and (arr.ndim == like.ndim) and all([i==j for i,j in zip(arr.shape,like.shape)]),"Error - arr and like are not identical shapes"

	# Convert arrays to fmt sparse arrays
	fmt = 'csr'	
	if sparse:
		arr = arr.asformat(fmt)
	else:
		arr = getattr(sp.sparse,'%s_matrix'%(fmt))(arr)

	fmt = 'coo'
	if likesparse:
		like = like.asformat(fmt)
	else:
		like = getattr(sp.sparse,'%s_matrix'%(fmt))(like)

	# Match sparsity structure
	data,row,col = [],like.row,like.col
	for i,j in zip(row,col):
		data.append(arr[i,j])


	fmt = 'coo'
	arr = getattr(sp.sparse,'%s_matrix'%(fmt))((data,(row,col)),shape=shape)
	arr = arr.asformat(format)


	out = arr

	return out






def powerset(p,n):
	''' 
	Get all powerset of p non-negative integers less than or equal to n
	
	Args:
		p (int): Number of integers
		n (int): Maximum value of integers
	Returns:
		integers (ndarray): powerset of integers of shape ((n+1)^p,p)	
	'''
	integers = np.array(list(itertools.product(range(n+1),repeat=p)))
	return integers

def combinations(p,n,unique=False):
	''' 
	Get all combinations of p number of non-negative integers that sum up to at most n
	
	Args:
		p (int): Number of integers
		n (int): Maximum sum of integers
		unique (bool): Return unique combinations of integers and q = choose(p+n,n) else q = (p^(n+1)-1)/(p-1)
	Returns:
		combinations (ndarray): All combinations of integers of shape (q,p)
	'''
	combinations = []
	iterable = range(p)
	for i in range(n+1):
		combos = list((tuple((j.count(k) for k in iterable)) for j in itertools.product(iterable,repeat=i)))
		if unique:
			combos = sorted(set(combos),key=lambda i:combos.index(i))
		combinations.extend(combos)
		
	combinations = np.vstack(combinations)
	return combinations


def ncombinations(p,n,unique=False):
	'''
	Number of all combinations of p number of non-negative integers that sum up to at most n
	
	Args:
		p (int): Number of integers
		n (int): Maximum sum of integers
		unique (bool): Return unique combinations of integers and q = choose(p+n,n) else q = (p^(n+1)-1)/(p-1)		
	Returns:
		q (int): Number of all combinations of integers    
	'''
	if p > 1:
		if unique:
			q = sp.special.comb(p+n,p,exact=True)
		else:
			q = int((p**(n+1)-1)/(p-1))
	else:
		q = n + 1
	return q


def icombinations(iterable,n,unique=False):
	''' 
	Get all combinations of p number of non-negative integers that sum up to at most n
	
	Args:
		iterable (int,iterable): Number of integers or iterable of length p
		n (int,iterable): Maximum number of elements, or allowed number of elements
		unique (bool): Return unique combinations of integers and q = choose(p+n,n) else q = (p^(n+1)-1)/(p-1)
	Returns:
		combinations (list): All combinations of iterable with q list of lists of length up to n, or lengths in n
	'''
	iterable = list(iterable) if not isinstance(iterable,int) else range(iterable)
	p = len(iterable)
	n = range(n+1) if isinstance(n,(int,np.integer)) else n
	combinations = []
	for i in n:
		combos = list((tuple(sorted(j,key=lambda i:iterable.index(i))) for j in itertools.product(iterable,repeat=i)))
		if unique:
			combos = sorted(set(combos),key=lambda i:combos.index(i))
		combinations.extend(combos)
	return combinations
	
	
def polynomials(X,order,derivative=None,selection=None,commutativity=False,intercept_=True,
			   variables=None,
			   return_poly=True,return_derivative=False,
			   return_coef=False,return_polylabel=False,return_derivativelabel=False,
			   return_polyindices=False,return_derivativeindices=False,
			   **kwargs):
	r'''
	Get matrix of multivariate polynomial V, where each term is product of monomials up to order order.
	
	A polynomial can be constructed with a vector of coefficients alpha, where
	p(x) = V \dot \alpha = \sum_{q=0}^{order} \sum_{\lambda : \sum \lambda = q} \alpha_{\lambda} x^{\lambda},
	where x^{\lambda} = \prod_{\mu} x^{\mu}^{\lambda_{\mu}} and V can be written as a block matrix, 
	where the block with q order terms has elements
		{V_q}_\lambda = x^{\lambda} 
	for all \lambda such that \sum \lambda = q.
	
	The derivative of this polynomial, for a p-length derivative index \nu is
	{d}_{\nu}(x) = D_{\nu} \dot \alpha = \sum_{q=0}^{order} \sum_{\lambda : \sum \lambda = q} \alpha_{\lambda} x^{\lambda}/x^{\nu} \prod_{\mu} (\lambda_{\mu}!/(\lambda_{\mu}-\nu_{\mu})!),
	where x^{\lambda} = \prod_{\mu} x^{\mu}^{\lambda_{\mu}} and D can be written as a block matrix, 
	where the block with q order terms has elements
		D_\nu_k_\nu_q_\lambda = dx^{\lambda}/dx^{\nu} 
	for all p-length \nu such that \sum \nu = k, and p-length \lambda such that \sum \lambda = q.
	
	If selection is None, then V has only unique terms of polynomial expansion and V has shape (n, p+order Choose order).
	If selection is 'unique', then V has only unique terms of polynomial expansion and V has shape (n, p+order Choose order).
	If unique is 'nonunique, then V has all terms of polynomial expansion and V has shape (n, (p^(order+1)-1)/(p-1)).
	If selection is 'powerset', then each monomial in a term can have up to maximum order order and V has shape (n, (order+1)^p)
	If intercept _ is False, then do not have constant term all 0 powers in monomials
	
	D has is matrix of all derivatives (assuming non-commutativity, q = (p^(derivative+1)-1)/(p-1)  or non-commutativity q = p+derivative choose p ) of shape (n,q,V.shape[1])
	
	Args:
		X (ndarray): matrix of data of shape (n,p) for n points of p dimensional data
		order (int): maximum order of polynomial
		derivative (int): maximum order of derivatives, order if None
		selection (str,ndarray,None): If None, include all terms with each term up to order order, 
			if 'unique', only unique monomial terms, 
			if 'powerset', each sub-monomial in term can have up to order order,
			if ndarray, keep only powers that are present in selection.
		commutativity (bool): Whether derivatives commute.
		intercept _ (bool): Keep term with all 0 powers in monomials
		variables (iterable,None): p dimensional variable names
		return_poly (bool): Return matrix of polynomial terms
		return_derivative (bool): Return matrix of derivative of polynomial terms
		return_coef (bool): Return vector of coefficients for polynomial terms
		return_polylabel (bool): Return dictionary of labels and indices of coefficients for polynomial terms        
		return_derivativelabel (bool): Return dictionary of labels and indices of derivative orders for polynomial terms        
		return_polyindices (bool): Return powers for matrix of polynomial terms
		return_derivativeindices (bool): Return powers for matrix of derivative of polynomial terms
	Returns:
		V (ndarray): matrix of data of polynomial terms
		D (ndarray): matrix of data of derivatives of polynomial terms
		alpha (ndarray): vector of coefficients for matrix of polynomial terms
		polylabel (dict): dictionary of labels and indices for matrix of polynomial terms
		derivativelabel (dict): dictionary of labels and indices for matrix of derivatives of polynomial terms
		polyindices (ndarray): powers for matrix of polynomial terms
		derivativeindices (ndarray): powers for matrix of derivative of polynomial terms
	'''
	
	n,p = X.shape
	order = 0 if None else order
	derivative = order if None else derivative

	if selection is None:
		indices = combinations(p,order,unique=True)
	elif selection in ['unique']:		
		indices = combinations(p,order,unique=True)
	elif selection in ['nonunique']:
		indices = combinations(p,order,unique=False)		
	elif selection in ['powerset']:
		indices = powerset(p,order)
	else:
		indices = np.array(selection)
		intercept_ = False

	derivatives = combinations(p,derivative,unique=commutativity)	


	X = broadcast(X)
	indices = broadcast(indices)
	derivatives = broadcast(derivatives)


	returns = ()
	
	if return_poly:
		V = (X**indices.transpose(2,1,0)).prod(axis=1)
		if not intercept_:
			V = V[:,1:]
		returns += (V,)
	
	if return_derivative:     
		
		numerator = sp.special.factorial(indices,exact=True)
		denominator = sp.special.factorial(indices-derivatives.transpose(2,1,0),exact=True)
		factorials = broadcast((numerator*invert(denominator,constant=0.0)).transpose(1,2,0),axes=0)

		numerator = broadcast(X**indices.transpose(2,1,0),axes=-2)
		denominator = broadcast(X**derivatives.transpose(2,1,0),axes=-1) 
		powers = numerator*invert(denominator,constant=0.0)

		# Make sure 0/0 -> 1 and 0^i/0^j -> 0 where i>j cases are handled
		zeroes = broadcast(X)**ones(powers.shape[2:])

		mask = (
			((broadcast((indices-derivatives.transpose(2,1,0)).transpose(1,2,0),0))>0)
			& (zeroes==0)
			)
		powers[mask] = 0

		mask = (
			((broadcast((indices-derivatives.transpose(2,1,0)).transpose(1,2,0),0))==0)
			& (zeroes==0)
			)		
		powers[mask] = 1

		D = (powers*factorials).prod(axis=1)
 
		if not intercept_:
			D = D[:,:,1:]
			
		returns += (D,)
		
	if return_coef:
		alpha = zeros(indices.shape[0])
		if not intercept_:
			alpha = alpha[1:]
			
		returns += (alpha,)
		
	if return_polylabel:   
		if variables is None:
			variables = ['x%d'%(i) for i in range(p)]
		name = ''
		delimeter = ''        
		splitter = '-'
		separator = '_'
		polylabel = {int(i-(1-intercept_)):'%s%s%s'%(name,delimeter,splitter.join(
							['%s%s%d'%(variable,separator,ind) for variable,ind in zip(variables,inds)]))
							for i,inds in enumerate(indices) if intercept_ or i>0}
				
		returns += (polylabel,)
		
	if return_derivativelabel:   
		
		if variables is None:
			variables = ['x%d'%(i) for i in range(p)]
		name = ''
		delimeter = ''        
		splitter = '-'
		separator = '_'
		derivativelabel = {int(i-(1-intercept_)):'%s%s%s'%(name,delimeter,splitter.join(
							['%s%s%d'%(variable,separator,ind) for variable,ind in zip(variables,inds)]))
							for i,inds in enumerate(derivatives) if intercept_ or i>0}
				
		returns += (derivativelabel,)        
	
	
	if return_polyindices:
		indices = indices[...,0]
		if not intercept_:
			indices = indices[1:]
		returns += (indices,)

	if return_derivativeindices:
		derivatives = derivatives[...,0]
		if not intercept_:
			derivatives =  derivatives[1:]
		returns += (derivatives,)

	
	return returnargs(returns)



def mesh(n,d=1,bounds=(0,1),distribution='uniform',shape=None,dtype=None):
	'''
	Generate d dimensional mesh with n points per dimension along bounds
	
	Args:
		n (int,iterable): Size of mesh along each dimension. If iterable, is different mesh size along each dimension
		d (int): Number of dimensions
		bounds (iterable): Bounds of data. Either iterable of [start,end] or iterable of iterables of [start,end] is different bounds along each dimension
		distribution (str,callable): Type of distribution to generate points, string in 'uniform',linspace','logspace','rand','randn','randint','chebyshev'
		shape (tuple): Shape if mesh is embedded in sparse array. Shape must have total number of elements N >= n**{d}
		dtype (str,data-type): Data type of mesh
	Returns:
		grid (ndarray): mesh of n^{d}, d dimensional points
	'''
	def wrap(func): 
		@functools.wraps(func)
		def wrapper(*args,**kwargs):
			out = func(*args,**kwargs)
			if out.ndim > 1:
				shape = out.shape
				out = sorted([tuple(o.ravel().tolist()) for o in out],key=lambda x:x)
				out = np.array([list(o) for o in out]).reshape(shape)
			else:
				out.sort()
			return out
		return wrapper
	
	types = {
		'constant':{
			'distributions':['uniform','linspace','logspace'],
			'type':'meshgrid',
			'func': lambda dim,n,d,bounds,points: points[:,dim]},
		'random':{
			'distributions':['random','rand','randn',],
			'type':'meshgrid',
			'func': lambda dim,n,d,bounds,points: points[:,dim]},
		'custom':{
			'distributions':['randint','randuniform','randintuniform'],
			'type':'meshgrid',
			'func': lambda dim,n,d,bounds,points: points(dim,n,d,bounds)},
		'mesh':{
			'distributions':['randommesh','randmesh','randnmesh','randintmesh','randuniformmesh'],
			'type':'mesh',
			'func': lambda dim,n,d,bounds,points: points(dim,n,d,bounds)},
	}

	assert callable(distribution) or any([distribution in types[typed]['distributions'] for typed in types]), "Error - distribution %r not permitted"%(distribution)

	if not isinstance(n,(list,np.ndarray)):
		n = [n]*d
	if not all([isiterable(bound) for bound in bounds]):
		bounds = [bounds]*d


	if callable(distribution):
		name = distribution
		@wrap
		def func(dim,n,d,bounds,name=name):
			return distribution(dim,n,d,bounds)
		func = distribution
	if distribution in ['uniform','linspace']:
		name = distribution
		@wrap		
		def func(dim,n,d,bounds,name=name):
			return np.linspace(*bounds,n,endpoint=True)			
	elif distribution in ['logspace']:        
		name = distribution
		@wrap		
		def func(dim,n,d,bounds,name=name):
			return np.logspace(*bounds,n)
	elif distribution in ['random','rand','randn']:
		name = 'rand' if distribution in ['random'] else distribution
		@wrap		
		def func(dim,n,d,bounds,name=name):
			return bounds[0] + (bounds[1]-bounds[0])*(getattr(np.random,name)(n)-0)/(1-0)
	elif distribution in ['randint']:
		name = 'randint' if distribution in [] else distribution		
		@wrap		
		def func(dim,n,d,bounds,name=name):
			return getattr(np.random,name)(bounds[0],bounds[1],n)
	elif distribution in ['randuniform']:
		name = 'randint' if distribution in ['randuniform'] else distribution		
		@wrap		
		def func(dim,n,d,bounds,name=name):
			return getattr(np.random,name)(bounds[0],bounds[1],n)/(bounds[1]-bounds[0])

	elif distribution in ['randintuniform']:
		name = 'randint' if distribution in ['randintuniform'] else distribution		
		@wrap		
		def func(dim,n,d,bounds,name=name):
			return getattr(np.random,name)(bounds[0],bounds[1],n)

	elif distribution in ['randommesh','randmesh','randnmesh']:
		name = 'rand' if distribution in ['randommesh','randmesh'] else 'randn' if distribution	['randnmesh'] else distribution
		@wrap		
		def func(dim,n,d,bounds,name=name):
			return bounds[0] + (bounds[1]-bounds[0])*(getattr(np.random,name)(n**d,d)-0)/(1-0)
	elif distribution in ['randintmesh']:
		name = 'randint' if distribution in ['randintmesh'] else distribution		
		@wrap		
		def func(dim,n,d,bounds,name=name):
			return getattr(np.random,name)(bounds[0],bounds[1],(n**d,d))
	elif distribution in ['randuniformmesh']:
		name = 'randint' if distribution in ['randuniformmesh'] else distribution		
		@wrap		
		def func(dim,n,d,bounds,name=name):
			return getattr(np.random,name)(bounds[0],bounds[1],(n**d,d))/(bounds[1]-bounds[0])

	if any([distribution in types[typed]['distributions'] for typed in types if types[typed]['type'] in ['meshgrid']]):
		grid = [u.reshape(-1,1) for u in np.meshgrid(*[func(dim,m,d,bound,name) for dim,(m,bound) in enumerate(zip(n,bounds))])]
	
	elif any([distribution in types[typed]['distributions'] for typed in types if types[typed]['type'] in ['mesh']]):
		dim = -1
		grid = func(dim,n[dim],d,bounds[dim],name)

	
	def wrapper(grid,d,axis=-1): 
		# return grid[:,[1,0,*arange(2,d)] if d>2 else slice(None)]
		if isinstance(grid,(list,tuple)):
			grid = concatenate(grid,axis=-1)
		return grid
	grid = wrapper(grid,d,axis=-1)

	if shape is not None:
		out = grid.ravel()
		size,dtype = out.size,out.dtype
		density = max(0,min(size/(shape[0]*shape[1]),1))
		grid = sp.sparse.rand(*shape,density=density,format='csr').astype(dtype)
		grid.data[:] = out[:grid.nnz]		
	

	dtype = grid.dtype if dtype is None else dtype
	grid = grid.astype(dtype)

	return grid



def zeros(shape,dtype=None,order='C',format=None):
	'''
	Create array of zeros
	
	Args:
		shape (int,tuple): Shape of array
		dtype (str,data-type): Data type of array
		order (str): 'C' for row-major or 'F' for column-major ordering
		format (str): Format of array, sparse_matrix or sparse_array
	Returns:
		out (ndarray,sparse_matrix,sparse_array): array of zeros
	'''

	if format in [None,'array']:
		out = np.zeros(shape,dtype=dtype,order=order)
	elif format in ['csr','csc','coo','lil','bsr','dia','dok']:
		out = getattr(sp.sparse,'%s_matrix'%(format))(shape,dtype=dtype)		
	elif format in ['COO']:
		format = format.lower()
		out = sparray.zeros(shape,dtype=dtype,format=format)
	else:
		out = np.zeros(shape,dtype=dtype,order=order)

	return out


def ones(shape,dtype=None,order='C',format=None):
	'''
	Create array of ones
	
	Args:
		shape (int,tuple): Shape of array
		dtype (str,data-type): Data type of array
		order (str): 'C' for row-major or 'F' for column-major ordering
		format (str): Format of array, sparse_matrix or sparse_array
	Returns:
		out (ndarray,sparse_matrix,sparse_array): array of ones
	'''

	if format in [None,'array']:
		out = np.ones(shape,dtype=dtype,order=order)
	elif format in ['csr','csc','coo','lil','bsr','dia','dok']:
		out = getattr(sp.sparse,'%s_matrix'%(format))(np.ones(shape,dtype=dtype,order=order),shape,dtype=dtype)
	elif format in ['COO']:
		format = format.lower()
		out = sparray.ones(shape,dtype=dtype,format=format)		
	else:
		out = np.ones(shape,dtype=dtype,order=order)

	return out


def arange(start,stop=None,step=None,dtype=None):
	'''
	Create array of zeros. Create range between start and stop, in increments of step
	
	Args:
		start (int,float): starting value of range, or range from 0 to start if stop and stop is None
		stop (int,float): stopping value of range
		step (int,float): step value of range
		dtype (str,data-type): Data type of range
	Returns:
		out (ndarray): array of range
	'''

	out = np.arange(start,stop,step,dtype=dtype)

	return out


def boundaries(data,size,adjacency=None,reverse=False,excluded=None,atol=None,rtol=None,kind='mergesort',n_jobs=None):
	'''
	Get indices of points in data that have at least a neighborhood of size within the data and are outside boundary region
	
	Args:
		data (ndarray): data array of shape (n,p) to be searched
		size (int): size of neighborhood
		adjacency (ndarray,sparse_matrix): array of shape (n,n) of where to compute differences between elements						
		reverse (bool): return indices inside boundary region
		excluded (ndarray,list): excluded argsort indices
		atol (float): Absolute tolerance of difference between unique elements
		rtol (float): Relative tolerance of difference between unique elements
		kind (str): Sort algorithm                		
		n_jobs (int): Number of parallel jobs		
	Returns:
		indices (ndarray): indices of data that are inside or outside of boundary region
	'''
	if data.ndim != 2:
		data = data.reshape((data.shape[0],-1))
	n,p = data.shape

	size = min(size,n)
	metric = 2
	if excluded is None:
		excluded = []

	
	indices = arange(n)

	sparsity = size+1

	weights = similarity_matrix(data,data,adjacency=adjacency,metric=metric,sparsity=sparsity,directed=False,kind=kind,n_jobs=n_jobs)

	argsort,counts = unique_argsort(np.abs(weights),return_counts=True,atol=atol,rtol=rtol,signed=False,kind=kind)


	excluded = np.array(excluded)
	excluded = [excluded[excluded<=i] for i in range(n)]
	nullsums = [np.array([add(counts[i][excluded[i][excluded[i]<=j]]) for j,c in enumerate(counts[i])]) if excluded[i].size>0 else 0 for i in range(n)]
	sums = [(cumsum(counts[i]) - nullsums[i]) for i in range(n)]

	nulllength = 0
	lengths = [(s - nulllength)>=(size) for s in sums]
	lengths = [where(l)[0] if l.any() else s.max() for s,l in zip(sums,lengths)]
	length = np.min(lengths)

	if not reverse:
		mask = lengths <= length
	else:
		mask = lengths > length

	indices = indices[where(mask)]

	return indices


def min_eps(arr,is_sorted=False,kind='mergesort'):
	'''	
	Get minimum distance between elements of array
	
	Args:
		arr (ndarray): Array to be searched
		is_sorted (bool): If array is previously sorted, do not sort when computing epsilon
		kind (str): Sort algorithm                
	Returns:
		eps (float): Minimum distance between elements of arr
	'''
	if not is_sorted or arr.ndim > 1:
		arr = np.diff(np.sort(arr.ravel(),kind=kind))
	else:
		arr = np.diff(arr.ravel())

	eps = (arr[arr>0]).min()
	return eps

def decimals(arr,return_eps=False):
	'''
	Get minimum number of decimals that when rounded, elements of array can still be distinguished
	
	Args:
		arr (ndarray): array of elements
		return_eps (bool): Return smallest relative difference between elements of arr
	Returns:
		decs (int): decimal places of smallest difference between elements of arr
		eps (float): smallest difference between elements of arr
	'''
	
	# Get minimum difference and base-10 decimals
	eps = min_eps(arr)
	decs = np.abs(np.int(np.floor(np.log10(eps))))

	# Get return values
	returns = ()
	returns += (decs,)
	if return_eps:
		returns += (eps,)

	return returnargs(returns)


# Safely invert x
def invert(x,constant=1,copy=True):
	# Safely divide by zero 
	# error state ignored due to unknown numpy error:
	# RuntimeWarning: divide by zero encountered in true_divide
	with np.errstate(divide='ignore'):
		if isarray(x):
			x = x.copy() if copy else x
			sparse = issparse(x)
			if sparse:
				isx = (np.isnan(x.data) | np.isinf(x.data) | eq(x.data,0))
				x.data[isx] = 1
				x.data[:] = 1/x.data[:]
				x.data[isx] = constant
			else:
				y = x.copy()
				isx = (np.isnan(x) | np.isinf(x) | eq(x,0))
				x[isx] = 1
				x = 1/x
				x[isx] = constant
		else:		
			isx = (np.isnan(x) | np.isinf(x) | eq(x,0))
			x = 1 if isx else x
			x = 1/x
			x = constant if isx else x
	return x


# Safely divide y/x and account for y==x==0 -> zero
def divide(y,x,constant=0,zero=1,copy=True):
	d = multiply(y,invert(x,constant=constant,copy=copy))
	d[eq(y,0) & eq(x,0)] = zero
	return d

# Remove diagonal from array
def nodiagonal(arr):
	n,m = arr.shape[:2]
	return arr[~np.eye(n,dtype=bool)].reshape(n,-1)


# Multiply along axis
def _multiply_along(arr,multiple,axis):
	shape = ones((1,arr.ndim),(int,np.integer)).ravel()
	shape[axis] = -1
	multiple = multiple.reshape(shape)
	return arr*multiple  


# As numpy ndarray function
def asndarray(arr,dtype=None,order=None):
	if issparse(arr):
		arr = arr.todense()
	return np.array(arr)

# As array function
def asarray(arr,dtype=None,order=None,format=None,like=None):
	format = like.getformat() if issparse(like) else None
	dtype = getattr(arr,'dtype',type(arr)) if dtype is None else dtype
	if format in ['csr','csc','coo','lil','bsr','dia','dok']:
		typed = 'sparse'
		format = format
		arraylike = lambda arr,dtype,order,format,like: assparse(arr,like,dtype,order,format)
	elif format in ['array',None]:
		typed = 'dense'
		format = 'array'
		arraylike = lambda arr,dtype,order,format,like: np.asarray(arr,dtype,order)
	else:
		typed = 'dense'
		format = 'array'
		arraylike = lambda arr,dtype,order,format,like: np.asarray(arr,dtype,order)

	scalar = isscalar(arr)
	listtuple = islisttuple(arr)

	if scalar:
		shape = like.shape if like is not None else 1
		arr = arr*ones(shape,dtype,order) 
	elif listtuple:
		shape = len(arr)
		arr = arr

	sparse = issparse(arr)	

	arr = arr.A if sparse and typed not in ['sparse'] else arr

	return arraylike(arr,dtype,order,format,like)


# As sparse array function
def assparse(arr,like,dtype=None,order=None,format=None):
	dtype = getattr(arr,'dtype',type(arr)) if dtype is None else dtype
	if like is None:
		like = getattr(sp.sparse,'%s_matrix'%(format))(arr.shape,dtype=dtype)
	else:
		like = like.asformat(format,copy=True).astype(dtype)
	sparse = issparse(arr)
	scalar = isscalar(arr)
	if sparse:
		like.data[:] = arr.data[:]
	elif scalar:
		like.data[:] = arr
	else:
		like.data[:] = arr.ravel()[:like.nnz]
	return like


# As format function
def asformat(arr,format):
	out = arr
	if format is None:
		return out
	if issparsematrix(arr):
		out = arr.asformat(format)
	elif issparsearray(arr):
		fmt = 'csr'
		out = getattr(arr,'to%s'%(fmt))().asformat(format)
	elif format not in ['array']:
		fmt = 'csr'
		out = getattr(sp.sparse,'%s_matrix'%(fmt))(arr).asformat(format)
	return out


# As dtype function
def asdtype(arr,dtype):
	if dtype is None:
		out = arr
	else:
		out = arr.astype(dtype)
	return out


def broadcast(arr,axes=None,shape=None,axis=None,newaxis=True):
	'''
	Broadcast array  to have expanded shape, depending on shape and axes
	
	Args:
		arr (ndarray,sparse_matrix,sparse_array): array of shape (n_0,...,n_ndim-1) to be broadcasted
		axes (int,tuple,list,ndarray): axes on which to expand array. Defaults to -1 axis
		shape (int,tuple,list,ndarray): sizes of dimensions to expand or repeat array. Defaults to 1
		axis (int): single axis to broadcast, default if axes is None
		newaxis (bool,tuple,list,ndarray): booleans on whether to expand dimension at axis, or repeat existing axis
	Returns:
		out (ndarray,sparse_matrix,sparse_array): array with expanded dimensions
	'''
	
	# Get if matrix is sparse
	sparse = issparse(arr)
	
	# Ensure arguments are iterables
	if shape is None:
		shape = 1
	if axes is None and axis is None:
		axes = -1
	elif axes is None and isinstance(axis,(int,np.integer)):
		axes = axis
	if isinstance(shape,(int,np.integer)):
		shape = [shape]
	if isinstance(axes,(int,np.integer)):
		axes = [axes]*len(shape)
	if isinstance(newaxis,bool):
		newaxis = [newaxis]*len(shape)

	out = arr

	# Expand shape of arr
	for size,axis,new in zip(shape,axes,newaxis):
		if new:
			out = expand_dims(out,axis)
		out = repeat(out,size,axis)

	return out
  


def outer(func,a,b,where=None,squeeze=False):
	'''
	Apply binary function on broadcasted arrays
	
	Args:
		func (callable): binary function that takes 2 broadcasted arrays as arguments of shapes (n,(d),m,(d)) and returns broadcasted function of shape (n,(d),m,(d))
		a (ndarray): array of shape (n,(d))
		b (ndarray): array of shape (m,(d))
		where (ndarray,sparse_matrix): array of shape (n,m) of where to compute differences between elements
		squeeze (bool): If a and b are 1 dimensional, (d) = (), then remove expanded dimensions that have size 1 after function
	Returns:
		out (ndarray,sparse_matrix,sparse_array): array of shape (n,m,(d)) of binary function between elements in array
	'''

	# Reshape arrays
	isndim = a.ndim == 1
	ismdim = b.ndim == 1
	if isndim:
		a = a.reshape((-1,1))
	if ismdim:
		b = b.reshape((-1,1))





	# Get array shapes	
	n,d,ndim = a.shape[0],a.shape[1:],a.ndim
	m,p,mdim = b.shape[0],b.shape[1:],b.ndim


	assert (ndim==mdim) and (all([i==j for i,j in zip(d,p)])), "Error - unequal a,b array shapes"

	shape = (n,m)


	# Get booleans for where
	exists = where is not None
	sparse = issparse(where) and exists

	# Get format of where
	format = 'array' if not sparse else where.getformat()



	# Get booleans for dimensionality of where	
	ndsparse = exists and ndim>1


	if ndsparse:
		fmt = 'COO'
		where = getattr(sparray,fmt)(where).reshape((*where.shape,*[1]*(ndim-1)))

	onedsparse = not ndsparse and sparse




	# Expand array dimensions for broadcasted operations
	# a shape (n,d) -> (n,(d),1,(1)) with ndim = 2(1+|d|)
	# b shape (m,d) -> (1,(1),m,(d)) with mdim = 2(1+|d|)


	if not exists:
		a = broadcast(a,axes=(-1,)*ndim,shape=(1,)*ndim)
		b = broadcast(b,axes=(0,)*ndim,shape=(1,)*ndim)
	elif onedsparse:
		a = where.T.multiply(a).T
		b = where.multiply(b)
	elif ndsparse:
		a = (where.swapaxes(0,1)*a).swapaxes(0,1) 
		b = where*b
	else:
		a = (where.T*a).T 
		b = where*b


	# Compute outer function on arrays
	# func shape (n,(d),m,(d)) if not exists else (n,m,(d))
	out = func(a,b)

	# Get diagonal elements of out such that out is elementwise function 
	# out shape (n,m,(d))
	if not exists:
		for dim in range(ndim-1):
			out = getdiag(out,offset=0,axis1=1,axis2=ndim-dim+1)

	# Squeeze last dimensions if original arrays are 1 dimensional
	if squeeze and (isndim and ismdim):
		out = out.reshape((n,m))

	fmt = 'csr'
	if not sparse and ndsparse and exists:
		out = out.todense()
	elif onedsparse or (isndim and ismdim):
		out = getattr(out,'to%s'%(fmt))().asformat(format)
	elif ndsparse:
		pass

	return out





# Invert sparse boolean array
def invert_sparse_bool(arr):
	shape,dtype,format = arr.shape,arr.dtype,arr.getformat()
	arr = getattr(sp.sparse,'%s_matrix'%(format))(~arr.A)
	return arr

# Reshape data
def reshaper(X,shape,axis={1:-1},order='C'):
	shape = [size for size in shape]
	for ax in axis:
		shape[ax] = axis[ax]
	return X.reshape(shape,order=order)


# Delete obj indices along axis from arr
def delete(arr,obj,axis=None):
	try:
		out = np.delete(arr,obj,axis)
	except (AttributeError,TypeError):
		_obj = arange(arr.shape[axis])
		_obj = _obj[isin(_obj,obj,invert=True)]
		out = take(arr,_obj,axis)
	return out

# Concatenate arrays along axis
def concatenate(arrs,axis=None):
	sparse = any([issparse(arr) for arr in arrs])
	if sparse:
		sparsematrix = any([issparsematrix(arr) for arr in arrs])
		sparsearray = all([issparsearray(arr) for arr in arrs])
		if sparsematrix:
			if axis in [0,None]:
				out = sp.sparse.vstack(arrs)
			elif axis in [1,-1]:
				out = sp.sparse.hstack(arrs)
		elif sparsearray:
			out = sparray.concatenate(arrs,axis=axis)
	else:
		out = np.concatenate(arrs,axis=axis)
	return out

# Take indices along axis from arr
def take(arr,indices,axis):
	sparse = issparse(arr)
	single = isinstance(axis,(int,np.integer))
	if not single:
		axes = np.array(axis,dtype=int)
		indexes = np.array(indices,dtype=object)

		argsort = axes.argsort()[::-1]
		axes = axes[argsort]
		indexes = indexes[argsort]
	else:
		axes = [axis]
		indexes = [indices]

	out = arr
	for axis,indices in zip(axes,indexes):
		integer = isinstance(indices,(int,np.integer))
		shape = out.shape
		if sparse:
			sparsematrix = issparsematrix(arr)
			sparsearray = issparsearray(arr)
			if sparsematrix:
				if axis in [0]:
					out = out[indices,:]
				elif axis in [1,-1]:
					out = out[:,indices]			
			elif sparsearray:
				if integer:
					out = out.reshape((1,*shape))
				out = arr.swapaxes(axis,0)[indices]
				if integer:
					out = out.reshape((*shape[:axis],*shape[axis+1:]))
				else:
					out = out.swapaxes(axis,0).reshape((*shape[:axis],len(indices),*shape[axis+1:]))
		else:
			out = np.take(out,indices,axis)
	return out


# Expand dimensions along axis
def expand_dims(arr,axis):
	shape,ndim = list(arr.shape),arr.ndim
	axis = ndim+1+axis if axis<0 else axis
	shape.insert(axis,1)

	out = arr.reshape(shape)
	return out

# Indices in one dimensional array
def isin(arr,elements,assume_unique=False,invert=False):
	sparse = issparse(arr)
	if sparse:

		sparsematrix = issparsematrix(arr)
		sparsearray = issparsearray(arr)

		if sparsematrix:
			shape = arr.shape
			format = arr.getformat()

			fmt = 'csr'
			out = arr.asformat(fmt,copy=True).astype(bool)
			indices = out.indices
			indptr = out.indptr

			fmt = 'lil'
			out = out.asformat(fmt)

			for i in range(shape[0]):
				out[i,indices[indptr[i]:indptr[i+1]]] = np.isin(arr[i].data,elements,assume_unique=assume_unique,invert=invert)
			out = out.asformat(format)

		elif sparsearray:
			shape = arr.shape
			format = 'COO'
			arr = arr.reshape(-1)
			coords = arr.nonzero()
			data = np.isin(arr[indices].todense(),elements,assume_unique=assume_unique,invert=invert)
			out = getattr(sparray,format)(coords=coords,data=data,shape=shape)
	else:
		out = np.isin(arr,elements,assume_unique=assume_unique,invert=invert)
	return out


# Find where elements in array are true
def where(condition,x=None,y=None):
	# Find where sparse elements in array are true
	def wheresparse(condition,x=None,y=None):
		def wheretrue(rows,cols,data,like,x):
			x = asarray(x,like=like)
			shape,dtype,format = x.shape,x.dtype,x.getformat()

			fmt = 'coo'
			data = ones(data.shape,dtype=dtype)
			data = getattr(sp.sparse,'%s_matrix'%(fmt))((data,(rows,cols)),shape=shape,dtype=dtype)

			fmt = 'csr'
			try:
				x = x.asformat(fmt)[rows,cols][0]
			except:
				pass
			data.data[:] = x
			data = data.asformat(format)
			data.eliminate_zeros()        
			return data

		def wherefalse(rows,cols,data,like,y):
			like = invert_sparse_bool(like)
			y = asarray(y,like=like)
			shape,dtype,format = y.shape,y.dtype,y.getformat()

			fmt = 'lil'
			data = y.asformat(fmt,copy=True)
			data[rows,cols] = 0
			data = data.asformat(format)
			data.eliminate_zeros()
			return data

		shape = condition.shape
		
		is1d = condition.shape[0] == 1

		isxy = (x is not None) and (y is not None)
		   
		rows,cols,data = sp.sparse.find(condition)

		inds = rows.argsort()
		rows,cols,data = rows[inds],cols[inds],data[inds]

		if isxy:
			data = wheretrue(rows,cols,data,condition,x) + wherefalse(rows,cols,data,condition,y)
			return data
		else:
			returns = ()

			if is1d:
				returns += (cols,)
			else:
				returns += (rows,cols,)
			return returnargs(returns)
		
		return
		

	# Find where dense elements in array are true
	def wheredense(condition,x=None,y=None):
		
		isxy = (x is not None) and (y is not None)
		
		if isxy:
			return np.where(condition,x,y)
		else:
			return returnargs(np.where(condition))



	sparse = issparse(condition)

	if sparse:
		return wheresparse(condition,x,y)
	else:
		return wheredense(condition,x,y)



# Get cumulative sum of array
def cumsum(arr,axis=None):
	sparse = issparse(arr)
	if sparse:
		shape,dtype = arr.shape,arr.dtype
		sparsematrix = issparsematrix(arr)
		sparsearray = issparsearray(arr)
		if sparsematrix:
			format = arr.getformat()
			fmt = 'lil'
			out = getattr(sp.sparse,'%s_matrix'%(fmt))((shape[1-axis],shape[axis]),dtype=dtype)
			arr = arr.T if axis in [0] else arr
			for i in range(shape[1-axis]):
				out[i,arr.indices[arr.indptr[i]:arr.indptr[i+1]]] = np.cumsum(arr[i].data)
			out = out.T if axis in [0] else out
			out = out.asformat(format)
		elif sparsearray:
			raise ValueError("Not implemented for sparse_array")
	else:
		out = np.cumsum(arr,axis=axis)
	return out


# Returns a boolean array where two arrays are element-wise equal within a tolerance.
def isclose(a, b=None, rtol=None, atol=None, equal_nan=False):

	sparse = issparse(a) or issparse(b)
	format = a.getformat() if sparse else None
	shape,dtype = a.shape,a.dtype

	if b is None:
		b = zeros(shape,dtype=dtype,format=format)


	# Get tolerances based on minimum differences
	tol = 1e-10
	absa = np.abs(a)	
	try:
		tol = max(tol,(absa.min())/max(1,(absa.max()-absa.min()))*(tol**(1/2)))
	except:
		tol = tol
	if atol is None:
		atol = tol
	if rtol is None:
		rtol = tol

	if sparse:
		out = le(np.abs(a-b),atol + rtol*absa)
	else:
		out = np.isclose(a,b,rtol=rtol,atol=atol,equal_nan=equal_nan)
	return out


# Number of non-zero elements
def nonzeros(arr,axis=-1):
	sparse = issparse(arr)
	scalar = isscalar(arr)
	if sparse:
		counts = arr.astype(bool).sum(axis).astype(int)
	elif scalar:
		counts = int(arr != 0)
	else:
		counts = np.count_nonzero(arr,axis=axis)
	return counts


# Single Tiles (array only)
def tile(arr,reps,axis=-1):
	sparse = issparse(arr)
	if sparse:
		raise
		shape = arr.shape
		size = shape[axis]
		
		out = stack([arr]*repeats,axis)
		
		indices = np.array([arange(i,size*repeats,size) for i in range(size)]).reshape(-1)
		permutation = np.array([arange(i*repeats,(i+1)*repeats,1) for i in range(size)]).reshape(-1)
		
		out = out.reshape((*shape[:axis],-1,*shape[axis+1:]))
		out = out.swapaxes(0,axis)
		out[permutation] = out[indices]
		out = out.swapaxes(0,axis)
	else:
		out = np.tile(arr,reps=reps)
	return out


# Single Repeats
def repeat(arr,repeats,axis=-1):
	sparse = issparse(arr)
	if sparse:
		shape = arr.shape
		size = shape[axis]
		
		out = stack([arr]*repeats,axis)
		
		indices = np.array([arange(i,size*repeats,size) for i in range(size)]).reshape(-1)
		permutation = np.array([arange(i*repeats,(i+1)*repeats,1) for i in range(size)]).reshape(-1)
		
		out = out.reshape((*shape[:axis],-1,*shape[axis+1:]))
		out = out.swapaxes(0,axis)
		out[permutation] = out[indices]
		out = out.swapaxes(0,axis)
	else:
		out = np.repeat(arr,repeats=repeats,axis=axis)
	return out


# Single Stack along new axis
def stack(arrs,axis=-1):
	sparse = any([issparse(arr) for arr in arrs])
	if sparse:
		fmt = 'COO'
		sparsematrix = any([issparsematrix(arr) for arr in arrs])
		sparsearray = all([issparsearray(arr) for arr in arrs])
		if sparsematrix:
			format = arr.getformat()
			out = sparray.stack([getattr(sparray,fmt)(arr) for arr in arrs],axis)
		elif sparsearray:
			out = sparray.stack(arrs,axis)
	else:
		out = np.stack(arrs,axis=axis)
	return out





def reduce(func,arr,axis,dtype=None,where=True,**kwargs):
	'''
	Reduce elements in arr along axis at locations where
	
	Args:
		arr (ndarray,sparse_matrix,sparse_array): array to be reduced
		axis (int): axis along which to reduce
		dtype (str,data-type): datatype of resulting output
		where (bool,ndarray): where to reduce elements of arr
	Returns:
		out (ndarray,sparse_matrix,sparse_array): output reduced array
	'''

	sparse = issparse(arr)

	dtype = arr.dtype if dtype is None else dtype
	where = True if where is None else where


	if sparse:

		sparsearray = issparsearray(arr)			
		sparsematrix = issparsematrix(arr)			

		fmt = 'csr'
		shape = list(arr.shape)
		dim = shape.pop(axis)
		size = prod(np.array(shape))

		
		if sparsematrix:
			format = arr.getformat()
			if axis in [0]:
				arr = arr.T.asformat(fmt) # fmt sparse matrix of shape (size,dim)
			elif axis in [1,-1]:
				arr = arr.asformat(fmt) # fmt sparse matrix of shape (size,dim)

		elif sparsearray:
			format = 'COO'
			arr = getattr(arr.swapaxes(axis,-1).reshape((-1,dim)),'to%s'%(fmt))() # fmt sparse matrix of shape (size,dim)			
		
		if issparsematrix(where):
			where = where.asformat(fmt).reshape((-1,1))
		elif issparsearray(where):
			where = getattr(where,'to%s'%(fmt))().asformat(fmt).reshape((-1,1))

		inds,iptr = arr.indices,arr.indptr

		fmt = 'csr'
		data,indices,indptr = [],[],[0]

		arr = multiply(arr,where)

		for i in range(size):
			if iptr[i+1]>iptr[i]:
				data.append(func(arr[i,inds[iptr[i]:iptr[i+1]]].data,axis=-1,dtype=dtype,**kwargs))
				indices.append(0)
				indptr.append(indptr[-1]+1)
			else:
				indptr.append(indptr[-1])



		shape = (*shape,1) if len(shape) < 2 else shape
		out = getattr(sp.sparse,'%s_matrix'%(fmt))((data,indices,indptr),shape=(size,1),dtype=dtype).reshape(shape)

		if sparsematrix:
			out = out.asformat(format)
		elif sparsearray:
			out = getattr(sparray,format)(out)
	else:
		out = func(arr,axis=axis,dtype=dtype,where=where,**kwargs)

	return out



def add(arr,axis=-1,dtype=None,where=True,size=False):
	'''
	Sum elements in arr along axis at locations where, dividing by size
	
	Args:
		arr (ndarray,sparse_matrix,sparse_array): array to be summed
		axis (int): axis along which to sum
		dtype (str,data-type): datatype of resulting output
		where (bool,ndarray): where to sum elements of arr
		size (bool): Normalize sum by number of sum
	Returns:
		out (ndarray,sparse_matrix,sparse_array): output summed array	
	'''

	# Get sparsity
	sparse = issparse(arr)
	exists = not isscalar(where)
	sparsearray = issparsearray(arr)			
	sparsematrix = issparsematrix(arr)				
	sparsewhere = issparse(where)

	if size:
		if exists:
			size = nonzeros(where,axis=axis)
		elif sparse:
			fmt = 'csr'
			if sparsematrix:
				size = arr.getnnz(axis)
			elif sparsearray:
				size = getattr(sp.sparse,'%s_matrix'%(fmt))(getattr(arr.swapaxes(0,axis).reshape((arr.shape[axis],-1)).T,'to%s'%(fmt)).getnnz(-1))
		else:
			size = arr.shape[axis]

	else:
		size = 1

	def func(arr,axis=None,dtype=None,where=True):
		out = arr.sum(axis=axis,dtype=dtype,where=where)

		return out

	out = reduce(func,arr,axis=axis,dtype=dtype,where=where)

	sparsearray = issparsearray(out)			
	sparsematrix = issparsematrix(out)				

	if sparsematrix:
		format = out.getformat()
	elif sparsearray:
		format = 'COO'
	else:
		format = 'array'

	out = (multiply(out.T,invert(size,constant=0.0))).T

	if sparsematrix:
		out = out.asformat(format)
	elif sparsearray:
		pass
	else:
		pass

	return out


def subtract(a,b,where=None,dtype=None,squeeze=True):
	'''
	Differences elements in b-a along first axis with shape (n,m,(d)) (b-a)[i,j] = b[j]-a[i]

	Args:
		a (ndarray): array with shape (n,(d)) to be subtracted by
		b (ndarray): array with shape (m,(d)) to be subtracted from
		where (bool,ndarray,sparse_matrix,sparse_array): where to compute differences in arrays
		dtype (str,data-type): datatype of resulting output		
		squeeze (bool): If a and b are 1 dimensional, (d) = (), then remove expanded dimensions that have size 1 after function		
	Returns:
		out (ndarray): output of differences of arrays, of shape (n,m,(d))
	'''

	def func(a,b):
		return -np.subtract(a,b,dtype=dtype) + np.array(0,dtype=dtype)

	out = outer(func,a,b,where=where,squeeze=squeeze)

	return out


def prod(arr,axis=-1,dtype=None,where=True):
	'''
	Multiply elements in arr along axis at locations where

	Args:
		arr (ndarray): array to be summed
		axis (int): axis along which to sum
		dtype (str,data-type): datatype of resulting output
		where (bool,ndarray): where to sum elements of arr
	Returns:
		out (ndarray): output multiplied array		
	'''


	def func(arr,axis=None,dtype=None,where=True):
		try:
			out = arr.prod(axis=axis,dtype=dtype,where=where)
		except:
			out = np.prod(arr,dtype=dtype,where=where)
		return out

	out = reduce(func,arr,axis=axis,dtype=dtype,where=where)

	return out



def multiply(*arrs):
	'''
	Multiply many arrays elementwise in place

	Args:
		arrs: Arrays to multiply
	Returns:
		out (ndarray) if out argument is not None		
	'''
	
	out = arrs[0]

	sparse = issparse(out)


	for arr in arrs[1:]:
		if not sparse:
			out = out*arr
		else:
			out = out.multiply(arr)
	
	return out

def mean(arr,axis=None,ddof=0):
	return np.mean(arr,axis=axis)

def var(arr,barr=None,axis=None,ddof=0):
	return np.covar(arr,barr,axis=axis,ddof=ddof)

def sqrt(arr):
	sparse = issparse(arr)
	if sparse:
		out = arr.sqrt()
	else:
		out = np.sqrt(arr)
	return out


# Get ord-norm of y-x, with sign of cos(x,y) = x.y/|x|.|y| along axis
def normsign(x,y,ord=2,axis=-1,normed=True,signed=True):
	shape = list(x.shape)
	size = shape.pop(axis)
	z = ones(shape)
	if normed:
		z = norm(y-x,axis=axis,ord=ord)
	if signed:
		z *= sign(x,y,ord=ord,axis=axis)
	return z

# Get ord-norm of y-x, with sign of cos(y-x,1) = y-x-1/|y-x|.|1| along axis
def normsigndiff(x,y,ord=2,axis=-1,normed=True,signed=True):
	shape = list(x.shape)
	size = shape.pop(axis)
	z = ones(shape)
	if normed:
		z = norm(y-x,axis=axis,ord=ord)
	if signed:
		z *= signdiff(x,y,ord=ord,axis=axis)
	return z

# Get sign of cos(x,y) = x.y/|x|.|y| along axis
def sign(x,y,ord=2,axis=-1):
	isx = eq(x,0).all(axis=axis)
	isy = eq(y,0).all(axis=axis)

	s = np.sign(cosine(x,y,ord=ord,axis=axis))

	s[multiply(isx,ne(isy,True))] = np.sign(add(y,axis=axis))[multiply(isx,ne(isy,True))]
	s[multiply(ne(isx,True),isy)] = np.sign(add(x,axis=axis))[multiply(ne(isx,True),isy)]

	return s

# Get sign of cos(y-x,1) = (y-x).1/|y-x|.|1| along axis
def signdiff(x,y,ord=2,axis=-1):
	o = ones(x.shape[axis],dtype=x.dtype)
	s = np.sign(cosine(y-x,o,ord=ord,axis=axis))
	iszero = (~(x==y).all(axis=axis)) & (s==0)
	s[iszero] = -1
	return s

# Get cos(x,y) = x.y/|x|.|y| along axis
def cosine(x,y,ord=2,axis=-1):
	xy = multiply(x,y)**(ord/2)
	return multiply(add(xy,axis=axis),invert(norm(x,axis=axis,ord=ord)*norm(y,axis=axis,ord=ord),constant=1.0))

# Get acos(x,y) = acos(x.y/|x|.|y|) along axis
def acosine(x,y,ord=2,axis=-1):
	return np.acos(cosine(x,y,ord=ord,axis=axis))


def gt(arr,value):
	'''
	Greater than function for explicit elements of array
	
	Args:
		arr (ndarray,sparse_matrix,sparse_array): array for comparision
		value (ndarray,int,float): value for comparison
	Returns:
		out (ndarray,sparse_matrix,sparse_array): boolean array of comparison of arr to value	
	'''
	sparse = issparse(arr)
	scalar = isscalar(value)
	if sparse:
		if scalar:
			if value < 0:
				out = arr.astype(bool,copy=True)
				out.data = arr.data > value				
			else:
				out = arr > value
		else:
			out = arr > value
	else:
		out = arr > value
	return out

def lt(arr,value):
	'''
	Less than function for explicit elements of array
	
	Args:
		arr (ndarray,sparse_matrix,sparse_array): array for comparision
		value (ndarray,int,float): value for comparison
	Returns:
		out (ndarray,sparse_matrix,sparse_array): boolean array of comparison of arr to value	
	'''
	sparse = issparse(arr)
	scalar = isscalar(value)
	if sparse:
		if scalar:
			if value > 0:
				out = arr.astype(bool,copy=True)
				out.data = arr.data < value
			else:
				out = arr < value
		else:
			out = arr < value
	else:
		out = arr < value
	return out

def ge(arr,value):
	'''
	Greater than or equal function for explicit elements of array
	
	Args:
		arr (ndarray,sparse_matrix,sparse_array): array for comparision
		value (ndarray,int,float): value for comparison
	Returns:
		out (ndarray,sparse_matrix,sparse_array): boolean array of comparison of arr to value
	'''
	sparse = issparse(arr)
	scalar = isscalar(value)
	if sparse:
		if scalar: 
			if value <= 0:
				out = arr.astype(bool,copy=True)
				out.data = arr.data >= value				
			else:
				out = arr >= value
		else:
			out = arr >= value
	else:
		out = arr >= value
	return out

def le(arr,value):
	'''
	Less than or equal function for explicit elements of array
	
	Args:
		arr (ndarray,sparse_matrix,sparse_array): array for comparision
		value (ndarray,int,float): value for comparison
	Returns:
		out (ndarray,sparse_matrix,sparse_array): boolean array of comparison of arr to value
	'''
	sparse = issparse(arr)
	scalar = isscalar(value)
	if sparse:
		if scalar: 
			if value >= 0:
				out = arr.astype(bool,copy=True)
				out.data = arr.data <= value				
			else:
				out = arr <= value
		else:
			out = arr <= value
	else:
		out = arr <= value
	return out


def eq(arr,value):
	'''
	Equal function for explicit elements of array
	
	Args:
		arr (ndarray,sparse_matrix,sparse_array): array for comparision
		value (ndarray,int,float): value for comparison
	Returns:
		out (ndarray,sparse_matrix,sparse_array): boolean array of comparison of arr to value
	'''
	sparse = issparse(arr)
	scalar = isscalar(value)
	if sparse:
		if scalar: 
			if value == 0:
				out = arr.astype(bool,copy=True)
				out.data = arr.data == value
			else:
				out = arr == value
		else:
			out = arr == value
	else:
		out = arr == value
	return out

def ne(arr,value):
	'''
	Not equal function for explicit elements of array
	
	Args:
		arr (ndarray,sparse_matrix,sparse_array): array for comparison
		value (ndarray,int,float): value for comparison
	Returns:
		out (ndarray,sparse_matrix,sparse_array): boolean array of comparison of arr to value
	'''
	sparse = issparse(arr)
	scalar = isscalar(value)
	if sparse:
		if scalar: 
			if value in [1,0,True,False]:
				out = arr.astype(bool,copy=True)
				out.data = arr.data != value
			else:
				out = arr != value
		else:
			out = arr != value
	else:
		out = arr != value
	return out





# Get indices,indptr from row,col of sparse matrix
def indices_rowcol(row,col,shape,format):
	argsort = row.argsort()
	row,col = row[argsort],col[argsort]

	if format in ['csr','csc','coo','lil','bsr','dia','dok']:
		indices = col
		indptr = zeros(shape[0]+1,dtype=int)
		row,counts = unique_tol(row,return_unique=True,return_counts=True)
		indptr[row+1] = counts
		indptr = cumsum(indptr)
	else:
		indices,indptr = row,col

	return indices,indptr



# Get row,col from indices,indptr of sparse matrix
def rowcol_indices(indices,indptr,shape,format):
	if format in ['csr','csc','coo','lil','bsr','dia','dok']:
		diffs = np.diff(indptr)
		inds = where(diffs != 0)
		diffs = diffs[inds]
		nnz,size,counts = diffs.sum(),inds.size,cumsum([0,*diffs])
		row = zeros(nnz,dtype=int)
		for i in range(size):
			row[counts[i]:counts[i+1]] = inds[i]
		col = indices
	else:
		row,col = indices,indptr
	return row,col


def subarray(arr,indices,dtype=None,format=None):
	'''
	Get submatrix of array along indices, with explicit zeros if sparse
	
	Args:
		arr (ndarray,sparse_matrix,sparse_array): array of shape (n,m) to get submatrix
		indices (ndarray): arr row indices of length size <= n
		dtype (str,data-type): data type of submatrix, defaults to data type of arr if None
		format (str): format of submatrix ('array','csr','csc','coo','lil','bsr','dia','dok','COO'), defaults to format of arr if None
	Returns:
		out (ndarray,sparse_matrix,sparse_array): submatrix of shape (n,m) with size elements from arr at indices
	'''

	shape,size = arr.shape,indices.size
	rows,cols = repeat(indices,repeats=shape[1],axis=0).ravel(),repeat(arange(shape[1])[None,:],repeats=size,axis=0).ravel()
	subrows,subcols = rows,cols
	out = submatrix(arr,shape=shape,rows=rows,cols=cols,subrows=subrows,subcols=subcols,dtype=dtype,format=format)

	return out



def submatrix(arr,shape,rows,cols,subrows,subcols,dtype=None,format=None):
	'''
	Get submatrix of array, with explicit zeros if sparse
	
	Args:
		arr (ndarray,sparse_matrix,sparse_array): array of shape (n,m) to get submatrix
		shape (iterable): shape of submatrix (s,t)
		rows (ndarray): arr row indices of length k <= n*m
		cols (ndarray): arr column indices of length k <= n*m
		subrows (ndarray): submatrix row indices of length k <= n*m
		subcols (ndarray): submatrix column indices of length k <= n*m
		dtype (str,data-type): data type of submatrix, defaults to data type of arr if None
		format (str): format of submatrix ('array','csr','csc','coo','lil','bsr','dia','dok','COO'), defaults to format of arr if None
	Returns:
		out (ndarray,sparse_matrix,sparse_array): submatrix of shape (s,t) with k elements from arr at rows and cols
	'''

	n,m = arr.shape
	k = min(rows.size,cols.size,subrows.size,subcols.size)	

	sparse = issparse(arr)
	sparsematrix = issparsematrix(arr)
	sparsearray = issparsearray(arr)

	if dtype is None:
		dtype = arr.dtype
	if format is None:
		if not sparse:
			format = 'array'
		elif sparsematrix:
			format = arr.getformat()
		elif sparsearray:
			format = 'COO'


	data = zeros(k,dtype=dtype)
	for l,(i,j) in enumerate(zip(rows,cols)):
		data[l] = arr[i,j]

	if format in ['array']:
		out = zeros(shape,dtype=dtype,format=format)
		for l,(i,j) in enumerate(zip(subrows,subcols)):
			out[i,j] = data[l]
	elif format in ['csr','csc','coo','lil','bsr','dia','dok']:
		out = getattr(sp.sparse,'%s_matrix'%(format))((data,(subrows,subcols)),shape=shape,dtype=dtype)
	elif format in ['COO']:
		coords = np.array([subrows,subcols])
		out = getattr(sparray,format)(coords,data=data,shape=shape)		


	return out



def vandermonde(X,order,ones=True,unique=True,dims=None,along=False):
	'''
	Vandermonde matrix of data matrix of unique polynomial terms up to order of shape (n,q), where for d = len(dims) and o = 1-ones
	If unique and along - q = choose(p+order-d,p) + o
	If unique and not along - q = choose(p+order,p) - o
	If not unique and along - q = (p^(order)-1)/(p-1) - o
	If not unique not along - q = q = (p^(order+1)-1)/(p-1) - o
	
	Args:
		X (ndarray,sparse_matrix,sparse_array): data matrix of shape (n,p) for n p-dimensional data points
		order (int): order of powers of data
		ones (bool): Include constant ones column in vandermonde
		unique (bool): Return unique combinations of integers
		dims (int,list): dimensions of column vectors of data X to divide vandermonde by
		along (bool): keep polynomials only with powers along dims
	Returns:
		V (ndarray,sparse_matrix,sparse_array) vandermonde of data of shape (n,q = choose(p+n,p) - ones) (if not along else (n,q = choose(p+n-1,p) + ones))
	'''	

	sparse = issparse(X)

	if not sparse:
		X = asarray(X)


	X = X.reshape((-1,1)) if X.ndim != 2 else X
	n,p = X.shape
	dims = [dims] if isinstance(dims,(int,np.integer)) else dims if isinstance(dims,list) else []

	# Powers of X with shape (p,q = choose(p+n,p))
	I = combinations(p,order,unique=unique).T


	# Reduce powers along dims to safely divide by columns of X along dims
	masks = {}
	for dim in dims:
		masks[dim] = where(I[dim] == 0)

		I[dim] -= 1

		if along:
			I = delete(I,masks[dim],1)
		else:
			I[dim,masks[dim]] += 1

	# Get expanded vandermonde of shape(p,q,n)	
	V = (broadcast(X)**(I)).transpose(1,2,0)


	# Safely divide by columns of X along dims
	for dim in dims:
		if along:
			pass
		else:
			V[dim,masks[dim]] *= invert(X[:,dim],constant=0.0)
			I[dim,masks[dim]] -= 1
		I[dim] += 1

	# Get product for polynomial V of shape (n,q)
	V = prod(V,axis=0).transpose(1,0)


	if not ones:
		mask = ~(I==0).all(0)
		V = V[:,mask]

	return V


def linearly_independent(x,y,order,size,basis,adjacency=None,weights=None,dimension=None,ones=False,unique=True,tol=None,atol=None,rtol=None,kind='mergesort',diagonal=False,verbose=False,return_differences=False):
	'''
	Get indices of p-dimensional points in y that are linearly independent, in terms of the multi-dimensional basis and closest to points in x. Computes differences between points, as per adjacency, and finds set of nearest neighbouring points that are independent.
	
	Args:
		x (ndarray): data array of shape (n,p)
		y (ndarray): data array of shape (m,p)
		order (int,iterable): order of basis
		size (int,iterable): number of basis constraints
		basis (callable): function to yield basis, or string in ['vandermonde']
		adjacency (array,sparse_matrix): adjacency matrix of shape (n,m) of which data points are adjacent
		weights (array,sparse_matrix): weights matrix of shape (n,m) of pairwise weights of data points
		dimension (int): dimension direction of basis, between 0 ... p-1
		ones (bool): Include constant ones column in basis
		unique (bool): Include unique basis terms q = choose(p+n,n) else q = (p^(n+1)-1)/(p-1)		
		tol (float): Tolerance for rank and singularity of basis
		atol (float): Absolute tolerance of difference between unique elements
		rtol (float): Relative tolerance of difference between unique elements		
		kind (str): Sort algorithm
		diagonal (bool): explicitly include diagonal of adjacency
		verbose (int): Print out linear dependent findings
		return_differences (bool): return differences of data points
	Returns:
		indices (list): list of arrays of indices of linearly independent points of length n
		z (ndarray,sparse_array): array of differences in data points of shape (n,m,p)
	'''

	# Get basis
	default = 'vandermonde'
	bases = {'vandermonde':vandermonde}
	if isinstance(basis,str):
		basis = bases.get(basis,bases.get(default))
	assert callable(basis), "Error - basis function is not callable"

	# Reshape data to ensure p>0 dimensional data
	isdimension = dimension is not None

	if x.ndim < 1:
		x = x.reshape((-1,1))
	if y.ndim < 1:
		y = y.reshape((-1,1))

	# Get shape of data
	ndim,dtype = x.ndim,x.dtype
	n,p = x.shape
	m,d = y.shape
	shape = (n,m)

	assert ndim == 2, "Error - data is not of shape (n,p)"
	assert (p==d), "Error - x,y not of shape shape (n/m,p)"


	# Get sparsity
	exists = adjacency is not None
	sparse = issparse(adjacency)
	distances = weights is not None
	

	# Include diagonal of adjacency
	if diagonal and exists:		
		adjacency = setdiag(adjacency,diagonal)		

	# Get adjacency properties
	format = adjacency.getformat() if sparse else 'array'

	# Get differences in data of shape (n,m,p)
	z = subtract(x,y,where=adjacency)	

	if not distances:

		# Get radius of differences of shape (n,m)
		fmt = 'csr'
		r = norm(z,ord=2,axis=-1,where=adjacency)
		r = getattr(r,'to%s'%(fmt))().asformat(format) if sparse else r
	else:
		r = np.abs(weights)


	# Include explicit zero differences along diagonal
	if diagonal:
		indices = [adjacency[i].indices for i in range(n)]
		r = explicit_zeros(setdiag(r,0),indices,format=format)
	elif sparse:
		r.eliminate_zeros()

	# Setup system of constraints for basis
	order = repeat(order,n) if isinstance(order,(int,np.integer)) else order	
	size = repeat(size,n) if isinstance(size,(int,np.integer)) else size	

	# Indices output
	indices = [None for i in range(n)]

	with Bar("Linear %d independent points..."%(size[0]),max=n) as bar:

		# Iterate through data points and get q points to form stencil
		for i in range(n):

			# logger.log(verbose,"Point %d"%(i))
			bar.next()

			# Get indices of adjacent data points
			if sparse:
				indices[i] = r[i].indices			
				indices[i] = indices[i][r[i].data.argsort(kind=kind)]
			elif exists:
				indices[i] = arange(m)[r[i].astype(bool)]
				indices[i] = indices[i][r[i][indices[i]].argsort(kind=kind)]
			else:
				indices[i] = arange(m)
				indices[i] = indices[i][r[i].argsort(kind=kind)]

			# Get size of neighbourhood, order, number of constraints, and number of removed data points
			s = indices[i].size
			o = order[i]
			q = size[i]
			k = 0

			# Iterate through neighbourhood data points
			v = None
			for j in range(s):
				
				# Check if neighbourhood size equals number of constraints
				if (j-k+1) > q:
					indices[i] = indices[i][:q]
					break

				# Check if difference in data points is zero along dimension
				if isdimension and (z[i,indices[i][j-k],dimension] == 0):
					indices[i] = delete(indices[i],j-k)
					k += 1
					continue


				# Check if basis of constraints if full rank
				inds = indices[i][:j+1-k]
				u = subarray(z[i],indices=inds,format=format)
				u = u[inds].todense() if sparse else u[inds]
				v = basis(u,order=o,ones=ones,dims=dimension,unique=unique)
				if not isfullrank(v,tol=tol):
					indices[i] = delete(indices[i],j-k)
					k += 1
					continue

			# Assert basis of constraints is full rank
			assert v is not None, "Error - less data points than constraints"
			assert isfullrank(v,tol=tol), "Error - basis for point %d, with shape %r is singular: %r\n%r\n%r"%(i,v.T.shape,rank(v,tol=tol),v.T,indices[i])

	# Returns
	returns = ()

	returns += (indices,)

	if return_differences:
		returns += (z,)

	return returnargs(returns)



def stencils(x,order,size,basis,adjacency=None,weights=None,dimension=None,tol=None,atol=None,rtol=None,kind='mergesort',verbose=False):
	'''
	Get weights of stencil of order accurate derivative along dimension for n p-dimensional data points
	Finds reduced weights a of shape (n,n) such that the derivatives at x0 are:
	df(x)/dx^{dimension}(x0) = sum_{adjacency(x0)}((f(x)-f(x0)/(x^{dimension}-x0^{dimension})*a(x0,x))
	The stencil weights of shape (n,n) are then computed as w(x0,x) = adjacency(x0).size*a(x0,x)/(x^{dimension}-x0^{dimension})^2
	The weights are found by computing the multi-dimensional basis of the data v, and for order accurate stencil and {d} points,
	this matrix has q=choose(p+order,p)-1 terms and shape ( d,q ), and so d=q points must be chosen such that the terms are linearly dependent and
	v is full rank.
	
	Args:
		x (ndarray): data array of shape (n,p)
		order (int,iterable): order of accuracy of stencil
		size (int,iterable): number of constraints of stencil
		basis (str,callable): function to yield basis, or string in ['vandermonde']
		adjacency (array,sparse_matrix): adjacency matrix of shape (n,n) of which data points are used in stencil
		weights (array,sparse_matrix): weights matrix of shape (n,n) of weights of data points used in stencil
		dimension (int): dimension of derivative, between 0 ... p-1
		tol (float): Tolerance of rank and singularity of basis
		atol (float): Absolute tolerance of difference between unique elements
		rtol (float): Relative tolerance of difference between unique elements		
		kind (str): Sort algorithm
		verbose (bool,int): Print out details of stencils							 
	Returns:
		out (ndarray,sparse_matrix): weights of stencil of shape (n,n) 
	'''

	from .estimator import OLS

	# Get basis
	default = 'vandermonde'
	bases = {'vandermonde':vandermonde}
	name = basis
	if isinstance(basis,str):
		basis = bases.get(basis,bases.get(default))
	assert callable(basis), "Error - basis function is not callable"

	# Get sparsity
	n,p = x.shape
	shape = (n,n)
	dtype = x.dtype
	sparse = issparse(adjacency)
	format = adjacency.getformat() if sparse else 'array'

	# Setup system of constraints for basis
	order = repeat(order,n) if isinstance(order,(int,np.integer)) else order			
	size = repeat(size,n) if isinstance(size,(int,np.integer)) else size			
	solver = 'lstsq' if (order!=n).any() else 'lstsq'
	estimator = OLS(solver=solver)
	unique = True


	# Get linearly independent points and differences in points
	indices,z = linearly_independent(x,x,order,size,basis,adjacency=adjacency,weights=weights,dimension=dimension,ones=False,
								   tol=tol,atol=atol,rtol=rtol,kind=kind,diagonal=False,verbose=verbose,return_differences=True)
	

	# Get number of nnz indices in each row
	sizes = np.array([i.size for i in indices])

	# Setup output data,rows,cols
	fmt = 'coo'
	data,rows,cols = [],[],[]


	# Iterate through data points and get q points to form stencil
	for i in range(n):

		# Set up linear system of constraints with chosen stencil of points
		u = subarray(z[i],indices=indices[i],format=format)
		u = u[indices[i]].todense() if sparse else u[indices[i]]
		v = basis(u,order=order[i],ones=False,dims=dimension,unique=unique)
		e = zeros(sizes[i],dtype=dtype)

		e[dimension] = 1

		estimator.fit(v.T,e)
		coef = estimator.get_coef_() + 0.0

		data.extend(coef)
		rows.extend(i*ones(indices[i].size,dtype=int))
		cols.extend(indices[i])


	# Convert data,rows,cols to array
	data,rows,cols = np.array(data),np.array(rows),np.array(cols)

	# Eliminate zeros within tolerance
	data[isclose(data,atol=atol,rtol=rtol)] = 0

	# Get output array
	out = getattr(sp.sparse,'%s_matrix'%(fmt))((data,(rows,cols)),shape=shape,dtype=dtype)

	# Eliminate zeros
	out.eliminate_zeros()

	# Get number of nnz indices in each row
	sizes = out.getnnz(-1)

	if sparse:
		fmt = 'csr'
		out = out.asformat(fmt)
		u = getattr(z[...,dimension],'to%s'%(fmt))()
		out = multiply(invert(u.power(2),constant=0.0),out,sizes)
		out = out.asformat(format)
	else:
		out = out.A
		u = z[...,dimension]
		out = multiply(invert(u**2,constant=0.0),out,sizes)





	return out


# Grammian of array
def gram(X,*args,**kwargs):
	return X.T.dot(X)

# Projection of vector onto array
def project(X,y,*args,**kwargs):
	return X.T.dot(y)

# Array rank
def rank(arr,tol=None,**kwargs):
	try:
		return np.linalg.matrix_rank(arr,tol=tol)
	except:
		return 0


# Round array
def round(arr,decimals):
	sparse = issparse(arr)
	if sparse:
		sparsematrix = issparsematrix(arr)
		sparsearray = issparsearray(arr)
		if sparsematrix:
			out = arr.copy()
			out.data[:] = np.round(asarray(out.data),decimals)
		elif sparsearray:
			out = arr.round(decimals)
	elif isndarray(arr):
		out = arr.round(decimals)
	else:
		try:
			out = arr.round(decimals)
		except:
			out = np.round(arr,decimals)
	out += 0.0
	return out

# Array condition number
def cond(arr,ord=None,**kwargs):
	try:
		return np.linalg.cond(arr,p=ord)
	except np.linalg.LinAlgError:
		return 0


def getdiag(arr,offset=0,axis1=0,axis2=1):
	'''
	Get offset diagonal of array along axis1 and axis2
	
	Args:
		arr (array): array to extract diagonal
		offset (int): offset from diagonal to extract
		axis1 (int): first axis to extract diagonal
		axis2 (int): second axis to extract diagonal
	Returns:
		out (array): array with axis1 and axis2 condensed to only diagonal elements along last axis
	'''

	out = np.diagonal(arr,offset=offset,axis1=axis1,axis2=axis2)

	return out

def setdiag(arr,value=None,k=0):
	'''
	Set k-diagonal of array with value, including explicit zeros
	
	Args:
		arr (array,sparse_matrix): array to add diagonal elements to
		value (object): value to add to array
		k (int): index of diagonal from center
	Returns:
		out (array,sparse_matrix): array with value at k-diagonal
	'''

	# Get sparsity
	shape,dtype = arr.shape,arr.dtype
	n = min(shape)
	sparse = issparse(arr)
	format = arr.getformat() if sparse else 'array'

	# Get value
	if value is None:
		value = (arr.diagonal(k=k) if sparse else getdiag(arr,offset=k))
	scalar = isscalar(value)

	# Get if value is zero for explicit zeros
	zero = (scalar and value==0) or ((not scalar) and (value==0).any())

	# Set value as array
	value = value*ones(n,dtype=type(value)) if scalar else value

	if sparse:
		if not zero:
			fmt = 'lil'
			out = arr.asformat(fmt)
			out.setdiag(value)
		else:
			fmt = 'coo'
			out = arr.asformat(fmt)
			data,row,col = out.data,out.row,out.col
			if k < 0:
				data,row,col = np.array([*data,*value[:n+k]]),np.array([*row,*arange(-k,n)]),np.array([*col,*arange(0,n+k)])
			else:
				data,row,col = np.array([*data,*value[:n-k]]),np.array([*row,*arange(0,n-k)]),np.array([*col,*arange(k,n)])
			out = getattr(sp.sparse,'%s_matrix'%(fmt))((data,(row,col)),shape=shape,dtype=dtype)
		out = out.asformat(format)
	else:
		out = adjacency.copy()
		np.fill_diagonal(adjacency,diagonal or getdiag(adjacency))

	return out


# Get if array is singular
def issingular(arr,tol=None,**kwargs):
	ndim = arr.ndim
	assert ndim==2, "Error - array is not 2 dimensional"
	n,m = arr.shape
	r = rank(arr,tol=tol,**kwargs)
	return not ((n==m) and (r==n))

# Get if array is full rank
def isfullrank(arr,tol=None,**kwargs):
	ndim = arr.ndim
	assert ndim==2, "Error - array is not 2 dimensional"
	n = min(*arr.shape)
	r = rank(arr,tol=tol,**kwargs)
	return  (r==n)




# Get if array is sparse
def issparse(arr,*args,**kwargs):
	return issparsematrix(arr) or issparsearray(arr)

# Get if array is sparse matrix
def issparsematrix(arr,*args,**kwargs):
	return sp.sparse.issparse(arr)

# Get if array is sparse array
def issparsearray(arr,*args,**kwargs):
	return isinstance(arr,sparray.SparseArray)

# Get if array is numpy array
def isndarray(arr,*args,**kwargs):
	return isinstance(arr,(np.ndarray))

# Get if array is pandas dataframe
def isdataframe(arr,*args,**kwargs):
	return isinstance(arr,(pd.DataFrame))

# Get if array is array
def isarray(arr,*args,**kwargs):
	return isndarray(arr) or issparse(arr)

# Get if array is scalar
def isscalar(arr,*args,**kwargs):
	return (not isarray(arr) and not islisttuple(arr)) or (isarray(arr) and (arr.ndim<1) and (arr.size<2))

# Get if array is None
def isnone(arr,*args,**kwargs):
	return arr is None

# Get if array is python list
def islist(arr,*args,**kwargs):
	return isinstance(arr,(list))

# Get if array is python tuple
def istuple(arr,*args,**kwargs):
	return isinstance(arr,(tuple))

# Get if array is python list,tuple
def islisttuple(arr,*args,**kwargs):
	return islist(arr) or istuple(arr)

# Random seed
def seed(seed):
	if seed is None or seed is np.random:
		return np.random.mtrand._rand
	if isinstance(seed, int):
		return np.random.RandomState(seed)
	if isinstance(seed, np.random.RandomState):
		return seed
	raise ValueError('%r cannot be used to seed a numpy.random.RandomState'
					 ' instance' % seed)


# Shuffle array along axis
def shuffle(arr,axis,inplace=True):
	axes = [axis] if isinstance(axis,(int,np.integer)) else axis
	for axis in axis:
		n = arr.shape[axis]
		indices = np.random.permutation(n)
		if inplace:
			arr[:] = take(arr,indices,axis)
		else:
			arr = take(arr,indices,axis)
	if inplace:
		return
	else:
		return arr


def checksum(objs,attrs,alls=None):
	'''
	Check attributes of objects are equal
	
	Args:
		objs (iterable): objects to check
		attrs (str,iterable): string attributes to test
	Returns:
		checksum (bool): boolean of all pairwise checks of attributes
	'''

	attrs = [attrs] if isinstance(attrs,str) else attrs
	checksum = all([all([getmethod(getattr(a,attr,a)==getattr(b,attr,b),'all')() for attr in attrs]) 
			for a in objs for b in objs])
	return checksum


# Soft threshold function for a <,=,> b
def soft_threshold(a,b):
	if a < - b:
		return (a + b)
	elif a >  b:
		return (a - b)
	else: 
		return 0


# Get norm of array along axis
def norm(arr,axis,ord,*args,where=True,**kwargs):
	def func(arr,axis=-1,dtype=None,where=True,ord=2):
		out = np.linalg.norm(arr,axis=axis,ord=ord)
		return out
	try:
		out = reduce(func,arr,axis,dtype=None,where=where,ord=ord)
	except ValueError:		
		shape = list(arr.shape)
		shape.pop(axis)
		out = ones(shape)		
	return out

def set_norm(norm_func,*args,**kwargs):
	default = 'l2'
	field = 'norm'
	norm_func = str(norm_func)
	globs = globals()
	func = globs.get('_'.join([field,norm_func]),globs['_'.join([field,default])])
	return wrapper(func,*args,**kwargs)

def norm_norm(arr,axis,ord,*args,**kwargs):
	ords = {'l1':1,'l2':2,'linf':np.inf,'uniform':'uniform'}
	ord = ords.get(ord,ord)
	return norm(arr,axis,ord,*args,**kwargs)

def norm_l2(arr,*args,**kwargs):
	ord = kwargs.pop('ord','l2')
	axis = kwargs.pop('axis',0)
	return norm_norm(arr,axis,ord,*args,**kwargs)

def norm_l1(arr,*args,**kwargs):
	ord = kwargs.pop('ord','l1')
	axis = kwargs.pop('axis',0)
	return norm_norm(arr,axis,ord,*args,**kwargs)

def norm_linf(arr,*args,**kwargs):
	ord = kwargs.pop('ord','linf')
	axis = kwargs.pop('axis',0)
	return norm_norm(arr,axis,ord,*args,**kwargs)

def norm_uniform(arr,*args,**kwargs):
	ord = kwargs.pop('ord','uniform')
	axis = kwargs.pop('axis',0)
	return norm_norm(arr,axis,ord,*args,**kwargs)

def norm_None(arr,*args,**kwargs):
	ord = kwargs.pop('ord','uniform')
	axis = kwargs.pop('axis',0)
	return norm_norm(arr,axis,ord,*args,**kwargs)


# Set array scale
def scale(arr,axis,norm_func,*args,**kwargs):

	def func(arr,axis,ord): 
		return invert(norm(arr,axis=axis,ord=ord),constant=1.0)

	ords = {'l1':1,'l2':2,'linf':np.inf}
	ord = ords.get(norm_func,norm_func)

	ndim,shape = arr.ndim,list(arr.shape)
	dims = shape.pop(-1)
	shape.pop(axis)

	if ndim < 2:
		out = func(arr,axis,ord)
		arr[...] *= out
	else:
		out = zeros((dims,*shape))		
		for dim in range(dims):
			out[dim] = func(arr[...,dim],axis,ord)
			arr[...,dim] *= out[dim]

	return out


def set_scale(norm_func,*args,**kwargs):
	return wrapper(scale,*args,norm_func=norm_func,**kwargs)



# Set array loss
def loss(y_pred,y,loss_func,*args,**kwargs):
	field = 'loss'
	default = 'rmse'
	loss_func = str(loss_func)
	globs = globals()
	func = globs.get('_'.join([field,loss_func]),globs['_'.join([field,default])])
	return func(y_pred,y,*args,**kwargs)


def set_loss(loss_func,*args,**kwargs):
	field = 'loss'
	default = 'rmse'
	loss_func = str(loss_func)
	globs = globals()
	func = globs.get('_'.join([field,loss_func]),globs['_'.join([field,default])])
	return wrapper(func,*args,**kwargs)

def loss_norm(y_pred,y,axis,ord,*args,**kwargs):
	field = 'loss'
	ords = {'l1':1,'l2':2,'linf':np.inf,'uniform':np.inf}
	ord = ords.get(ord,ord)
	try:
		inds = kwargs['%s_%s_inds'%(field,str(ord))]
		inds = inds[inds<y.shape[axis]]
	except:
		inds = slice(None)
	try:
		scale = kwargs['%s_%s_scale'%(field,str(ord))][inds]
	except:
		scale = 1
	try:
		arr = ((y_pred-y)[inds])*scale
	except:
		inds = slice(None)
		arr = ((y_pred-y)[inds])*scale
	try:
		C = (arr.shape[axis])**(-1/ord)
	except:
		C = 1
	return C*norm_norm(arr,axis=axis,ord=ord)

def loss_l2(y_pred,y,*args,**kwargs):
	ord = kwargs.pop('ord','l2')
	axis = kwargs.pop('axis',0)
	return loss_norm(y_pred,y,axis,ord,*args,**kwargs)

def loss_l1(y_pred,y,*args,**kwargs):
	ord = kwargs.pop('ord','l1')
	axis = kwargs.pop('axis',0)
	return loss_norm(y_pred,y,axis,ord,*args,**kwargs)

def loss_linf(y_pred,y,*args,**kwargs):
	ord = kwargs.pop('ord','linf')
	axis = kwargs.pop('axis',0)
	return loss_norm(y_pred,y,axis,ord,*args,**kwargs)

def loss_uniform(y_pred,y,*args,**kwargs):
	ord = kwargs.pop('ord','uniform')
	axis = kwargs.pop('axis',0)

	return loss_norm(y_pred,y,axis,ord,*args,**kwargs)

def loss_None(y_pred,y,*args,**kwargs):
	ord = kwargs.pop('ord','uniform')
	axis = kwargs.pop('axis',0)
	return loss_norm(y_pred,y,axis,ord,*args,**kwargs)


def loss_rmse(y_pred,y,*args,**kwargs):
	return loss_l2(y_pred,y,*args,**kwargs)

def loss_weighted(y_pred,y,*args,**kwargs):
	try:
		weights = kwargs['loss_weights']
	except:
		weights = {'l2':1}
	axis = kwargs.pop('axis',0)
	return sum([weights[ord]*loss_norm(y_pred,y,axis,ord,*args,**kwargs)
				for ord in weights])

def score(y_pred,y,score_func,*args,**kwargs):
	field = 'score'
	default = 'rmse'
	score_func = str(score_func)
	globs = globals()
	func = globs.get('_'.join([field,score_func]),globs['_'.join([field,default])])
	return func(y_pred,y,*args,**kwargs)

def set_score(score_func,*args,**kwargs):
	field = 'score'
	default = 'rmse'
	score_func = str(score_func)
	globs = globals()
	func = globs.get('_'.join([field,score_func]),globs['_'.join([field,default])])
	return wrapper(func,*args,**kwargs)

def score_norm(y_pred,y,axis,ord,*args,**kwargs):
	field = 'score'
	ords = {'l1':1,'l2':2,'linf':np.inf,'uniform':np.inf}
	ord = ords.get(ord,ord)
	try:
		inds = kwargs['%s_%s_inds'%(field,str(ord))]
	except:
		inds = slice(None)
	try:
		scale = kwargs['%s_%s_scale'%(field,str(ord))][inds]
	except:
		scale = 1	
	try:
		arr = ((y_pred-y)[inds])*scale
	except:
		inds = slice(None)
		arr = ((y_pred-y)[inds])*scale
	try:
		C = (arr.shape[axis])**(-1/ord)
	except:
		C = 1
	return -C*norm_norm(arr,axis=axis,ord=ord)

def score_l2(y_pred,y,*args,**kwargs):
	ord = kwargs.pop('ord','l2')
	axis = kwargs.pop('axis',0)
	return score_norm(y_pred,y,axis,ord,*args,**kwargs)

def score_l1(y_pred,y,*args,**kwargs):
	ord = kwargs.pop('ord','l1')
	axis = kwargs.pop('axis',0)
	return score_norm(y_pred,y,axis,ord,*args,**kwargs)

def score_linf(y_pred,y,*args,**kwargs):
	ord = kwargs.pop('ord','linf')
	axis = kwargs.pop('axis',0)
	return score_norm(y_pred,y,axis,ord,*args,**kwargs)

def score_uniform(y_pred,y,*args,**kwargs):
	ord = kwargs.pop('ord','uniform')
	axis = kwargs.pop('axis',0)
	return score_norm(y_pred,y,axis,ord,*args,**kwargs)

def score_rmse(y_pred,y,*args,**kwargs):
	return score_l2(y_pred,y,*args,**kwargs)

def score_weighted(y_pred,y,*args,**kwargs):
	try:
		weights = kwargs['score_weights']
	except:
		weights = {'l2':1}
	axis = kwargs.pop('axis',0)
	return sum([weights[ord]*score_norm(y_pred,y,axis,ord,*args,**kwargs)
				for ord in weights])


def score_r2(y_pred,y,*args,**kwargs):
	n = max(1,y.shape[0])
	axis = kwargs.pop('axis',0)
	ord = 'l2'
	return (norm_norm(y_pred-y,axis,ord,*args,**kwargs)*
			invert(norm_norm(y-y.mean(axis=axis),axis,ord,*args,**kwargs),constant=1.0)*
			((n-1)/(n-0)))



def criteria(loss,complexity_,losses,complexities_,criteria_func,*args,**kwargs):
	field = 'criteria'
	default = 'F_test'
	score_func = str(criteria_func)
	globs = globals()
	func = globs.get('_'.join([field,criteria_func]),globs['_'.join([field,default])])
	return func(loss,complexity_,losses,complexities_,*args,**kwargs)


def set_criteria(criteria_func,*args,**kwargs):
	field = 'criteria'
	default = 'F_test'
	criteria_func = str(criteria_func)
	globs = globals()
	func = globs.get('_'.join([field,criteria_func]),globs['_'.join([field,default])])
	return wrapper(func,*args,**kwargs)

def criteria_F_test(loss,complexity_,losses,complexities_,*args,**kwargs):
	try:
		return ((loss-losses[-1])*(complexities_[0]-complexities_[-1]))*(
				invert(((losses[-1])*(complexities_[-1]-complexity_)),constant=0.0))
	except Exception as e:
		return zeros(np.shape(loss))







# Check equality along axis
def equal_axis(a,axis,axis_axis=False):
	assert a.ndim == 2, "Multidimensional array: ndim > 2"
	if axis == 0:
		return np.all(a==take(a,0,axis),axis=axis if axis_axis else None)
	elif axis == 1:
		return np.all((a.T==take(a,0,axis)).T,axis=axis if axis_axis else None)
	return False


# Conjunction of logical conditions
def conjunction(*conditions):
	if conditions not in [[]]:
		return functools.reduce(np.logical_and, conditions)
	else:
		return None

# Disjunction of logical conditions
def disjunction(*conditions):
	if conditions not in [[]]:
		return functools.reduce(np.logical_or, conditions)
	return None




# Safely convert to constants
def convert(x,constant=1):
	isx = (np.isnan(x) | np.isinf(x) | (x==0))
	if isinstance(x,np.ndarray):
		x[isx] = constant
	else:
		x = constant if isx else x
	return x


# Convert between iterable types
def toiter(a,iterable):
	try:
		return iterable((toiter(i,iterable) for i in a))
	except:
		return a

# Make set of same iterable type
def toset(a):
	return toiter(set(toiter(a,tuple)),type(a))


# Check if iterable
def isiterable(obj):
	field = '__iter__'
	try:
		return hasattr(obj,field)
	except:
		return False


# Convert Series of lists to Array
def series_array(series,**kwargs):
	try:
		arr = np.stack(series.values,**kwargs)
	except ValueError:
		arr = [x for x in series.values]
	return arr

# Convert Array to Series of lists
def array_series(arr,**kwargs):
	series = pd.Series(arr.tolist(),**kwargs)
	return series


# Index array with list of lists indices
def masked_index(arr,indices,fill_value=-1):
	mask = np.array([len(index) for index in indices])
	mask = mask[:,None] <= arange(mask.max())    
	inds = np.full(mask.shape,fill_value,dtype=int)
	inds[~mask] = concatenate(indices)    
	out = np.ma.masked_array(arr[arange(arr.shape[0])[:,None],inds],mask,fill_value=0)
	return out


# Alias of keywords
def alias(kwds,aliases,**kwargs):
	kwds.update({aliases.get(k,k): (kwds[aliases.get(k,k)](kwargs[k]) if callable(kwds[aliases.get(k,k)]) else kwargs[k])
					for k in kwargs if ((aliases.get(k,k) in kwds) and 
					((k == aliases.get(k,k)) or (aliases.get(k,k) not in kwargs)))})
	kwds.update({k: kwds[k]() for k in kwds if callable(kwds[k])})
	return


# Check if callable
def iscallable(obj,*args,**kwargs):
	if callable(obj):
		return obj(*args,**kwargs)
	else:
		return obj

# Factors of integers
def factors(n):
	return sorted(set((f for i in range(1, int(n**0.5)+1) 
					if n % i == 0 
					for f in [i, n//i])))

# Convert between units
def units(value,bases):
	values = [value]
	for base in bases:
		values.extend(divmod(values[-1],base))



def isnumber(s):
	'''
	Check if object is a float or integer number
	
	Args:
		s(object): Object to be checked as number
	Returns:
		Boolean of whether object s is a number
	'''
	try:
		s = float(s)
		return True
	except:
		try:
			s = int(s)
			return True
		except:
			return False




def scinotation(number,decimals=2,base=10,order=2,zero=True,scilimits=[-1,1],usetex=True):
	'''
	Put number into scientific notation string
	
	Args:
		number (str,int,float): Number to be processed
		decimals (int): Number of decimals in base part of number
		base (int): Base of scientific notation
		order (int): Max power of number allowed for rounding
		zero (bool): Make numbers that equal 0 be the int representation
		scilimits (list): Limits on where not to represent with scientific notation
		usetex (bool): Render string with Latex
	Returns:
		String with scientific notation format for number
	'''
	if not isnumber(number):
		return str(number)
	try:
		number = int(number) if int(number) == float(number) else float(number)
	except:
		string = number
		return string

	maxnumber = base**order
	if number > maxnumber:
		number = number/maxnumber
		if int(number) == number:
			number = int(number)
		string = str(number)
	
	if zero and number == 0:
		string = '%d'%(number)
	
	elif isinstance(number,(int,np.integer)):
		string = str(number)
		# if usetex:
		# 	string = r'\textrm{%s}'%(string)
	
	elif isinstance(number,(float,np.float64)):		
		string = '%0.*e'%(decimals,number)
		string = string.split('e')
		basechange = np.log(10)/np.log(base)
		basechange = int(basechange) if int(basechange) == basechange else basechange
		flt = string[0]
		exp = str(int(string[1])*basechange)
		if int(exp) in range(*scilimits):
			flt = '%0.*f'%(decimals,float(flt)/(base**(-int(exp))))
			string = r'%s'%(flt)
		else:
			string = r'%s%s'%(flt,r'\cdot %d^{%s}'%(base,exp) if exp!= '0' else '')
	if usetex:
		string = r'%s'%(string.replace('$',''))
	else:
		string = string.replace('$','')
	return string

# Generator wrapper to restart stop number of times
def generator(stop=None):
	def wrap(func):
		def set(*args,**kwargs):
			return func(*args,*kwargs)
		@functools.wraps(func)
		def wrapper(*args,stop=stop,**kwargs):
			generator = set(*args,**kwargs)
			while stop:
				try:
					yield next(generator)
				except StopIteration:
					stop -= 1
					generator = set(*args,**kwargs)
					yield next(generator)
		return wrapper
	return wrap


# List from generator
def list_from_generator(generator,field=None):
	item = next(generator)
	if field is not None:
		item = item[field]    
	items = [item]
	for item in generator:
		if field is not None:
			item = item[field]
		if item == items[0]:
			break
		items.append(item)

	# Reset iterator state:
	for item in generator:
		if item == items[-1]:
			break
	return items


# Sliced indices
def slicing(arr,n,base,dim,offset=0):
	arr = arr.reshape([n]*dim)
	for d in range(dim):
		arr = take(arr,arange(offset,n,base),axis=d)
	arr = arr.reshape((-1))
	return arr

# Mesh refinement indices
def refine(n,dim,base,power):
	indices = arange(n**dim)
	slices = []
	refine = base**power
	for j in range(base):
		slices.append(slicing(indices,n,refine,dim,int(j*refine/base)))
	return slices


# Mesh refinement
def refinement(n,dim,base,powers=None,string='slice',boundary=False):
	# Given powers exponents, will refine dim-dimensional mesh 
	# by base^powers
	# powers is either None, list [power_min,power_max,power_step] or
	#	   numpy array of powers
	# i.e) L = nh, N = int(log_base(n))
	#	   n -> n/base^p for p in powers = [2,3,...,N]
	# 	   h -> h*base^p for p in powers = [2,3,...,N]
	n += (boundary*(1-n%base))

	N = int(np.log(n)/np.log(base))

	if powers is None or isinstance(powers,list):
		powers = [None,None,None] if powers is None else powers
		powers = [(powers[i] if powers[i]>=0 else N+powers[i]) if i<len(powers) and powers[i] is not None else {0:2,1:N,2:1}[i] 
					for i in range(3)]
		powers = arange(*powers)
	slices = {} 

	for power in powers:
		key = '%s_%d'%(string,power)
		slices[key] = {'%s_%d_%d'%(string,power,j):s for j,s in enumerate(refine(n,dim,base,power))}
	return slices




# Find indices of extrema of array
def extrema(x,**kwargs):

	# Find extremal points of array
	def _extrema(x,method,passes,**kwargs):
		defaults = {'distance':1}
		scales = {'min':-1,'max':1,'default':1}
		scale = scales.get(method,scales['default'])
		kwargs.update({k: kwargs.get(k,defaults[k]) for k in defaults})
		n = x.shape[0]
		inds = arange(n)
		for passer in range(passes):
			_inds = sp.signal.find_peaks(scale*x[inds],**kwargs)[0]
			inds = np.array([inds[(scale*x[inds]).argmax(axis=0)]]) if (len(_inds)==0) else _inds
		return inds


	# Weave maxima and minima of array
	def weaver(a,b,x):
		a = copy.deepcopy(a).tolist()
		b = copy.deepcopy(b).tolist()
		c = None
		d = None
		o = []
		while(len(a)>0 and len(b)>0):
			ab = a[0]<b[0]
			if a[0]==b[0]:
				a0 = a.pop(0)
				b0 = b.pop(0)
				if ab:
					o.append(a0)
				else:
					o.append(b0)
				continue
			elif ab:
				c = a
				d = b
			elif  not ab:
				c = b
				d = a
			extrema = [(i,c[i]) for i in range(len(c)) if c[i]<d[0]]
			extrema = [i for (i,x) in sorted(extrema,key=lambda i: x[i[1]],reverse=(not ab))]
			c0 = c[extrema[0]]
			for i in sorted(extrema,reverse=True):
				c.pop(i);
			if len(c) == 0:
				o.extend([c0,d[0]])
				break
			extrema = [(i,d[i]) for i in range(len(d)) if d[i]<=c[0]]
			extrema = [i for (i,x) in sorted(extrema,key=lambda i: x[i[1]],reverse=ab)]
			d0 = d[extrema[0]]
			for i in sorted(extrema,reverse=True):
				d.pop(i);

			o.extend([c0,d0])

		return o
	defaults = {'passes':1,'distance':1}
	kwargs.update({k: kwargs.get(k,defaults[k]) for k in defaults})
	n = len(x)
	inds_range = arange(n)
	inds_min = _extrema(x,'min',**kwargs)
	inds_max = _extrema(x,'max',**kwargs)
	inds = np.array(weaver(inds_min,inds_max,x))
	inds = endpoints(inds,inds_range)
	return inds


# Append endpoints of x at x_new
def endpoints(x_new,x,y_new=None,y=None):
	isx = len(x)>0
	isy = (y_new is not None) and (y is not None)

	if isx and (x[0] not in x_new):
		x_new = np.array([x[0],*x_new])
		if isy:
			y_new = np.array([y[0],*y_new])
	if isx and (x[-1] not in x_new):
		x_new = np.array([*x_new,x[-1]])
		if isy:
			y_new = np.array([*y_new,y[-1]])
	if isy:
		return x_new,y_new
	else:
		return x_new	




# Filter array by interpolating over average of envelope
def filtering(y,x=None,inds=None,**kwargs):


	# Interpolate y(x) at x_new
	def interpolate(x,y,x_new,**kwargs):
		min_points = 4
		x_min = np.min(x,axis=0)
		x_max = np.max(x,axis=0)
		x_new = x_new[(x_new>=x_min) & (x_new<=x_max)]
		if len(x)<min_points:
			return x,y
		else:
			return x_new,sp.interpolate.interp1d(x, y,**kwargs)(x_new)


	defaults = {'extrema':{'distance':1,'passes':1},'interpolate':{'kind':'cubic'}}
	kwargs.update({k:kwargs.get(k,defaults[k]) for k in defaults})

	inds = extrema(y,**kwargs['extrema']) if inds is None else inds

	n = len(y)
	m = len(inds)
	x = arange(n) if x is None else x

	x_filtered = np.split(x[inds],m//2,axis=-1).mean(axis=-1)
	y_filtered = np.split(y[inds],m//2,axis=-1).mean(axis=-1)
	x_filtered,y_filtered = endpoints(x_filtered,x,y_filtered,y)
	x_filtered,y_filtered = interpolate(x_filtered,y_filtered,x,**kwargs.get('interpolate',{}))
	return y_filtered,x_filtered,inds



# Compute size of basis
def basis_size(inputs,outputs,order,basis,constants,samples,intercept_):
	Nbasis = {
		None: lambda Ninputs,Noutputs,Nconstants,order,intercept_: (Ninputs - Nconstants + intercept_),
		'taylorseries': lambda Ninputs,Noutputs,Nconstants,order,intercept_: ((((Ninputs-Nconstants)**(order+1)-1)/((Ninputs-Nconstants)-1)) if (Ninputs-Nconstants)>1 else 1+order),
		'linear': lambda Ninputs,Noutputs,Nconstants,order,intercept_: (Ninputs - Nconstants + intercept_),
		'polynomial': lambda Ninputs,Noutputs,Nconstants,order,intercept_: ((((order+1)**Ninputs)-(1-intercept_)) if Ninputs>1 else order + intercept_) - Nconstants,
		**{k: (lambda Ninputs,Noutputs,Nconstants,order,intercept_: (Ninputs*(order+1)) - Nconstants + intercept_) 
			for k in ['monomial','chebyshev','legendre','hermite']}
		}
	Nconstant = {
		None: lambda inputs,outputs,constants: len(set([x for y in constants for x in constants[y]])) if (constants is not None and len(constants)>0) else 0,
		'taylorseries': lambda inputs,outputs,constants: max([len([x for x in constants[y] if x in inputs]) for y in constants]) if (constants is not None and len(constants)>0) else 0,
		'polynomial': lambda inputs,outputs,constants: len(set([x for y in constants for x in constants[y]])) if (constants is not None and len(constants)>0) else 0,
		**{k:(lambda inputs,outputs,constants: len(set([x for y in constants for x in constants[y]])) if (constants is not None and len(constants)>0) else 0)
			for k in ['linear','monomial','chebyshev','legendre','hermite']}
		}

	basis = None if basis not in Nconstant or basis not in Nbasis else basis
	order = max(order) if isinstance(order,list) else order


	Ninputs = len(inputs)
	Noutputs = len(outputs)
		
	Nconstants = Nconstant[basis](inputs,outputs,constants)
	Nbasises = int(Nbasis[basis](Ninputs,Noutputs,Nconstants,order,intercept_))

	return Nbasises


def position(site,n,d,dtype=np.int32):
	# Return position coordinates in d-dimensional n^{d} lattice 
	# from given linear site position in 1d N^{d} length array
	# i.e) [int(site/(self.n**(i))) % self.n for i in range(self.{d})]
	n_i = np.power(n,arange(d,dtype=dtype))

	isint = isinstance(site,(int,np.integer))

	if isint:
		site = np.array([site])
	position = np.mod(((site[:,None]/n_i)).astype(dtype),n)
	if isint:
		return position[0]
	else:
		return position

def site(position,n,d,dtype=np.int32):
	# Return linear site position in 1d n^{d} length array 
	# from given position coordinates in {d}-dimensional n^{d} lattice
	# i.e) sum(position[i]*self.n**i for i in range(self.{d}))
	
	n_i = np.power(n,arange(d,dtype=dtype))

	is1d = isinstance(position,(int,np.integer,list,tuple)) or position.ndim < 2

	if is1d:
		position = np.atleast_2d(position)
	
	site = position.dot(n_i).astype(dtype)

	if is1d:
		return site[0]
	else:
		return site






import sys
from types import ModuleType, FunctionType
from gc import get_referents

def getsizeof(obj,units='B'):
    """sum size of object & members."""

	# Custom objects know their class.
	# Function objects seem to know way too much, including modules.
	# Exclude modules as well.
    known = (type,ModuleType,FunctionType)

    if isinstance(obj, known):
        raise TypeError('getsize() does not take argument of type: '+ str(type(obj)))
    seen_ids = set()
    size = 0
    objects = [obj]
    while objects:
        need_referents = []
        for obj in objects:
            if not isinstance(obj, known) and id(obj) not in seen_ids:
                seen_ids.add(id(obj))
                size += sys.getsizeof(obj)
                need_referents.append(obj)
        objects = get_referents(*need_referents)

    conversions = {'B':1,'KB':2**10,'MB':2**20,'GB':2**30}
    conversion = conversions.get(units,1)
    size /= conversion
    return size





class lattice(object):
	
	# Define a (Square) Lattice class for lattice sites configurations with:
	# Lattice Length L, Lattice Dimension d
	
	def __init__(self,n=10,d=3):
		# Define parameters of system        
		self.n = n
		self.d = d
		self.N = n**d
		self.z = 2*d
		
		if self.N > 2**32-1:
			self.dtype = np.int64
		else:
			self.dtype=np.int32

		# Prepare arrays for Lattice functions

		# Define array of sites
		self.sites = arange(self.N)
		
		# n^i for i = 1:d array
		self.n_i = np.power(self.n,arange(self.d,dtype=self.dtype))
		
		# Arrays for finding coordinate and linear position in d dimensions
		self.I = np.identity(self.d)
		self.R = arange(1,np.ceil(self.n/2),dtype=self.dtype)

		
		# Calculate array of arrays of r-distance neighbour sites,
		# for each site, for r = 1 : n/2 
		# i.e) self.neighbour_sites = np.array([[self.neighboursites(i,r) 
		#                                 for i in range(self.N)]
		#                                 for r in range(1,
		#                                             int(np.ceil(self.n/2)))])
		self.neighbours = self.neighbour_sites()


		
	def position(self,site):
		# Return position coordinates in d-dimensional n^{d} lattice 
		# from given linear site position in 1d N^{d} length array
		# i.e) [int(site/(self.n**(i))) % self.n for i in range(self.{d})]
		isint = isinstance(site,(int,np.integer))

		if isint:
			site = np.array([site])
		position = np.mod(((site[:,None]/self.n_i)).
						astype(self.dtype),self.n)
		if isint:
			return position[0]
		else:
			return position
	
	def site(self,position):
		# Return linear site position in 1d N^{d} length array 
		# from given position coordinates in {d}-dimensional n^{d} lattice
		# i.e) sum(position[i]*self.n**i for i in range(self.{d}))
		is1d = isinstance(position,(list,tuple)) or position.ndim < 2

		if is1d:
			position = np.atleast_2d(position)
		
		site = position.dot(self.n_i).astype(self.dtype)

		if is1d:
			return site[0]
		else:
			return site
	
	def neighbour_sites(self,r=None,sites=None):
		# Return array of neighbour spin sites 
		# for a given site and r-distance neighbours
		# i.e) np.array([self.site(np.put(self.position(site),i,
		#                 lambda x: np.mod(x + p*r,self.n))) 
		#                 for i in range(self.d)for p in [1,-1]]) 
		#                 ( previous method Time-intensive for large n)
		
		if sites is None:
			sites = self.sites
		
		sitepos = self.position(sites)[:,None]
		
		if r is None:
			Rrange = self.R
		elif isinstance(r,list):
			Rrange = r
		else:
			Rrange = [r]
		return np.array([np.concatenate(
							(self.site(np.mod(sitepos+R*self.I,self.n)),
							 self.site(np.mod(sitepos-R*self.I,self.n))),1)
								for R in Rrange])                     

		
	def neighbour_states(self,r=1):
		# Return spins of r-distance neighbours for all spin sites
		return np.array([np.index(self.sites,self.neighbour_sites[r-1][i]) 
									for i in range(len(self.sites))])





====================================================================================================
mechanoChemML\src\hparameters_cnn_grid.py
====================================================================================================
import itertools
from itertools import combinations
from SALib.sample import saltelli
import numpy as np
import pickle
import matplotlib.pyplot as plt
import statistics
from os import path

def getlist_str(option, sep=',', chars=None):
    """Return a list from a ConfigParser option. By default, 
     split on a comma and strip whitespaces."""
    list0 = [(chunk.strip(chars)) for chunk in option.split(sep)]
    list0 = [x for x in list0 if x]
    return list0


def getlist_int(option, sep=',', chars=None):
    """Return a list from a ConfigParser option. By default, 
     split on a comma and strip whitespaces."""
    list0 = option.split(sep)
    list0 = [x for x in list0 if x]
    if (len(list0)) > 0:
        return [int(chunk.strip(chars)) for chunk in list0]
    else:
        return []


class HyperParametersCNN:
    """
  This class is created to perform hyper-parameters search for CNNs.
  """

    def __init__(self,
                 config,
                 input_shape=1,
                 output_shape=1,
                 filter_size=3,
                 pool_size=1,
                 padding='same',
                 dim=2,
                 uniform_sample_number=10,
                 neighbor_sample_number=5,
                 iteration_time=3,
                 sample_ratio=0.3,
                 best_model_number=10,
                 max_total_parameter=1e5,
                 repeat_train=3,
                 debug=False):

        self.config = config
        self.input_shape = input_shape
        self.output_shape = output_shape
        self.filter_size = filter_size
        self.pool_size = pool_size
        self.padding = padding
        self.dim = dim
        self.uniform_sample_number = uniform_sample_number
        self.neighbor_sample_number = neighbor_sample_number
        self.iteration_time = iteration_time
        self.sample_ratio = sample_ratio
        self.best_model_number = best_model_number
        # set a maximum parameter number to avoid gpu running out of memory
        self.max_total_parameter = max_total_parameter
        self.repeat_train = repeat_train
        self.debug = debug

        if (self.dim == 3):
            raise ValueError("Dim = 3 for Conv3D is not programmed!")

        self.all_hyperparameter_info = {
            "current_iteration": -1,
            "current_index": 0,
            "sampling_range": [],
            "search_hl_range": [],
            "search_filter_range": [],
            "index_list": [],
            "best_model": [],
            "all_model_structure": [],
            "model_short_summary": [],    # save the averaged loss, val_loss
            "model_long_summary": [],    # save the loss, val_loss history
            "model_loss_summary": [],    # save min(loss, val_loss) for each training
            "model_train_count": []
        }

        self.hyperparameter_filename = "saved_hyperparameter_search.pickle"

        self.prepare_parameter_search()

    def prepare_parameter_search(self):
        if (not self.load_saved_parameter_search()):
            self.generate_all_CNNs()
            self.initialize_info()
            self.do_the_sampling()

        print("Total CNN structures: ", len(self.all_hyperparameter_info["all_model_structure"]))
        if len(self.all_hyperparameter_info["all_model_structure"]) < self.best_model_number:
            print("...ERR...: Total CNN structure = ", len(self.all_hyperparameter_info["all_model_structure"]), " is smaller than the best model number!!!",
                  self.best_model_number)
            print("Please modify the hyperparameter search setting or delete previously saved file: ", self.hyperparameter_filename)
            print(self.all_hyperparameter_info)
            exit(0)
        # print(self.all_hyperparameter_info["index_list"])
        # print(self.all_hyperparameter_info)

    def initialize_info(self):
        """ """
        count = 0
        for _ in self.all_hyperparameter_info["all_model_structure"]:
            self.all_hyperparameter_info["model_train_count"].append(0)
            self.all_hyperparameter_info["model_short_summary"].append([count, 1e9, 1e9])
            self.all_hyperparameter_info["model_long_summary"].append([])
            self.all_hyperparameter_info["model_loss_summary"].append({'index': count, 'loss': [], 'val_loss': []})
            count += 1
        self.all_hyperparameter_info["sampling_range"] = [0, count]

    def load_saved_parameter_search(self):
        """ """
        # print(self.all_hyperparameter_info)
        if (path.exists(self.hyperparameter_filename)):
            self.all_hyperparameter_info = pickle.load(open(self.hyperparameter_filename, "rb"))
            print("successfully load saved hyperparameter files!")
            return True
        else:
            return False

    def save_current_parameter_search(self):
        """ """
        pickle_out = open(self.hyperparameter_filename, "wb")
        pickle.dump(self.all_hyperparameter_info, pickle_out)
        pickle_out.close()

    def update_model_info(self, index0, history):
        """ update the summary for a particular CNN """
        self.all_hyperparameter_info['model_train_count'][index0] += 1

        # ... get min(training loss): obsolete
        # train_loss_min = min(history['loss'])
        # train_loss_min_index = history['loss'].index(train_loss_min)
        # val_loss = history['val_loss'][train_loss_min_index]

        # ... get min(validation loss)
        val_loss_min = min(history['val_loss'])
        val_loss_min_index = history['val_loss'].index(val_loss_min)
        train_loss = history['loss'][val_loss_min_index]
        self.all_hyperparameter_info['model_loss_summary'][index0]['loss'].append(float(train_loss))    # convert to float from either float32 or float64 to avoid problems
        self.all_hyperparameter_info['model_loss_summary'][index0]['val_loss'].append(float(val_loss_min))

        # ... only save the min loss, but not the averaged value: obsolete
        # if (self.all_hyperparameter_info['model_short_summary'][index0][1] > train_loss_min):
        # self.all_hyperparameter_info['model_short_summary'][index0][1] = train_loss_min
        # self.all_hyperparameter_info['model_short_summary'][index0][2] = val_loss

        # ... get the averaged value for different loss, more reasonable.
        self.all_hyperparameter_info['model_short_summary'][index0][1] = statistics.mean(self.all_hyperparameter_info['model_loss_summary'][index0]['loss'])
        self.all_hyperparameter_info['model_short_summary'][index0][2] = statistics.mean(self.all_hyperparameter_info['model_loss_summary'][index0]['val_loss'])
        print('mean(loss):', self.all_hyperparameter_info['model_short_summary'][index0][1], 'loss:', self.all_hyperparameter_info['model_loss_summary'][index0]['loss'])
        print('mean(val_loss):', self.all_hyperparameter_info['model_short_summary'][index0][2], 'val_loss:',
              self.all_hyperparameter_info['model_loss_summary'][index0]['val_loss'])

        self.all_hyperparameter_info['model_long_summary'][index0].append(history)

        all_model_summary = self.all_hyperparameter_info['model_short_summary'][:]
        # sort with validation loss.
        all_model_summary.sort(key=lambda x: x[2])

        self.all_hyperparameter_info['best_model'] = all_model_summary
        print("current best model with least val_loss:")
        print("index \t train_loss \t\t val_loss")
        for i0 in range(0, self.best_model_number):
            m0 = self.all_hyperparameter_info['best_model'][i0]
            print(m0[0], '\t', m0[1], '\t', m0[2])
        print('model index:', [x[0] for x in self.all_hyperparameter_info['best_model'][0:self.best_model_number]])

        self.save_current_parameter_search()

        # print(self.all_hyperparameter_info)

    def compute_total_parameters_per_CNN(self, filters):
        """
    if pool_size == 1, no maxpooling layer, if pool_size > 1, pooling layer will be inserted
    if outputlayer = dense, a flatten layer will be inserted
    """

        cnn_filters = []
        cnn_layername = []
        total_parameters = 0
        conv2D_name = 'Conv2D_' + str(self.filter_size) + '_' + str(self.filter_size)
        pool2D_name = 'MaxPooling2D_' + str(self.pool_size) + '_' + str(self.pool_size)

        # initial x_dim
        x_dim = self.input_shape

        # after first pooling layer
        f0 = filters[0]
        total_parameters += (self.filter_size * self.filter_size) * f0 + f0
        # print(total_parameters)

        cnn_filters.append(f0)
        cnn_layername.append(conv2D_name)

        if (self.pool_size > 1):
            x_dim = int((self.input_shape + (self.pool_size - 1)) / self.pool_size)
            cnn_filters.append(0)
            cnn_layername.append(pool2D_name)

        for n1 in range(1, len(filters)):
            f_n0 = filters[n1 - 1]
            f_n1 = filters[n1]
            total_parameters += (self.filter_size * self.filter_size) * f_n1 * f_n0 + f_n1
            # print('this layer: ', (self.filter_size * self.filter_size) * f_n1 * f_n0 + f_n1, f_n0, f_n1)

            cnn_filters.append(f_n1)
            cnn_layername.append(conv2D_name)

            if (self.pool_size > 1):
                x_dim = int((x_dim + (self.pool_size - 1)) / self.pool_size)
                cnn_filters.append(0)
                cnn_layername.append(pool2D_name)

                # avoid this structure by setting a very layer total parameter numbers
                # because of too many maxpooling layers.
                if (x_dim == 1):
                    total_parameters += 1e9

                # print('x_dim = ', x_dim)
            # print(total_parameters)

        # add a flatten layer if needed
        if (self.config['MODEL']['OutputLayer'].lower() == 'Dense'.lower()):
            cnn_filters.append(0)
            cnn_layername.append('Flatten')
            # final dense layer
            total_parameters += x_dim * x_dim * filters[-1] + 1

            # print(total_parameters)
        # print(cnn_filters, cnn_layername, total_parameters)

        return cnn_filters, cnn_layername, total_parameters

    def generate_all_CNNs(self):

        # [1, 20]
        hidden_layer_num = getlist_int(self.config['HYPERPARAMETERS']['HiddenLayerNumber'])
        # print(hidden_layer_num)
        # [2, 512, 2]
        filter_num = getlist_int(self.config['HYPERPARAMETERS']['NodesList'])
        # print(filter_num)

        self.all_hyperparameter_info["search_hl_range"] = hidden_layer_num[:]
        self.all_hyperparameter_info["search_filter_range"] = filter_num[:]

        if (len(hidden_layer_num) != 2):
            raise ValueError("Please provide HiddenLayerNumber: min, max in the input for HYPERPARAMETERS")
        if (len(filter_num) != 3):
            raise ValueError("Please provide NodesList: min, max, step in the input for HYPERPARAMETERS")

        # create all combinations of layers and neurons
        _tmp_parameters = [[], []]
        _tmp_parameters[0] = list(range(hidden_layer_num[0], hidden_layer_num[1]))
        _tmp_parameters[1] = list(range(filter_num[0], filter_num[1], filter_num[2]))    # reverse the NodeList for decreasing filters, not common
        _tmp_studies = []

        if len(_tmp_parameters[1]) >= 18:
            print("... WARNING .. : combination n=", len(_tmp_parameters[1]), " choose r=", _tmp_parameters[0][-1], " could be huge!!!")
            print("                 try to keep n<18 !!!!")
            exit(0)

        for p1 in _tmp_parameters[0]:
            comb = combinations(_tmp_parameters[1], p1)
            # print('combine: ', comb, p1)
            count = 0
            for i1 in comb:
                count += 1
                # print(p1, i1, count)
                _tmp_studies.append(list(i1))

        # get total_parameters for each CNN
        self.all_hyperparameter_info["all_model_structure"] = []
        for _CNN in _tmp_studies:
            # print("check _CNN: ", _CNN)
            _CNN_filter, _CNN_layername, _total_parameters = self.compute_total_parameters_per_CNN(_CNN)

            # layer info, and total parameters
            self.all_hyperparameter_info["all_model_structure"].append([_CNN_filter, _CNN_layername, _total_parameters])
            # print([_CNN_filter, _CNN_layername, _total_parameters])
        # print(len(_tmp_studies))

        # sort by total parameters and remove CNNs with total_parameters exceed a predefined maximum value
        self.all_hyperparameter_info["all_model_structure"].sort(key=lambda x: x[2])
        count = 0
        for _CNN in self.all_hyperparameter_info["all_model_structure"]:
            # print(_CNN)
            if _CNN[2] > self.max_total_parameter:
                del self.all_hyperparameter_info["all_model_structure"][count:]
                return
            count += 1

    def do_the_sampling(self):
        # uniform sampling
        index_list = np.linspace(self.all_hyperparameter_info["sampling_range"][0], self.all_hyperparameter_info["sampling_range"][1], self.uniform_sample_number + 2, dtype=int)
        # exclude the 1st and last index
        extended_index_list = [x for x in index_list[1:-1]]

        # exclude the 1st index
        index_list = index_list[1:]

        # add the neighbor_sampling
        for i0 in range(0, len(index_list) - 1):
            ind0 = index_list[i0 + 0]
            ind1 = index_list[i0 + 1]

            count = 1
            for j0 in range(ind0, ind1):
                if (count >= self.neighbor_sample_number):
                    continue
                extended_index_list.append(j0)
                count += 1
            # store all info
        self.all_hyperparameter_info["index_list"] = sorted(extended_index_list)
        self.all_hyperparameter_info["current_iteration"] += 1
        self.all_hyperparameter_info["current_index"] = 0
        print('new index_list: len=', len(self.all_hyperparameter_info["index_list"]), self.all_hyperparameter_info["index_list"])
        self.save_current_parameter_search()

    def update_index_list(self):
        """ """
        print("update_index_list")

        # get all the trained models
        all_model_summary = self.all_hyperparameter_info['model_short_summary'][:]
        all_model_summary.sort(key=lambda x: x[2])
        all_model_summary = [x for x in all_model_summary if x[1] < 1e6]
        # print('all model summary: ', all_model_summary)

        # choose the ratio
        total_select_model = int(len(all_model_summary) * self.sample_ratio)
        model_index = [x[0] for x in all_model_summary[0:total_select_model]]
        model_index.sort()
        # print(total_select_model, model_index)
        self.all_hyperparameter_info["sampling_range"] = [model_index[0], model_index[-1]]
        self.do_the_sampling()

    def get_next_model(self):
        """ """

        if (self.all_hyperparameter_info["current_iteration"] == self.iteration_time
                and self.all_hyperparameter_info["current_index"] == len(self.all_hyperparameter_info["index_list"])):
            print(
                "You reach to the end of the searching criteria. You can test other models by restart the simulation and increase the iteration_time. (By the meantime, you can also increase the uniform_sample_number and neighbor_sample_number.)"
            )
            exit(0)
        elif (self.all_hyperparameter_info["current_iteration"] < self.iteration_time
              and self.all_hyperparameter_info["current_index"] == len(self.all_hyperparameter_info["index_list"])):
            print("come to next iteration", self.all_hyperparameter_info["current_index"], len(self.all_hyperparameter_info["index_list"]),
                  self.all_hyperparameter_info["current_iteration"], self.iteration_time)
            self.update_index_list()

        index0 = self.all_hyperparameter_info["index_list"][self.all_hyperparameter_info["current_index"]]

        if (self.all_hyperparameter_info["model_train_count"][index0] >= self.repeat_train):
            self.all_hyperparameter_info["current_index"] += 1

        this_CNN = self.all_hyperparameter_info["all_model_structure"][index0]
        print("index0: ", index0, this_CNN)
        ##[16, 16, 16, 16, 16, 16]
        self.config['MODEL']['NodesList'] = ','.join([str(x) for x in this_CNN[0]])
        self.config['MODEL']['LayerName'] = ','.join(this_CNN[1])

        act_fcn = getlist_str(self.config['HYPERPARAMETERS']['Activation'])
        if (len(act_fcn) > 1):
            raise ValueError("Please provide only one Activation for HYPERPARAMETERS")
        else:
            self.act_fcn = act_fcn[0]

        self.config['MODEL']['Activation'] = ','.join([self.act_fcn] * len(this_CNN[0]))
        self.config['MODEL']['Padding'] = ','.join([self.padding] * len(this_CNN[0]))

        self.para_str = 'V' + str(this_CNN[2]) + '-' + 'model-' + 'H' + str(len(getlist_int(self.config['MODEL']['NodesList']))) \
                                 + 'N' + str(getlist_int(self.config['MODEL']['NodesList'])[0]) \
                                 + 'A' + getlist_str(self.config['MODEL']['Activation'])[0] \
                                 + 'L' + self.config['MODEL']['LearningRate'].replace(", ", '-') \
                                 + 'O' + self.config['MODEL']['Optimizer']

        if (self.all_hyperparameter_info["model_train_count"][index0] >= self.repeat_train):
            print("model: ", index0, " has been trained, move to next model!")
            return index0, self.para_str, False
        else:
            return index0, self.para_str, True

    def plot_best_models(self, model_number_to_plot=5):
        """ """
        print('[layer, neuron, total variable]', '[model index, loss, validation loss]')
        for m0 in range(0, model_number_to_plot):
            index0 = self.all_hyperparameter_info["best_model"][m0][0]
            # print(index0)
            summary = self.all_hyperparameter_info["model_long_summary"][index0][0]
            print(self.all_hyperparameter_info["all_model_structure"][index0], self.all_hyperparameter_info["model_short_summary"][index0])
            dnn_info = "-".join([str(x) for x in self.all_hyperparameter_info["all_model_structure"][index0]])
            val = plt.plot(summary["loss"], '-', label=str(index0) + ': loss ' + dnn_info)
            plt.plot(summary["val_loss"], '--', label=str(index0) + ': val_loss ' + dnn_info, color=val[0].get_color())
            plt.yscale('log')
        plt.legend()
        plt.show()

    def plot_all_models(self):
        """ """
        _short_summary = self.all_hyperparameter_info["model_short_summary"][:]
        short_summary = [x for x in _short_summary if x[1] < 1e6]

        parameter_number = []
        train_loss = []
        val_loss = []
        model_info = []
        for s0 in short_summary:
            index0 = s0[0]
            parameter_number.append(self.all_hyperparameter_info["all_model_structure"][index0][2])
            train_loss.append(s0[1])
            val_loss.append(s0[2])
            model_info.append('L' + str(self.all_hyperparameter_info["all_model_structure"][index0][0]) + 'N' + str(self.all_hyperparameter_info["all_model_structure"][index0][1]))
            # print(self.all_hyperparameter_info["all_model_structure"][index0], '\t', s0[1], '\t', s0[2])

        # print(short_summary)
        # print(parameter_number)
        # print(train_loss)
        # print(val_loss)
        # print(model_info)

        fig, ax = plt.subplots()
        ax.scatter(parameter_number, train_loss, label='train_loss')
        ax.scatter(parameter_number, val_loss, label='val_loss')
        for i, txt in enumerate(model_info):
            ax.annotate(txt, (parameter_number[i], train_loss[i]))
            # ax.annotate(txt, (parameter_number[i], val_loss[i]))
        # plt.plot(parameter_number, train_loss, marker='o', label='train loss')
        # plt.plot(parameter_number, val_loss, marker='o', label='val loss')
        plt.yscale('log')
        ax.set_ylim([1e-7, 1e-0])
        plt.legend()
        plt.show()


if __name__ == '__main__':
    import sys
    print("...testing... : ", sys.argv[0])
    config = ""
    parameter = HyperParametersCNN(config, input_shape=61, output_shape=1)
    # print (parameter.all_hyperparameter_info["all_model_structure"], len(parameter.all_hyperparameter_info["all_model_structure"]))
    # print(type(index_list), index_list)
    # parameter.save_current_parameter_search()
    # parameter.load_saved_parameter_search()

    # l2 norm: not to large, avoid singularity
    # train very slow
    # train very long
    # small neuron networks

    try:
        parameter.plot_best_models(10)
        parameter.plot_all_models()
    except:
        parameter.plot_best_models(3)
        parameter.plot_all_models()


====================================================================================================
mechanoChemML\src\hparameters_dnn_grid.py
====================================================================================================
import itertools
from SALib.sample import saltelli
import numpy as np
import pickle
import matplotlib.pyplot as plt
import statistics
from os import path

def getlist_str(option, sep=',', chars=None):
    """Return a list from a ConfigParser option. By default, 
     split on a comma and strip whitespaces."""
    list0 = [(chunk.strip(chars)) for chunk in option.split(sep)]
    list0 = [x for x in list0 if x]
    return list0


def getlist_int(option, sep=',', chars=None):
    """Return a list from a ConfigParser option. By default, 
     split on a comma and strip whitespaces."""
    list0 = option.split(sep)
    list0 = [x for x in list0 if x]
    if (len(list0)) > 0:
        return [int(chunk.strip(chars)) for chunk in list0]
    else:
        return []

class HyperParametersDNN:
    """
  This class is created to perform hyper-parameters search for DNNs.
  """

    def __init__(self,
                 config,
                 input_shape=1,
                 output_shape=1,
                 uniform_sample_number=10,
                 neighbor_sample_number=5,
                 iteration_time=3,
                 sample_ratio=0.3,
                 best_model_number=10,
                 max_total_parameter=1e5,
                 repeat_train=3,
                 debug=False):

        self.config = config
        self.input_shape = input_shape
        self.output_shape = output_shape
        self.uniform_sample_number = uniform_sample_number
        self.neighbor_sample_number = neighbor_sample_number
        self.iteration_time = iteration_time
        self.sample_ratio = sample_ratio
        self.best_model_number = best_model_number
        # set a maximum parameter number to avoid gpu running out of memory
        self.max_total_parameter = max_total_parameter
        self.repeat_train = repeat_train
        self.debug = debug

        self.all_hyperparameter_info = {
            "current_iteration": -1,
            "current_index": 0,
            "sampling_range": [],
            "search_hl_range": [],
            "search_neuron_range": [],
            "index_list": [],
            "best_model": [],
            "all_model_structure": [],
            "model_short_summary": [],    # save the averaged loss, val_loss
            "model_long_summary": [],    # save the loss, val_loss history
            "model_loss_summary": [],    # save min(loss, val_loss) for each training
            "model_train_count": []
        }

        self.hyperparameter_filename = "saved_hyperparameter_search.pickle"

        self.prepare_parameter_search()

    def prepare_parameter_search(self):
        if (not self.load_saved_parameter_search()):
            self.generate_all_DNNs()
            self.initialize_info()
            self.do_the_sampling()

        print("total DNN structures: ", len(self.all_hyperparameter_info["all_model_structure"]))
        # print(self.all_hyperparameter_info["index_list"])
        # print(self.all_hyperparameter_info)

    def initialize_info(self):
        """ """
        count = 0
        for _ in self.all_hyperparameter_info["all_model_structure"]:
            self.all_hyperparameter_info["model_train_count"].append(0)
            self.all_hyperparameter_info["model_short_summary"].append([count, 1e9, 1e9])
            self.all_hyperparameter_info["model_long_summary"].append([])
            self.all_hyperparameter_info["model_loss_summary"].append({'index': count, 'loss': [], 'val_loss': []})
            count += 1
        self.all_hyperparameter_info["sampling_range"] = [0, count]

    def load_saved_parameter_search(self):
        """ """
        # print(self.all_hyperparameter_info)
        if (path.exists(self.hyperparameter_filename)):
            self.all_hyperparameter_info = pickle.load(open(self.hyperparameter_filename, "rb"))
            print("successfully load saved hyperparameter files!")
            return True
        else:
            return False

    def save_current_parameter_search(self):
        """ """
        pickle_out = open(self.hyperparameter_filename, "wb")
        pickle.dump(self.all_hyperparameter_info, pickle_out)
        pickle_out.close()

    def update_model_info(self, index0, history):
        """ update the summary for a particular DNN """
        self.all_hyperparameter_info['model_train_count'][index0] += 1

        # ... get min(training loss): obsolete
        # train_loss_min = min(history['loss'])
        # train_loss_min_index = history['loss'].index(train_loss_min)
        # val_loss = history['val_loss'][train_loss_min_index]

        # ... get min(validation loss)
        val_loss_min = min(history['val_loss'])
        val_loss_min_index = history['val_loss'].index(val_loss_min)
        train_loss = history['loss'][val_loss_min_index]
        self.all_hyperparameter_info['model_loss_summary'][index0]['loss'].append(float(train_loss))
        self.all_hyperparameter_info['model_loss_summary'][index0]['val_loss'].append(float(val_loss_min))

        # ... only save the min loss, but not the averaged value: obsolete
        # if (self.all_hyperparameter_info['model_short_summary'][index0][1] > train_loss_min):
        # self.all_hyperparameter_info['model_short_summary'][index0][1] = train_loss_min
        # self.all_hyperparameter_info['model_short_summary'][index0][2] = val_loss

        # ... get the averaged value for different loss, more reasonable.
        self.all_hyperparameter_info['model_short_summary'][index0][1] = statistics.mean(self.all_hyperparameter_info['model_loss_summary'][index0]['loss'])
        self.all_hyperparameter_info['model_short_summary'][index0][2] = statistics.mean(self.all_hyperparameter_info['model_loss_summary'][index0]['val_loss'])
        print('mean(loss):', self.all_hyperparameter_info['model_short_summary'][index0][1], 'loss:', self.all_hyperparameter_info['model_loss_summary'][index0]['loss'])
        print('mean(val_loss):', self.all_hyperparameter_info['model_short_summary'][index0][2], 'val_loss:',
              self.all_hyperparameter_info['model_loss_summary'][index0]['val_loss'])

        self.all_hyperparameter_info['model_long_summary'][index0].append(history)

        all_model_summary = self.all_hyperparameter_info['model_short_summary'][:]
        # sort with validation loss.
        all_model_summary.sort(key=lambda x: x[2])

        self.all_hyperparameter_info['best_model'] = all_model_summary
        print("current best model with least val_loss:")
        print("index \t train_loss \t\t val_loss")
        for i0 in range(0, self.best_model_number):
            m0 = self.all_hyperparameter_info['best_model'][i0]
            print(m0[0], '\t', m0[1], '\t', m0[2])
        print('model index:', [x[0] for x in self.all_hyperparameter_info['best_model'][0:self.best_model_number]])

        self.save_current_parameter_search()

        # print(self.all_hyperparameter_info)

    def compute_total_parameters_per_DNN(self, layers, neurons):
        """
    Here, identical neurons per layer is assumed.  
    """
        total_parameters = 0

        # input layer parameters
        total_parameters = (self.input_shape + 1) * neurons

        # output layer parameters
        total_parameters += (neurons + 1) * self.output_shape

        # hidden layers parameters
        total_parameters += (neurons + 1) * neurons * (layers - 1)

        return total_parameters

    def generate_all_DNNs(self):

        # [1, 20]
        hidden_layer_num = getlist_int(self.config['HYPERPARAMETERS']['HiddenLayerNumber'])
        # print(hidden_layer_num)
        # [2, 512, 2]
        neuron_num = getlist_int(self.config['HYPERPARAMETERS']['NodesList'])
        # print(neuron_num)

        self.all_hyperparameter_info["search_hl_range"] = hidden_layer_num[:]
        self.all_hyperparameter_info["search_neuron_range"] = neuron_num[:]

        if (len(hidden_layer_num) != 2):
            raise ValueError("Please provide HiddenLayerNumber: min, max in the input for HYPERPARAMETERS")
        if (len(neuron_num) != 3):
            raise ValueError("Please provide NodesList: min, max, step in the input for HYPERPARAMETERS")

        # create all combinations of layers and neurons
        _tmp_parameters = [[], []]
        _tmp_parameters[0] = range(hidden_layer_num[0], hidden_layer_num[1])
        _tmp_parameters[1] = range(neuron_num[0], neuron_num[1], neuron_num[2])
        _tmp_studies = list(itertools.product(*_tmp_parameters))

        # get total_parameters for each DNN
        self.all_hyperparameter_info["all_model_structure"] = []
        for _DNN in _tmp_studies:
            _total_parameters = self.compute_total_parameters_per_DNN(_DNN[0], _DNN[1])

            # layer number, neuron number, and total parameters, loss
            self.all_hyperparameter_info["all_model_structure"].append([_DNN[0], _DNN[1], _total_parameters])
            # print([_DNN[0], _DNN[1], _total_parameters])

        # sort by total parameters and remove DNNs with total_parameters exceed a predefined maximum value
        self.all_hyperparameter_info["all_model_structure"].sort(key=lambda x: x[2])
        count = 0
        for _DNN in self.all_hyperparameter_info["all_model_structure"]:
            # print(_DNN)
            if _DNN[2] > self.max_total_parameter:
                del self.all_hyperparameter_info["all_model_structure"][count:]
                return
            count += 1

    def do_the_sampling(self):
        # uniform sampling
        index_list = np.linspace(self.all_hyperparameter_info["sampling_range"][0], self.all_hyperparameter_info["sampling_range"][1], self.uniform_sample_number + 2, dtype=int)
        # exclude the 1st and last index
        extended_index_list = [x for x in index_list[1:-1]]

        # exclude the 1st index
        index_list = index_list[1:]

        # add the neighbor_sampling
        for i0 in range(0, len(index_list) - 1):
            ind0 = index_list[i0 + 0]
            ind1 = index_list[i0 + 1]

            # make sure the neighbor sampling will cover as many different NN structures as possible with different hidden layers
            search_layer_number = list(range(self.all_hyperparameter_info["search_hl_range"][0], self.all_hyperparameter_info["search_hl_range"][1]))
            # print(type(search_layer_number), search_layer_number)
            # remove the layer number of model structure ind0
            _layer_number = self.all_hyperparameter_info["all_model_structure"][ind0][0]
            search_layer_number.remove(_layer_number)

            count = 1
            for j0 in range(ind0, ind1):
                if (count >= self.neighbor_sample_number):
                    continue
                __layer_number = self.all_hyperparameter_info["all_model_structure"][j0][0]

                if __layer_number in search_layer_number:
                    search_layer_number.remove(__layer_number)
                    extended_index_list.append(j0)
                    count += 1
            # store all info
        self.all_hyperparameter_info["index_list"] = sorted(extended_index_list)
        self.all_hyperparameter_info["current_iteration"] += 1
        self.all_hyperparameter_info["current_index"] = 0
        print('new index_list: len=', len(self.all_hyperparameter_info["index_list"]), self.all_hyperparameter_info["index_list"])
        self.save_current_parameter_search()

    def update_index_list(self):
        """ """
        print("update_index_list")

        # get all the trained models
        all_model_summary = self.all_hyperparameter_info['model_short_summary'][:]
        all_model_summary.sort(key=lambda x: x[2])
        all_model_summary = [x for x in all_model_summary if x[1] < 1e6]
        # print('all model summary: ', all_model_summary)

        # choose the ratio
        total_select_model = int(len(all_model_summary) * self.sample_ratio)
        model_index = [x[0] for x in all_model_summary[0:total_select_model]]
        model_index.sort()
        # print(total_select_model, model_index)
        self.all_hyperparameter_info["sampling_range"] = [model_index[0], model_index[-1]]
        self.do_the_sampling()

    def get_next_model(self):
        """ """

        if (self.all_hyperparameter_info["current_iteration"] == self.iteration_time
                and self.all_hyperparameter_info["current_index"] == len(self.all_hyperparameter_info["index_list"])):
            print(
                "You reach to the end of the searching criteria. You can test other models by restart the simulation and increase the iteration_time. (By the meantime, you can also increase the uniform_sample_number and neighbor_sample_number.)"
            )
            exit(0)
        elif (self.all_hyperparameter_info["current_iteration"] < self.iteration_time
              and self.all_hyperparameter_info["current_index"] == len(self.all_hyperparameter_info["index_list"])):
            print("come to next iteration", self.all_hyperparameter_info["current_index"], len(self.all_hyperparameter_info["index_list"]),
                  self.all_hyperparameter_info["current_iteration"], self.iteration_time)
            self.update_index_list()

        index0 = self.all_hyperparameter_info["index_list"][self.all_hyperparameter_info["current_index"]]

        if (self.all_hyperparameter_info["model_train_count"][index0] >= self.repeat_train):
            self.all_hyperparameter_info["current_index"] += 1

        this_DNN = self.all_hyperparameter_info["all_model_structure"][index0]
        print("index0: ", index0, this_DNN)
        ##[16, 16, 16, 16, 16, 16]
        self.config['MODEL']['NodesList'] = ','.join([str(this_DNN[1])] * this_DNN[0])

        act_fcn = getlist_str(self.config['HYPERPARAMETERS']['Activation'])
        if (len(act_fcn) > 1):
            raise ValueError("Please provide only one Activation for HYPERPARAMETERS")
        else:
            self.act_fcn = act_fcn[0]

        self.config['MODEL']['Activation'] = ','.join([self.act_fcn] * this_DNN[0])

        self.para_str = 'V' + str(this_DNN[2]) + '-' + 'model-' + 'H' + str(len(getlist_int(self.config['MODEL']['NodesList']))) \
                                 + 'N' + str(getlist_int(self.config['MODEL']['NodesList'])[0]) \
                                 + 'A' + getlist_str(self.config['MODEL']['Activation'])[0] \
                                 + 'L' + self.config['MODEL']['LearningRate'].replace(", ", '-') \
                                 + 'O' + self.config['MODEL']['Optimizer']

        if (self.all_hyperparameter_info["model_train_count"][index0] >= self.repeat_train):
            print("model: ", index0, " has been trained, move to next model!")
            return index0, self.para_str, False
        else:
            return index0, self.para_str, True

    def plot_best_models(self, model_number_to_plot=5):
        """ """
        print('[layer, neuron, total variable]', '[model index, loss, validation loss]')
        for m0 in range(0, model_number_to_plot):
            index0 = self.all_hyperparameter_info["best_model"][m0][0]
            # print(index0)
            summary = self.all_hyperparameter_info["model_long_summary"][index0][0]
            print(self.all_hyperparameter_info["all_model_structure"][index0], self.all_hyperparameter_info["model_short_summary"][index0])
            dnn_info = "-".join([str(x) for x in self.all_hyperparameter_info["all_model_structure"][index0]])
            val = plt.plot(summary["loss"], '-', label=str(index0) + ': loss ' + dnn_info)
            plt.plot(summary["val_loss"], '--', label=str(index0) + ': val_loss ' + dnn_info, color=val[0].get_color())
            plt.yscale('log')
        plt.legend()
        plt.show()

    def plot_all_models(self):
        """ """
        _short_summary = self.all_hyperparameter_info["model_short_summary"][:]
        short_summary = [x for x in _short_summary if x[1] < 1e6]

        parameter_number = []
        train_loss = []
        val_loss = []
        model_info = []
        for s0 in short_summary:
            index0 = s0[0]
            parameter_number.append(self.all_hyperparameter_info["all_model_structure"][index0][2])
            train_loss.append(s0[1])
            val_loss.append(s0[2])
            model_info.append('L' + str(self.all_hyperparameter_info["all_model_structure"][index0][0]) + 'N' + str(self.all_hyperparameter_info["all_model_structure"][index0][1]))
            # print(self.all_hyperparameter_info["all_model_structure"][index0], '\t', s0[1], '\t', s0[2])

        # print(short_summary)
        # print(parameter_number)
        # print(train_loss)
        # print(val_loss)
        # print(model_info)

        fig, ax = plt.subplots()
        ax.scatter(parameter_number, train_loss, label='train_loss')
        ax.scatter(parameter_number, val_loss, label='val_loss')
        for i, txt in enumerate(model_info):
            ax.annotate(txt, (parameter_number[i], train_loss[i]))
            # ax.annotate(txt, (parameter_number[i], val_loss[i]))
        # plt.plot(parameter_number, train_loss, marker='o', label='train loss')
        # plt.plot(parameter_number, val_loss, marker='o', label='val loss')
        plt.yscale('log')
        ax.set_ylim([1e-7, 1e-0])
        plt.legend()
        plt.show()


if __name__ == '__main__':
    import sys
    print("...testing... : ", sys.argv[0])
    config = ""
    parameter = HyperParametersDNN(config, input_shape=4, output_shape=1)
    # print (parameter.all_hyperparameter_info["all_model_structure"], len(parameter.all_hyperparameter_info["all_model_structure"]))
    # print(type(index_list), index_list)
    # parameter.save_current_parameter_search()
    # parameter.load_saved_parameter_search()

    # l2 norm: not to large, avoid singularity
    # train very slow
    # train very long
    # small neuron networks

    try:
        parameter.plot_best_models(10)
        parameter.plot_all_models()
    except:
        parameter.plot_best_models(3)
        parameter.plot_all_models()


====================================================================================================
mechanoChemML\src\idnn.py
====================================================================================================
import sys, os

from tensorflow import keras
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Lambda, Dropout, Concatenate, Reshape
import tensorflow.keras.backend as K
from mechanoChemML.src.transform_layer import Transform
import tensorflow as tf

import numpy as np


class IDNN(tf.keras.Model):
  
  """
  Integrable deep neural network (idnn)
  Keras model class.

  :param input_dim: Size of input vector
  :type input_dim: int

  :param hidden_units: List containing the number of neurons in each hidden layer
  :type hidden_units: [int]

  :param dropout: Dropout parameter applied after each hidden layer (default is None)
  :type dropout: float

  :param transforms: List of functions to transform the input vector, applied before the first hidden layer (default is None)
  :type transforms: [function]

  :param unique_inputs: if True, requires separate input vectors for the function, its gradient, and its Hessian; if False, assumes the same input vector will be used for function and all derivatives
  :type unique_inputs: bool

  :param final_bias: if True, a bias is applied to the output layer (this cannot be used if only derivative data is used in training); if False, no bias is applied to the output layer (default is False)
  :type final_bias: bool

  The idnn can be trained with first derivative (gradient) data, second derivative (Hessian) data, and/or data from the function itself. (If only derivative data is used then the ``final_bias`` parameter must be ``False``.) The training data for the function and its derivatives can be given at the same input values (in which case, ``unique_inputs`` should be ``False``), or at different input values, e.g. providing the function values at :math:`x \in \{0,1,2,3\}` and the derivative values at :math:`x \in \{0.5,1.5,2.5,3.5\}` (requiring ``unique_inputs`` to be ``True``). Even when ``unique_inputs`` is ``True``, however, the same number of data points must be given for the derivatives and function, even though the input values themselves are different. So, for example, if one had first derivative values at :math:`x \in \{0,1,2,3\}` and second derivative values only at :math:`x \in \{0.5,1.5,2.5\}`, then some of the second derivative data would need to be repeated to that the number of data points are equal, e.g. :math:`x \in \{0.5,1.5,2.5,2.5\}`. Currently, the IDNN structure assumes the function output is a scalar, the gradient is a vector, and the Hessian is a matrix.

  The following is an example where values for the function and the first derivative are used for training, but they are known at different input values. Note that the loss and loss_weights are defined only for the given data (function data and first derivatative data), but fictitious data has to be given for the second derivative or an error will be thrown:

  .. code-block:: python 

     idnn = IDNN(1,
            [20,20],
            unique_inputs=True,
            final_bias=True)

     idnn.compile(loss=['mse','mse',None],
             loss_weights=[0.01,1,None],
             optimizer=keras.optimizers.RMSprop(lr=0.01))

     idnn.fit([c_train0,c_train,0*c_train],
              [g_train0,mu_train,0*mu_train],
              epochs=50000,
              batch_size=20)

  """

  def __init__(self, input_dim,hidden_units,activation='softplus',dropout=None,transforms=None,unique_inputs=False,final_bias=False):
    super().__init__()
    
    self.transforms = transforms
    self.unique_inputs = unique_inputs
    
    # Define dense layers
    self.dnn_layers = []
    self.dnn_layers.append(Dense(hidden_units[0], activation=activation, input_dim=input_dim))
    for i in range(1,len(hidden_units)):
      self.dnn_layers.append(Dense(hidden_units[i], activation=activation))
      if dropout:
        self.dnn_layers.append(Dropout(dropout))
    self.dnn_layers.append(Dense(1,use_bias=final_bias))
        
  @tf.function(autograph=False)
  def call(self, inputs):

    def DNN(y):
      if self.transforms:
        y = Transform(self.transforms)(y)
      for layer in self.dnn_layers:
        y = layer(y)
      return y

    if self.unique_inputs:
      x1 = inputs[0]
      x2 = inputs[1]
      x3 = inputs[2]
      
      y = DNN(x1)
      
      with tf.GradientTape() as g:
        g.watch(x2)
        y2 = DNN(x2)
      dy = g.gradient(y2,x2)
      
      with tf.GradientTape() as g2:
        g2.watch(x3)
        with tf.GradientTape() as g1:
          g1.watch(x3)
          y3 = DNN(x3)
        dy3 = g1.gradient(y3,x3)
      ddy = g2.batch_jacobian(dy3,x3)
      
    else:
      x1 = inputs
      with tf.GradientTape() as g2:
        g2.watch(x1)
        with tf.GradientTape() as g1:
          g1.watch(x1)
          y = DNN(x1)
        dy = g1.gradient(y,x1)
      ddy = g2.batch_jacobian(dy,x1)

    return [y,dy,ddy]  


def convex(M):

    # Check if positive definite
    try:
        np.linalg.cholesky(0.5*(M+M.T))
        return 1
    except np.linalg.LinAlgError:
        return 0

def convexMult(Ms):

    ind = np.zeros(Ms.shape[0],dtype=np.bool)
    for i in range(Ms.shape[0]):
        ind[i] = convex(Ms[i])

    return ind

def find_wells(idnn,x,dim=4,bounds=[0,0.25],rereference=True):

    # Find "wells" (regions of convexity, with low gradient norm)

    # First, rereference the free energy
    if idnn.unique_inputs:
        pred = idnn.predict([x,x,x])
    else:
        pred = idnn.predict(x)
    mu_test = 0.01*pred[1]
    if rereference:
        eta_test = np.array([bounds[0]*np.ones(dim),
                             bounds[1]*np.ones(dim)])
        if idnn.unique_inputs:
            y = 0.01*idnn.predict([eta_test,eta_test,eta_test])[0]
        else:
            y = 0.01*idnn.predict(eta_test)[0]
        g0 = y[0,0]
        g1 = y[1,0]
        mu_test[:,0] = mu_test[:,0] - 1./bounds[1]*(g1 - g0)
    gradNorm = np.sqrt(np.sum(mu_test**2,axis=-1))

    H = pred[2] # get the list of Hessian matrices
    ind2 = convexMult(H) # indices of points with local convexity
    eta = x[ind2]
    gradNorm = gradNorm[ind2]

    ind3 = np.argsort(gradNorm)
    
    # Return eta values with local convexity, sorted by gradient norm (low to high)

    return eta[ind3]


====================================================================================================
mechanoChemML\src\kbnn.py
====================================================================================================
import tensorflow as tf
import numpy as np
from tensorflow import keras
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Lambda, Dropout, Add
import tensorflow.keras.backend as K

def DNN(input_dim,hidden_units,output_dim=1,dropout=None,final_bias=False):
    # Define dense layers
    myLayers = []
    for i in range(len(hidden_units)):
        myLayers.append(Dense(hidden_units[i], activation='softplus',name=str(i)))
        if dropout:
           myLayers.append(Dropout(dropout))
    myLayers.append(Dense(output_dim,use_bias=final_bias,name='final'))
    
    # Define inputs and outputs
    inputs = Input(shape=(input_dim,))
    y = inputs
    for layer in myLayers:
        y = layer(y)
    outputs = y

    dnn = Model(inputs=inputs, outputs=outputs)
    return dnn

class KBNN():

    def __init__(self,LF,input_dim,hidden_units,output_dim=1,dropout=None,final_bias=False):
        # High fidelity "correcting" layers
        self.myLayers = []
        for i in range(len(hidden_units)):
            self.myLayers.append(Dense(hidden_units[i], activation='softplus'))
            if dropout:
                self.myLayers.append(Dropout(dropout))
        self.myLayers.append(Dense(output_dim))

        # Low fidelity model, lf = rho*LF(rho2*inputs+a2)
        # shift/scale layer, has weight of rho2 and bias of a2
        self.rho2_a2 = Dense(input_dim, kernel_initializer='ones')
        
        # Set LF model as not trainable
        self.LF = LF
        self.LF.trainable = False

        # weight LF contribution
        self.rho = Dense(output_dim, kernel_initializer='ones', use_bias=False)
        
        inputs = Input(shape=(input_dim,))
        # Pass input through HF correcting layers
        y = inputs
        for layer in self.myLayers:
            y = layer(y)
        correction = y

        # Pass input through shifted/scaled/weighted LF model
        # lf = self.rho*self.LF(self.rho2*inputs+self.a2)
        lf = inputs
        lf = self.rho2_a2(lf)
        lf = self.LF(lf)
        lf = self.rho(lf)

        # Combine in output layer
        outputs = Add()([correction, lf])

        self.model = Model(inputs=inputs,outputs=outputs)

        # self.LF = LF
        # self.rho = tf.Variable(1.)
        # self.rho2 = tf.Variable(np.ones(input_dim,dtype=np.float32))
        # self.a2 = tf.Variable(np.zeros(input_dim,dtype=np.float32))     

    def kbnn_loss(self, y_true, y_pred):
        c1 = 5.#10.
        c2 = 5.#10.
        c3 = 5.#10.
        MSE = keras.losses.mean_squared_error
        r = self.rho.get_weights()[0]
        r2 = self.rho2_a2.get_weights()[0]
        a2 = self.rho2_a2.get_weights()[1]
        #print('MSE(y_true, y_pred).shape: ', MSE(y_true,y_pred).shape)
        #print('a2.shape: ', a2.shape)
        return (MSE(y_true, y_pred) + c1*(r-1.)**2 + c2*tf.tensordot(r2-1,r2-1,axes=2) + c3*tf.tensordot(a2,a2,axes=1))


====================================================================================================
mechanoChemML\src\kfold_train.py
====================================================================================================
from sklearn.model_selection import KFold
import numpy as np
import pickle
from os import path
import tensorflow as tf
import matplotlib.pyplot as plt


class MLKFold:
    """ 
    this class is created to speed up the k-fold training process 
    by default, after an initial shuffle, 10% data will be saved for testing
    the 90% dataset will be used for k-fold train
    to train the best NN structure, 90% dataset will be split as 80%, 10% for train and validation
    the held 10% testing dataset will be used for final model evaluation 
    """

    def __init__(self, total_folds, dataset, split_ratio=[0.8, 0.1, 0.1]):
        """ 
        init the class 
        """

        size_of_data = len(dataset)

        self.kfold_filename = "saved_kfold_status.pickle"
        self.kfold_info = {
            "total_folds": total_folds,
            "size_of_data": size_of_data,
            "current_fold": 0,
            "train_index_list": [],
            "validation_index_list": [],
            "final_test_index_list": [],
            "final_train_index_list": [],
            "final_validation_index_list": [],
        }

        print("***WARNING***: TF1.x only support label with 1 variable.")
        self.prepare_kfold()

    def prepare_kfold(self):
        """ 
        prepare the kfold dataset split 
        """

        if not self.load_status():
            self.init_kfold()
            self.save_status()

    def any_left_fold(self):
        if self.kfold_info["current_fold"] < self.kfold_info["total_folds"]:
            return True
        else:
            self.kfold_info["current_fold"] = 0
            self.save_status()
            return False

    def plot_all_folds(self):
        """ 
        plot all the folds in one figure 
        """

        for i0 in range(0, self.kfold_info["total_folds"]):
            plt.plot(self.kfold_info["train_index_list"][i0], np.ones(len(self.kfold_info["train_index_list"][i0])) * (i0 + 1), 'k.')
            plt.plot(self.kfold_info["validation_index_list"][i0], np.ones(len(self.kfold_info["validation_index_list"][i0])) * (i0 + 1), 'r.')

        # final data
        plt.plot(self.kfold_info["final_train_index_list"], np.ones(len(self.kfold_info["final_train_index_list"])) * (self.kfold_info["total_folds"] + 1), 'k.')
        plt.plot(self.kfold_info["final_validation_index_list"], np.ones(len(self.kfold_info["final_validation_index_list"])) * (self.kfold_info["total_folds"] + 1), 'r.')
        plt.plot(self.kfold_info["final_test_index_list"], np.ones(len(self.kfold_info["final_test_index_list"])) * (self.kfold_info["total_folds"] + 1), 'b.')
        plt.ylabel('folds')
        plt.xlabel('data points')
        plt.title('black: train, red: validation, blue: final test data')

        plt.show()

    def get_next_fold(self, dataset, labels, derivative=[], fold_id=-1, final_data=False):
        """ 
        get next fold data for training/validation 

        input: features, labels, fold_id
               fold_id is only used when a specific number is given

        output: current fold of features and labels

        """

        print("In the fold: ", self.kfold_info["current_fold"], " with ", "train size: ", len(self.kfold_info["train_index_list"][self.kfold_info["current_fold"]]),
              "validation size: ", len(self.kfold_info["validation_index_list"][self.kfold_info["current_fold"]]))

        if (fold_id >= 0 and fold_id < self.kfold_info["total_folds"]):
            train_index = self.kfold_info["train_index_list"][fold_id]
            validation_index = self.kfold_info["validation_index_list"][fold_id]
            test_index = validation_index
        else:
            train_index = self.kfold_info["train_index_list"][self.kfold_info["current_fold"]]
            validation_index = self.kfold_info["validation_index_list"][self.kfold_info["current_fold"]]
            test_index = validation_index

        if final_data:
            train_index = self.kfold_info["final_train_index_list"]
            validation_index = self.kfold_info["final_validation_index_list"]
            test_index = self.kfold_info["final_test_index_list"]

            # fix the messed up index [array([])]
            if (len(train_index) == 1):
                train_index = [x for x in train_index[0]]
                validation_index = [x for x in validation_index[0]]
                test_index = [x for x in test_index[0]]
            # print('train_index:', train_index)

        if (tf.__version__[0:1] == '1'):
            # has to convert
            np_data = np.array([1, 2])
            # print(len(dataset), dataset.shape)
            # print(dataset[1:5,0:].shape)

            # print("TF1.x only support label with 1 variable.")
            if (type(dataset) == type(np_data)):
                # print(type(dataset[0]), type(np.float32(1.0)))
                # if feature only has one variables and is numpy array, use the following
                if type(dataset[0]) == type(np.float32(1.0)) or type(dataset[0]) == type(np.float64(1.0)):
                    train_dataset = dataset[train_index]
                    val_dataset = dataset[validation_index]
                    test_dataset = dataset[test_index]
                else:
                    train_dataset = dataset[train_index, 0:]
                    val_dataset = dataset[validation_index, 0:]
                    test_dataset = dataset[test_index, 0:]

                train_labels = labels[train_index]
                val_labels = labels[validation_index]
                test_labels = labels[test_index]
                # dataset = tf.convert_to_tensor(dataset, dtype=tf.float32) # for tensorflow 1.14
                # labels  = tf.convert_to_tensor(labels, dtype=tf.float32)
                # # tensor use tf.gather(index)
                # # else:
                # # pandas frame will use take(index)
            else:
                try:
                    train_dataset = dataset.take(train_index)
                    train_labels = labels.take(train_index)
                    val_dataset = dataset.take(validation_index)
                    val_labels = labels.take(validation_index)
                    test_dataset = dataset.take(test_index)    # pandas framedata, select the index
                    test_labels = labels.take(test_index)
                    # print("---", train_dataset)
                except:
                    ## but not for tensor
                    train_dataset = tf.gather(dataset, train_index)    ### hopefully, it will work for tensor
                    # print(train_dataset)
                    train_labels = tf.gather(labels, train_index)
                    val_dataset = tf.gather(dataset, validation_index)
                    val_labels = tf.gather(labels, validation_index)
                    test_dataset = tf.gather(dataset, test_index)    # tensor
                    test_labels = tf.gather(labels, test_index)
                    pass
        elif (tf.__version__[0:1] == '2'):
            np_data = np.array([1, 2])
            # print(type(dataset), type(labels))
            if (type(dataset) == type(np_data)):
                dataset = tf.convert_to_tensor(dataset, dtype=tf.float32)
                # print(type(dataset))
            if (type(labels) == type(np_data)):
                labels = tf.convert_to_tensor(labels, dtype=tf.float32)
                # print(type(labels))
            # print('-----------------------')

            try:
                train_dataset = dataset.take(train_index)    # pandas for take
                train_labels = labels.take(train_index)
                val_dataset = dataset.take(validation_index)
                val_labels = labels.take(validation_index)
                test_dataset = dataset.take(test_index)    # pandas framedata, select the index
                test_labels = labels.take(test_index)
            except:
                train_dataset = tf.gather(dataset, train_index)    # tensor use gather
                train_labels = tf.gather(labels, train_index)
                val_dataset = tf.gather(dataset, validation_index)
                val_labels = tf.gather(labels, validation_index)
                test_dataset = tf.gather(dataset, test_index)    # tensor
                test_labels = tf.gather(labels, test_index)
                pass

        if (len(derivative) > 0):
            self.train_derivative = derivative[train_index]
            self.val_derivative = derivative[validation_index]
            self.test_derivative = derivative[test_index]
        else:
            self.train_derivative = []
            self.val_derivative = []
            self.test_derivative = []

        #---- the following is to fix tf.13. etc for residual dataset after one model. Code will crash if this line of code is not here.
        tf.keras.backend.clear_session()
        return train_dataset, train_labels, val_dataset, val_labels, test_dataset, test_labels, self.test_derivative

    def get_current_fold_derivative_data(self):
        return self.train_derivative, self.val_derivative, self.test_derivative

    def update_kfold_status(self):
        """ 
        wait until the training is done before save the status 
        """

        self.kfold_info["current_fold"] += 1
        self.save_status()

    def save_status(self):
        """ 
        save the k_fold split information such that it will not change index of each fold after restart 
        """

        pickle_out = open(self.kfold_filename, "wb")
        pickle.dump(self.kfold_info, pickle_out)
        pickle_out.close()

    def load_status(self):
        """ 
        load previously saved k_fold split information for consistency 
        """

        if (path.exists(self.kfold_filename)):
            self.kfold_info = pickle.load(open(self.kfold_filename, "rb"))
            print("successfully load saved kfold status files!")
            return True
        else:
            return False

    def init_kfold(self):
        """ 
        initialize kfold split ratio, etc. 
        """

        print("init the kfold with n_splits = ", self.kfold_info["total_folds"], " size_of_dataset = ", self.kfold_info["size_of_data"], " current_fold = ",
              self.kfold_info["current_fold"])

        # prepare the final testing data
        kf = KFold(n_splits=10, shuffle=True)    #, random_state = 1 # provide a positional number to fix the random state
        # index_list is just a dummy dataset for kf.split() to generate the index
        index_list = np.ones(self.kfold_info["size_of_data"])
        count = 0
        for train_index, validation_index in kf.split(index_list):
            training_data = train_index
            self.kfold_info["final_test_index_list"] = validation_index
            # print('in k-fold (test) : ', count)
            # print('  training_data: ', training_data)
            # print('final test data: ', self.kfold_info["final_test_index_list"])
            count += 1
            break

        # prepare the final validation data
        kf = KFold(n_splits=9, shuffle=True)    #, random_state = 1 # provide a positional number to fix the random state
        count = 0
        for train_index, validation_index in kf.split(training_data):
            self.kfold_info["final_train_index_list"] = training_data[train_index]
            self.kfold_info["final_validation_index_list"] = training_data[validation_index]

            # print('in k-fold (train): ', count)
            # print('final train data: ', self.kfold_info["final_train_index_list"])
            # print('final valid data: ', self.kfold_info["final_validation_index_list"])
            count += 1
            break

        kf = KFold(n_splits=self.kfold_info["total_folds"], shuffle=True)    #, random_state = 1 # provide a positional number to fix the random state
        # print('index_list:', index_list)
        count = 0
        for train_index, validation_index in kf.split(training_data):
            self.kfold_info["train_index_list"].append(training_data[train_index])
            self.kfold_info["validation_index_list"].append(training_data[validation_index])
            # print('in k-fold (train): ', count)
            # print('train data: ', self.kfold_info["train_index_list"])
            # print('valid data: ', self.kfold_info["validation_index_list"])
            count += 1

if __name__ == "__main__":
    print("... testing ... MLKFold class")

    size_of_data = 100
    total_folds = 5
    dummy_features = np.linspace(1, 100, size_of_data)
    dummy_labels = np.linspace(101, 200, size_of_data)
    #print(dummy_features, dummy_labels)

    # the point for this test is that the code will run smoothly for a 5-fold split
    print("test 1 is started!")
    test1 = MLKFold(total_folds, dummy_features)
    test1.plot_all_folds()
    a, b, c, d, e, f, g = test1.get_next_fold(dummy_features, dummy_labels, fold_id=1)
    print(a, c, e)
    a, b, c, d, e, f, g = test1.get_next_fold(dummy_features, dummy_labels, final_data=True)
    print(a, c, e)
    exit(0)
    while test1.any_left_fold():
        test1.get_next_fold(dummy_features, dummy_labels)
        test1.update_kfold_status()
    print("test 1 is completed!")

    # create a case where simulation crash in the middle with fold = 3
    print("test 2 is started!")
    test2 = MLKFold(total_folds, dummy_features)
    count = 0
    while test2.any_left_fold():
        count += 1
        if count > 3:
            break
        test2.get_next_fold(dummy_features, dummy_labels)
        test2.update_kfold_status()
    print("test 2 is completed!")

    # create a case where simulation successfully restored and finish this fold
    print("test 3 is started!")
    test3 = MLKFold(total_folds, dummy_features)
    while test3.any_left_fold():
        test3.get_next_fold(dummy_features, dummy_labels)
        test3.update_kfold_status()
    print("test 3 is completed!")

    # create a case where simulation run 2-fold
    print("test 4 is started!")
    test4 = MLKFold(total_folds, dummy_features)
    count = 0
    while test4.any_left_fold():
        count += 1
        if count > 2:
            break
        test4.get_next_fold(dummy_features, dummy_labels)
        test4.update_kfold_status()
    print("test 4 is completed!")

    # create a case where simulation restore and run 2-fold more
    print("test 5 is started!")
    test5 = MLKFold(total_folds, dummy_features)
    count = 0
    while test5.any_left_fold():
        count += 1
        if count > 2:
            break
        test5.get_next_fold(dummy_features, dummy_labels)
        test5.update_kfold_status()
    print("test 5 is completed!")

    # create a case where simulation all simulations are finished
    print("test 6 is started!")
    test6 = MLKFold(total_folds, dummy_features)
    while test6.any_left_fold():
        a, b, c, d, e, f, g = test6.get_next_fold(dummy_features, dummy_labels)
        print('a: ', len(a), a)
        print('b: ', len(b), b)
        print('c: ', len(c), c)
        print('d: ', len(d), d)
        test6.update_kfold_status()
    print("test 6 is completed!")

    print("test 7 is started!")
    test7 = MLKFold(total_folds, dummy_features)
    while test7.any_left_fold():
        test7.get_next_fold(dummy_features, dummy_labels)
        test7.update_kfold_status()
    print("test 7 is completed!")


====================================================================================================
mechanoChemML\src\LeastR.py
====================================================================================================
"""
Zhenlin Wang 2019
"""


from sklearn import linear_model
from sklearn.linear_model import RidgeCV
import numpy as np

def fit(theta_matrix,X_matrix, n_jobs=1):
    reg = linear_model.LinearRegression(fit_intercept=False,n_jobs=n_jobs)
    reg.fit(theta_matrix,X_matrix)
    gamma_vector=reg.coef_
    loss=np.mean(np.square(reg.predict(theta_matrix)-X_matrix))
    
    return gamma_vector, loss

def fit_lasso(theta_matrix,X_matrix,alpha=0):
    reg =linear_model.Lasso(alpha=alpha, fit_intercept=False)
    reg.fit(theta_matrix,X_matrix)
    gamma_vector=reg.coef_
    loss=np.mean(np.square(reg.predict(theta_matrix)-X_matrix))
    
    return gamma_vector, loss

def fit_ridge(theta_matrix,X_matrix,alpha=0):
    reg =linear_model.Ridge(alpha=alpha, fit_intercept=False)
    reg.fit(theta_matrix,X_matrix)
    gamma_vector=reg.coef_
    loss=np.mean(np.square(reg.predict(theta_matrix)-X_matrix))
    score=reg.score(theta_matrix,X_matrix)
    return gamma_vector, loss, score
    
def fit_ridge_cv(theta_matrix,X_matrix,alpha=[0]):
    reg =RidgeCV(alphas=alpha, fit_intercept=False)
    reg.fit(theta_matrix,X_matrix)
    gamma_vector=reg.coef_
    loss=np.mean(np.square(reg.predict(theta_matrix)-X_matrix))   
    score=reg.score(theta_matrix,X_matrix)
    return gamma_vector, loss, score, reg.alpha_
    
def multiple_fit_ridge(theta_matrix_list,X_matrix_list,map_index_list,x0,alpha=0,method='powell', options={}):
  num_eq=len(map_index_list)
  
  def loss_(x):
    fun=0
    for i in range(num_eq):
      fun+=np.mean((np.matmul(theta_matrix_list[i],x[map_index_list[i]])-X_matrix_list[i])**2)
    fun+=alpha*np.mean(x**2)
    return fun
    
  res = minimize(loss_, x0, method='powell',options={'xtol': 1e-8, 'disp': True})
  gamma_vector=res.x
  loss=res.fun
  return gamma_vector, loss
  

====================================================================================================
mechanoChemML\src\load_dump.py
====================================================================================================
#!/usr/bin/env python

# Import python modules
import os,sys,copy,warnings,itertools,inspect
import glob,json,jsonpickle,h5py,pickle,dill
import numpy as np
import pandas as pd


from natsort import natsorted, ns,index_natsorted,order_by_index

# Logging
import logging
log = 'info'
logger = logging.getLogger(__name__)
# logger.setLevel(getattr(logging,log.upper()))


# Split path into directory,file,ext
def path_split(path,directory=False,file=False,ext=False,directory_file=False,file_ext=False,ext_delimeter='.'):
	if not (directory or file or ext):
		return path
	returns = {'directory':directory,'file':file or directory_file or file_ext,'ext':ext}
	paths = {}
	paths['directory'] = os.path.dirname(path)
	paths['file'],paths['ext'] = os.path.splitext(path)
	if paths['ext'].startswith(ext_delimeter):
		paths['ext'] = ext_delimeter.join(paths['ext'].split(ext_delimeter)[1:])
	if not directory_file:
		paths['file'] = os.path.basename(paths['file'])
	if file_ext and paths['ext'].startswith(ext_delimeter):
		paths['file'] = ext_delimeter.join([paths['file'],paths['ext']])
	paths = [paths[k] for k in paths if returns[k]] 
	return paths if len(paths)>1 else paths[0]

# Join path by directories, with optional extension
def path_join(*paths,ext=None,abspath=False,ext_delimeter='.'):
	path = os.path.join(*paths)
	if ext is not None and not path.endswith('%s%s'%(ext_delimeter,ext)):
		path = ext_delimeter.join([path,ext])
	if abspath:
		path = os.path.abspath(path)
	return path


# glob path
def path_glob(path,**kwargs):
	return glob.glob(os.path.abspath(os.path.expanduser(path)),**kwargs)


# Class wrapper for functions
class funcclass(object):
	def __init__(self,func=lambda x:x):
		self.func = func
	def __call__(self,*args,**kwargs):
		return self.func(*args,**kwargs)

# Serialize object to JSON
def serialize(obj,key='py/object'):
	if callable(obj) or isinstance(obj,(slice,range)):
		if callable(obj) and not inspect.isclass(obj):            
			obj = funcclass(obj)
		obj = jsonpickle.encode(obj)
	elif isinstance(obj,np.ndarray):
		obj = obj.tolist()
	return obj

# Deserialize object from JSON
def deserialize(obj,key='py/object'):
	if isinstance(obj,dict) and key in obj:
		obj = pickle.loads(str(obj[key]))
	# return  jsonpickle.decode(str(obj))
	return obj

# Load data - General file import
def load(path,wr='r',default=None,verbose=False,**kwargs):
	loaders = {**{ext: (lambda obj,ext=ext,**kwargs:getattr(pd,'read_%s'%ext)(obj,**kwargs)) for ext in ['csv']},
			   **{ext: (lambda obj,ext=ext,**kwargs:getattr(pd,'read_%s'%ext)(obj,**kwargs) if wr=='r' else (pickle.load(obj,**kwargs))) for ext in ['pickle']},
			   **{ext: (lambda obj,ext=ext,**kwargs: json.load(obj,**{'object_hook':deserialize,**kwargs})) for ext in ['json']},
			  }
	if not isinstance(path,str):
		return default
	
	if path is None:
		return default

	ext = path.split('.')[-1]
	if ('.' in path) and (ext in loaders):
		paths = {ext: path}
	else:
		paths = {e: '%s.%s'%(path,e) for e in loaders}
	loaders = {paths[e]: loaders[e] for e in paths}
	for path in loaders:
		loader = loaders[path]
		for wr in [wr,'r','rb']:
			try:
				data = loader(path,**kwargs)
				logger.log(verbose,'Loading path %s'%(path))
				return data
			except Exception as e:
				try:
					with open(path,wr) as obj:
						data = loader(obj,**kwargs)
						logger.log(verbose,'Loading obj %s'%(path))
						return data
				except:
					pass

	return default			
		
# Dump data - General file save/export
def dump(data,path,wr='w',verbose=False,**kwargs):

	dumpers = {**{ext: (lambda data,obj,ext=ext,**kwargs:getattr(data,'to_%s'%ext)(obj,**{'index':False,**kwargs})) for ext in ['csv']},
			   **{ext: (lambda data,obj,ext=ext,**kwargs:getattr(data,'to_%s'%ext)(obj,**kwargs) if isinstance(data,pd.DataFrame) else pickle.dump(data,obj,protocol=pickle.HIGHEST_PROTOCOL,**kwargs)) for ext in ['pickle']},
			   **{ext: (lambda data,obj,ext=ext,**kwargs: json.dump(data,obj,**{'default':serialize,'ensure_ascii':False,'indent':4,**kwargs})) for ext in ['json']},
			   **{ext: (lambda data,obj,ext=ext,**kwargs: obj.write(data,**kwargs)) for ext in ['tex']},
			  }

	if path is None:
		return
	ext = path.split('.')[-1]
	if ('.' in path) and (ext in dumpers):
		paths = {ext: path}
	else:
		paths = {e: '%s.%s'%(path,e) for e in dumpers}
		return
	dumpers = {paths[e]: dumpers[e] for e in paths}

	for path in dumpers:
		dirname = os.path.abspath(os.path.dirname(path))
		if not os.path.exists(dirname):
			os.makedirs(dirname)

	for path in dumpers:
		dumper = dumpers[path]
		for _wr in [wr,'w','wb']:		
			with open(path,_wr) as obj:
				try:
					dumper(data,path,**kwargs)
					logger.log(verbose,'Dumping path %s'%(path))
					return
				except Exception as e:
					try:
						dumper(data,obj,**kwargs)
						logger.log(verbose,'Dumping obj %s'%(path))
						return
					except Exception as e:
						try:
							# dumper(pickleable(copy.deepcopy(data),_return=True),path,**kwargs)
							dumper(data,path,**kwargs)
						except:	
							try:					
								# dumper(pickleable(copy.deepcopy(data),_return=True),obj,**kwargs)
								dumper(data,obj,**kwargs)
							except:
								pass

	return			



# Check if object can be written to file
# Check if object can be pickled
def pickleable(obj,path=None,_return=False):
	if isinstance(obj,dict):
		pickleables = {k: pickleable(obj[k],path,_return=False) for k in obj} 
		for k in pickleables:
			if not pickleables[k]:
				obj.pop(k);
				pickleables[k] = True
		if _return:
			return obj
		else:
			return all([pickleables[k] for k in pickleables])
	ispickleable = False
	if path is None:
		path  = '__tmp__.__tmp__.%d'%(np.random.randint(1,int(1e8)))
	with open(path,'wb') as fobj:
		try:
			pickle.dump(obj,fobj)
			ispickleable = True
		except Exception as e:
			pass
	if os.path.exists(path):
		os.remove(path)
	return ispickleable

# Import Data as pandas dataframe
def importer(files,directory,wr='rb',verbose=False):

	paths = []
	for file in files:
		path = path_join(directory,file,abspath=True)
		paths.extend(path_glob(path,recursive=True))
	paths = natsorted(paths)
	data = [load(path,wr=wr,verbose=verbose) for path in paths]
	if len(data) > 0:
		df = pd.concat(data,axis=0,ignore_index=True)
		return df
	else:
		return None


# Sort Data
def sorter(seq,index,multiple=False,wrapper=lambda arr:arr):
	index = index_natsorted(index)
	if multiple:
		return [wrapper(order_by_index(s,index)) for s in seq]
	else:
		return wrapper(order_by_index(seq,index))


# Flattening multi-dimensional data
def _flatten(df,exceptions=[]):
	if df is None:
		return
	for label in df:
		if label in exceptions:
			continue
		data = np.array([i for i in df[label].values]) #.astype('float64')
		if data.ndim<=1:
			continue

		shape = data.shape
		labels = itertools.product(*[range(i) for i in shape[1:]])
		for _label in labels:
			d = data.reshape(*shape[1:],shape[0])
			for i in _label:
				d = d[i]
			_label = '_'.join([label,*[str(i) for i in _label]])
			df[_label] = d
		df.drop(columns=label,inplace=True)
	return	

# Setup Data - Global function to call specific setup functions
def setup(data,metadata,files,directories__load,directories__dump=None,metafile=None,wr='rb',flatten_exceptions=[],verbose=False,**kwargs):
	_setups = _setup()
	_setup_ = _setups['default']
	
	# Get loader depending on extension of files
	for exts in _setups:
		if any([path_split(file,ext=True) == exts for file in files]):
			_setup_ = _setups[exts]
			break

	# Get regexed directories
	if directories__dump is None:
		directories__dump = directories__load.copy()
	_directories(directories__load,directories__dump)

	# Load Data
	_setup_(data,metadata,files,directories__load,metafile,wr,flatten_exceptions,verbose,**kwargs)


	for key in data:
		directory_load = metadata[key].pop('directory')
		for dir_load,dir_dump in zip(directories__load,directories__dump):
			if dir_load in directory_load and len(dir_load)>=len(directory_load):
				directory_dump = dir_dump
				break
		metadata[key]['directory'] = {'load':directory_load,'dump':directory_dump}
		metadata[key]['type'] = 'imported'

	return


def _directories(directories__load,directories__dump):

	def replace(strings,patterns,threshold=None,delimeter=''):
		if threshold is None:
			threshold = len(patterns)
		matches = []
		nonmatches = []
		for pattern in patterns:
			if patttern in strings:
				matches.append(pattern)
			else:
				nonmatches.append(pattern)
		if ((isinstance(threshold,(int,np.integer)) and (len(matches)>=threshold)) or (callable(threshold) and threshold(matches))):
			for i,(substring,pattern) in ennumerate(zip(strings,nonmatches)):
				strings[i] = patterns

		string = strings.join(delimeter)
		return string




	# Glob directories patterns
	# Ensure directories exist

	for directories in 	[directories__load,directories__dump]:
		directories_split = [directory.split('*') for directory in directories]
		directories_glob = natsorted([d for directory in directories 
								for d in path_glob(directory)])		
		if len(directories_glob) <= len(directories):
			for directory in directories:
				if not os.path.exists(directory):
					os.makedirs(directory)

		directories_glob = natsorted([d for directory in directories 
						for d in path_glob(directory)])		

		directories.clear()
		directories.extend(directories_glob)
	
	if len(directories__dump) < len(directories__load):
		directories__dump.clear()
		directories__dump.extend([directory for directory in directories__load])

	for directories in [directories__load,directories__dump]:
		for directory in directories:
			if not os.path.exists(directory):
				os.makedirs(directory)

	return


def _setup():	

	# Default CSV/Pickle file input

	def _default(data,metadata,files,directories,metafile=None,wr='rb',flatten_exceptions=[],verbose=False,**kwargs):
		
		for directory in directories:
			key = directory #os.path.basename(directory) 
			data[key] = importer(files,directory,wr=wr,verbose=verbose)
			if data[key] is None:
				data.pop(key)
				continue
			_flatten(data[key],flatten_exceptions)
			metadata[key] = {}
			if isinstance(metafile,str):
				metadata[key] = load(path_join(directory,metafile),default=metadata[key],wr=wr,verbose=verbose)

			metadata[key]['directory'] = directory
			logger.log(verbose,'Importing: %s %r'%(key,data[key].shape if data[key] is not None else None))

		return


	# csv file input
	def _csv(*args,**kwargs):
		return _default(*args,**kwargs)

	# csv file input
	def _pickle(*args,**kwargs):
		return _default(*args,**kwargs)

	
	# HDf5 file input
	def _h5(data,metadata,files,directories,metafile=None,wr='rb',flatten_exceptions=[],verbose=False,**kwargs):

		def zero_check(x,val=0):
			try:
				return x[~np.equal(x,val)][0]
			except IndexError:
				return val
		join = lambda *args,seperator = '/':seperator.join(["",*args])
		name = lambda state: int(state.split('_')[-1])
		
		for directory in directories:
			paths = []
			for file in files:
				path = path_join(directory,file,abspath=True)
				paths.extend(path_glob(path,recursive=True))
			paths = natsorted(paths)
			for path in paths:

				h = h5py.File(path,'r')

				keys = kwargs.get('keys',[{},{}])
				groups = kwargs.get('groups',[]) 
				states = [k for k in h.keys() if groups[0] in k]
				views = {}

				for s in states[:]:

					labels = {**{keys[0][key]:join(s,key) for key in keys[0]}, 
							  **{keys[1][key]:join(s,*groups[1:],key) 
							     for key in keys[1]}}

					s = name(s)
					views[s] = {}

					for label in labels:
						try:
							dataset = h[labels[label]]
							shape = dataset.shape
							ndim = dataset.ndim
							if ndim == 0:
								views[s][label] = float(dataset[...])
							elif ndim == 3:
								views[s][label] = zero_check(dataset[...])
							elif ndim == 4:
								for i in range(shape[0]):
									views[s]['%s_%d_%d'%(label,i,0)] = zero_check(dataset[i])
							elif ndim == 5:
								for i in range(shape[0]):
									for j in range(shape[1]):                    
										views[s]['%s_%d_%d'%(label,i,j)] = zero_check(dataset[i][j])                      
						except KeyError:
							pass

				key = directory

				df = pd.DataFrame.from_dict(views, orient='index')
				df.index.name = join(*groups).split('_')[0].replace('/','')
				df.reset_index(drop=False,inplace=True)
				df.to_pickle(path.replace('h5','pickle'))
				if data.get(key) is not None:
					data[key] = pd.concat([data[key],df],axis=0,ignore_index=True)
				else:
					data[key] = df.copy()


				if metadata.get(key) is None:
					metadata[key] = {}
					if isinstance(metafile,str):
						metadata[key] = load(path_join(directory,metafile),default=metadata[key],wr='rb',verbose=verbose)
					metadata[key]['directory'] = directory

				logger.log(verbose,'Importing: %s [%d]'%(key,len(data[key])))
		return



	def _mat(data,metadata,files,directories,metafile=None,wr='r',flatten_exceptions=[],verbose=False,**kwargs):

		loader = sp.io.loadmat

		for directory in directories:
			paths = []
			for file in files:
				path = path_join(directory,file,abspath=True)
				paths.extend(path_glob(path,recursive=True))
			paths = natsorted(paths)
			for path in paths:
				try: 
					_data = loader(path)
				except:
					return
				labels = [k for k in _data if ((not k.startswith('__')) and (not k.endswith('__')))]
				shapes = {k: np.array(_data[k].shape) for k in labels if all([s>1 for s in list(_data[k].shape)])}
				labels = list(shapes)
				dims = min([shapes[k].size for k in shapes])
				shape = [max([shapes[k][i] for k in shapes]) for i in range(dims)]

				keys = ['_'.join(str(j) for j in i) for i in itertools.product(*[range(i) for i in shape])]
				data.update({k:pd.DataFrame() for k in keys if k not in data})
				for key in keys:
					_shape = [int(i) for i in key.split('_')]
					for label in labels:
						d = _data[label]
						for i in _shape:
							d = d[i]
						d = np.squeeze(d.reshape(sorted(d.shape,reverse=True)))
						if d.ndim>1:
							d = [i for i in d]
						data[key][label] = d
						metadata[key] = {}
						if isinstance(metafile,str):
							metadata[key] = load(path_join(directory,metafile),metadata[key],wr='rb',verbose=verbose)
						metadata[key]['directory'] = directory
		return



	# xarray file input
	def _nc(data,metadata,files,directories,metafile=None,wr='r',flatten_exceptions=[],verbose=False,**kwargs):
		try:
			import xarray as xr
		except:
			return

		for directory in directories:
			paths = []
			for file in files:
				path = path_join(directory,file,abspath=True)
				paths.extend(path_glob(path,recursive=True))
			paths = natsorted(paths)
			for path in paths:

				df = xr.open_dataset(path).to_dataframe()

				df.reset_index(drop=False,inplace=True)
				df.rename(columns=lambda x:x.upper(),inplace=True)


				inputs = ['BURNUP',  'FUELTEMP', 'MODTEMP', 'MODDENS', 'BORON','BANK_POS',
				          'FUELTEMP_AVG', 'MODTEMP_AVG', 'MODDENS_AVG', 'BANK_POS_AVG']
				outputs = ['NXSF', 'XSF', 'XSRM', 'XSTR', 'XSS']
				groupby = ['INGROUP', 'OUTGROUP']
				label = 'SAMPLE'

				values = itertools.product(*[df[g].unique() for g in groupby])
				df_ = None
				for value in values:
				    _df = df.copy()
				    _df = _df[(_df[groupby]==value).all(axis=1)]
				    _df.rename(columns={y: '%s_%s'%(y,'_'.join([str(x) for x in value]))  for y in outputs},inplace=True)
				    if df_ is None:
				        df_ = _df.copy().drop(groupby,axis=1).reset_index(drop=True)
				    else:
				        df.reset_index()
				        df_ = pd.merge(df_,_df.drop(groupby,axis=1).reset_index(drop=True),on=[label,*inputs] if label in df_ else inputs,how='outer',copy=False).reset_index(drop=True)
				    df_ = df_.T.drop_duplicates().T

				df = df_


				if label in df and len(df[label].unique()) > 1:
					df_groupby = df.groupby(label)
					for key in df_groupby.groups:
						key = '%s_%s'%(path,key)
						data[key] = df_groupby.get_group(key).drop(label,axis=1).reset_index(drop=True)
						metadata[key]['directory'] = directory
				else:
					key = path
					data[key] = df.reset_index(drop=True)
					metadata[key]['directory'] = directory


		return

	locs = locals()

	funcs = {k[1:]:locs[k] for k in locs if callable(locs[k]) and k.startswith('_')}
	return funcs


====================================================================================================
mechanoChemML\src\nn_models.py
====================================================================================================
import os, sys
import numpy as np

import tensorflow as tf
import tensorflow_probability as tfp
import tensorflow.compat.v1 as tf1
import tensorflow.keras.backend as K

import mechanoChemML.src.pde_layers as pde_layers

"""Build NN models based on a list of layers provided from configuration file."""

def _build_one_layer(layer_dict, kl_divergence_function=None):
    """ 
    Return one keras layer based on the layer dictionary 

    Args:
        layer_dict (dict): a dictionary contains configurations of a Keras layer
        kl_divergence_function: scaled kl_divergence_function (default: None)

    Returns:
        A Keras layer

    Note:
        The following layers are supported:

        - BatchNormalization
        - Conv2D
        - Convolution2DFlipout
        - Convolution2DReparameterization
        - Dense
        - DenseFlipout
        - DenseReparameterization
        - Flatten
        - GaussianNoise
        - MaxPooling2D
        - PDERandom
        - Reshape
        - UpSampling2D
        - PDEZero

    """
    args = [] 
    tfkl = tf.keras.layers
    tfpl = tfp.layers
    tfd = tfp.distributions
    val_init = 0.1
    stddev_init = 0.2
    stddev_init = 0.1 # TF probability default value
    # remember to add the positional argument default value to the related function: add_layer_default_argument(layer_dict)
    if layer_dict['type'] == 'Convolution2DFlipout' :
        if 'padding' not in layer_dict: layer_dict['padding'] = 'valid'
        if 'activation' not in layer_dict: layer_dict['activation'] = None
        return tfpl.Convolution2DFlipout(
                filters=layer_dict['filters'], 
                kernel_size=layer_dict['kernel_size'], 
                activation=layer_dict['activation'], 
                kernel_divergence_fn=kl_divergence_function,
                bias_divergence_fn=kl_divergence_function,
                padding=layer_dict['padding'],
                kernel_posterior_fn=tfpl.default_mean_field_normal_fn(
                    loc_initializer=tf1.initializers.random_uniform(minval=-val_init, maxval=val_init), 
                    untransformed_scale_initializer=tf1.initializers.random_normal(mean=-3., stddev=stddev_init), 
                    ),
                bias_posterior_fn=tfpl.default_mean_field_normal_fn(
                    is_singular=True, # very important
                    loc_initializer=tf1.initializers.random_uniform(minval=-val_init, maxval=val_init),
                    untransformed_scale_initializer=tf1.initializers.random_normal(mean=-3., stddev=stddev_init), 
                    ),
                )
    elif layer_dict['type'] == 'Convolution2DReparameterization' :
        if 'padding' not in layer_dict: layer_dict['padding'] = 'valid'
        if 'activation' not in layer_dict: layer_dict['activation'] = None
        return tfpl.Convolution2DReparameterization(
                filters=layer_dict['filters'], 
                kernel_size=layer_dict['kernel_size'], 
                activation=layer_dict['activation'], 
                padding=layer_dict['padding'],
                kernel_divergence_fn=kl_divergence_function,
                bias_divergence_fn=kl_divergence_function,
                )
    elif layer_dict['type'] == 'Conv2D' :
        return tfkl.Conv2D(
                filters=layer_dict['filters'], 
                kernel_size=layer_dict['kernel_size'], 
                activation=layer_dict['activation'], 
                padding=layer_dict['padding'],
                )
    elif layer_dict['type'] == 'MaxPooling2D' :
        if 'padding' not in layer_dict: layer_dict['padding'] = 'valid'
        if 'strides' not in layer_dict: layer_dict['strides'] = None

        return tfkl.MaxPooling2D(
                pool_size=layer_dict['pool_size'], 
                padding=layer_dict['padding'],
                strides=layer_dict['strides'],
                )
    elif layer_dict['type'] == 'BatchNormalization' :
        return tfkl.BatchNormalization()
    elif layer_dict['type'] == 'GaussianNoise' :
        return tfkl.GaussianNoise(float(layer_dict['stddev']))
    elif layer_dict['type'] == 'GaussianDropout' :
        return tfkl.GaussianDropout(float(layer_dict['rate']))
    elif layer_dict['type'] == 'Flatten' :
        return tfkl.Flatten()
    elif layer_dict['type'] == 'DenseFlipout' :

        if 'activation' not in layer_dict: layer_dict['activation'] = None
        return tfpl.DenseFlipout(
                units=layer_dict['units'], 
                kernel_divergence_fn=kl_divergence_function,
                bias_divergence_fn=kl_divergence_function,
                activation=layer_dict['activation'], 
                kernel_posterior_fn=tfpl.default_mean_field_normal_fn(
                    loc_initializer=tf1.initializers.random_uniform(minval=-val_init, maxval=val_init),
                    untransformed_scale_initializer=tf1.initializers.random_normal(mean=-3., stddev=stddev_init), 
                    ),
                bias_posterior_fn=tfpl.default_mean_field_normal_fn(
                    is_singular=True,
                    loc_initializer=tf1.initializers.random_uniform(minval=-val_init, maxval=val_init),
                    untransformed_scale_initializer=tf1.initializers.random_normal(mean=-3., stddev=stddev_init), 
                    ),
                )
    elif layer_dict['type'] == 'DenseReparameterization' :
        if 'activation' not in layer_dict: layer_dict['activation'] = None
        return tfpl.DenseReparameterization(
                units=layer_dict['units'], 
                kernel_divergence_fn=kl_divergence_function,
                bias_divergence_fn=kl_divergence_function,
                activation=layer_dict['activation'], 
                )
    elif layer_dict['type'] == 'Dense' :
        if 'activation' not in layer_dict: layer_dict['activation'] = None
        return tfkl.Dense(
                units=layer_dict['units'], 
                activation=layer_dict['activation'], 
                )
    elif layer_dict['type'] == 'Reshape' :
        try:
            if layer_dict['input_shape'] != None:
                # print('reshape:', layer_dict['input_shape'])
                return tfkl.Reshape(target_shape=layer_dict['target_shape'], input_shape=layer_dict['input_shape'])
            else:
                return tfkl.Reshape(target_shape=layer_dict['target_shape'])
        except:
            return tfkl.Reshape(target_shape=layer_dict['target_shape'])
    elif layer_dict['type'] == 'UpSampling2D' :
        return tfkl.UpSampling2D(size=layer_dict['size'])
    elif layer_dict['type'] == 'PDERandom' :
        return pde_layers.LayerFillRandomNumber(name='input')
    elif layer_dict['type'] == 'PDEZero' :
        return pde_layers.LayerFillZeros(name='input')
    else:
        return ValueError ('The layer type = ' + layer_dict['type'] + ' is not coded yet! Please add it by yourself.')


def _is_digit(str0):
    """ 
    Check if a string is digit or not

    Args:
        str0 (str): a string

    Returns:
        bool: True if is digit, false if not.

    """
    return str0.isdigit()

def _is_tuple(str0):
    """ 
    Check if a string is tuple or not

    Args:
        str0 (str): a string

    Returns:
        bool: True if is digit, false if not.

    """
    if str0[0] == '(' and str0[-1] == ')':
        return True
    else:
        return False

def _is_list(str0):
    """ 
    Check if a string is list or not

    Args:
        str0 (str): a string

    Returns:
        bool: True if is digit, false if not.

    """
    if str0[0] == '[' and str0[-1] == ']':
        return True
    else:
        return False

def _form_parameter_value(str0):
    """ 
    Convert a string to the proper type (int, tuple, list, str)

    Args:
        str0 (str): a string

    Returns:
        Variable with the proper type (int, tuple, list, str)

    """
    if _is_digit(str0):
        return int(str0)
    elif _is_tuple(str0):
        return tuple(int(s) for s in str0.strip("()").split(","))
    elif _is_list(str0):
        return list(int(s) for s in str0.strip("[]").split(","))
    else : 
        return str0

def _form_NN_dict_from_str(str0):
    """
    Form a list with each item being a dictionary containing the layer configuration

    Args:
        str0 (str): a string
    
    Returns:
        list_of_layers_dict (dict): a list of layer dictionary

    Notes:
        - keys of the layer dictionary: 'type', 'activation', 'unit', 'padding', etc
        - the keys are different for different Keras layers. 
        - the keys are defined based on the argument name of each Keras layer

    """
    list_of_layers = [ x.strip() for x in str0.split(';') if x.strip()]
    list_of_layers_dict = []
    for s0 in list_of_layers:
        one_layer = {}
        list_of_parameters = [ x.strip() for x in s0.split('|') if x.strip()]
        # print(list_of_parameters)
        for p0 in list_of_parameters:
            _p0 = [ x.strip() for x in p0.split('=') if x.strip()]
            one_layer[_p0[0]] = _form_parameter_value(_p0[1])
        # print(one_layer)
        list_of_layers_dict.append(one_layer)
    return list_of_layers_dict

class BNN_user_weak_pde_general(tf.keras.Model):
    """ 
    User defined general weak-pde constrained BNNs. Automatically create a sequential BNN model based on the list of layers.

    Args:
        layers_str (str): a string contains all info of layers defining the NNs.
        NUM_TRAIN_EXAMPLES (int): scale factor for the kl-loss. See more explanation: https://www.tensorflow.org/probability/api_docs/python/tfp/layers/Convolution2DFlipout
        Sigma2 (float): initial value for the variance of residual. Used in the loss.
    """

    def __init__(self, layers_str, NUM_TRAIN_EXAMPLES, Sigma2=1.0e-4):
        super(BNN_user_weak_pde_general, self).__init__()
        isBNN = False
        if layers_str.find('Flipout') >= 0:
            isBNN = True

        self.list_of_layers_dict = _form_NN_dict_from_str(layers_str)

        self.NUM_TRAIN_EXAMPLES = NUM_TRAIN_EXAMPLES
        self.Sigma2 = tf.Variable(Sigma2, trainable=isBNN)

        tfd = tfp.distributions
        kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /  # pylint: disable=g-long-lambda
                                  tf.cast(self.NUM_TRAIN_EXAMPLES, dtype=tf.float32))

        # 'all_layers' prefix is needed for BNN warm start indexing
        # random 1st layer
        self.all_layers = [_build_one_layer(self.list_of_layers_dict[0])]
        for l0 in self.list_of_layers_dict[1:]:
            self.all_layers.append(_build_one_layer(l0, kl_divergence_function))

    def call(self, inputs, training=False):
        """ 
        Execute each layer: See https://www.tensorflow.org/api_docs/python/tf/keras/Model

        Args:
            inputs: a keras.Input object or list of keras.Input objects.
            training (bool): One can use it to specify a different behavior in training and inference. 
        """
        x = self.all_layers[0](inputs)
        for hl in self.all_layers[1:]:
            x = hl(x)
        return tf.concat([x, inputs], 3)

class BNN_user_weak_pde_general_heter(tf.keras.Model):
    """ 
    User defined general weak-pde constrained BNNs. Automatically create a sequential BNN model based on the list of layers.

    Heterogeneous inputs with [image, scalar]

    Args:
        layers_str (str): a string contains all info of layers defining the NNs.
        NUM_TRAIN_EXAMPLES (int): scale factor for the kl-loss. See more explanation: https://www.tensorflow.org/probability/api_docs/python/tfp/layers/Convolution2DFlipout
        Sigma2 (float): initial value for the variance of residual. Used in the loss.
    """

    def __init__(self, layers_str, NUM_TRAIN_EXAMPLES, Sigma2=1.0e-4):
        super(BNN_user_weak_pde_general_heter, self).__init__()
        isBNN = False
        if layers_str.find('Flipout') >= 0:
            isBNN = True

        self.list_of_layers_dict = _form_NN_dict_from_str(layers_str)

        self.NUM_TRAIN_EXAMPLES = NUM_TRAIN_EXAMPLES
        self.Sigma2 = tf.Variable(Sigma2, trainable=isBNN)

        tfd = tfp.distributions
        kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /  # pylint: disable=g-long-lambda
                                  tf.cast(self.NUM_TRAIN_EXAMPLES, dtype=tf.float32))

        def merge_two_tensor(a):
            return K.concatenate([a[0], a[1]], axis=1)

        # 'all_layers' prefix is needed for BNN warm start indexing
        # naming of part1, part2 should follow by order to avoid issue in warm start
        # random 1st layer
        self.all_layers_part1 = [_build_one_layer(self.list_of_layers_dict[0])]
        self.all_layers_part2 = [tf.keras.layers.Lambda(merge_two_tensor)]
        build_decoder = False
        for l0 in self.list_of_layers_dict[1:]:
            if l0['type'].find('Dense') >= 0 :
                build_decoder = True
            # the additional parameters will account for information of num_parameters/(num_parameters+dense unit)
            # it would not be a small fraction. And this will make sure that parameter information is well
            # blended into the whole NN structure.
            if build_decoder:
                self.all_layers_part2.append(_build_one_layer(l0, kl_divergence_function))
            else:
                self.all_layers_part1.append(_build_one_layer(l0, kl_divergence_function))
        # print('encoder:', self.all_layers_part1)
        # print('decoder:', self.all_layers_part2)
        # exit(0)
        self.pde_parameters = None

    def call(self, inputs, training=False):
        """ 
        Execute each layer: See https://www.tensorflow.org/api_docs/python/tf/keras/Model

        It is almost impossible to pass a scalar out without sacrificing the data format.
        New function call is defined to pass such data.

        Args:
            inputs: a keras.Input object or list of keras.Input objects.
            training (bool): One can use it to specify a different behavior in training and inference. 
        """
        x = self.all_layers_part1[0](inputs[0])
        for hl in self.all_layers_part1[1:]:
            x = hl(x)
        # combine parameters with dense layer
        y = self.all_layers_part2[0]([inputs[1], x])
        for hl in self.all_layers_part2[1:]:
            y = hl(y)
        self.pde_parameters = inputs[1]
        return tf.concat([y, inputs[0]], 3)

    def get_pde_parameters(self):
        """
        Pass scalar parameters from inputs to output
        """
        return self.pde_parameters

class BNN_user_general(tf.keras.Model):
    """ 
    User defined general BNNs. Automatically create a sequential BNN model based on the list of layers.

    Args:
        layers_str (str): a string contains all info of layers defining the NNs.
        NUM_TRAIN_EXAMPLES (int): scale factor for the kl-loss. See more explanation: https://www.tensorflow.org/probability/api_docs/python/tfp/layers/Convolution2DFlipout
    """

    def __init__(self, layers_str, NUM_TRAIN_EXAMPLES):
        super(BNN_user_general, self).__init__()
        self.list_of_layers_dict = _form_NN_dict_from_str(layers_str)

        self.NUM_TRAIN_EXAMPLES = NUM_TRAIN_EXAMPLES
        tfd = tfp.distributions
        kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /  # pylint: disable=g-long-lambda
                                  tf.cast(self.NUM_TRAIN_EXAMPLES, dtype=tf.float32))

        # random 1st layer
        self.all_layers = [_build_one_layer(self.list_of_layers_dict[0])]
        for l0 in self.list_of_layers_dict[1:]:
            self.all_layers.append(_build_one_layer(l0, kl_divergence_function))

    def call(self, inputs, training=False):
        """ 
        Execute each layer: See https://www.tensorflow.org/api_docs/python/tf/keras/Model

        Args:
            inputs: a keras.Input object or list of keras.Input objects.
            training (bool): One can use it to specify a different behavior in training and inference. 
        """

        x = self.all_layers[0](inputs)
        
        for hl in self.all_layers[1:]:
            x = hl(x)
        return x


class NN_user_general(tf.keras.Model):
    """ 
    User defined general NNs. Automatically create a sequential NN model based on the list of layers.

    Args:
        layers_str (str): a string contains all info of layers defining the NNs.
    """


    def __init__(self, layers_str):
        super(NN_user_general, self).__init__()
        self.list_of_layers_dict = _form_NN_dict_from_str(layers_str)

        # random 1st layer
        self.all_layers = [_build_one_layer(self.list_of_layers_dict[0])]
        for l0 in self.list_of_layers_dict[1:]:
            self.all_layers.append(_build_one_layer(l0))

    def call(self, inputs, training=False):
        """ 
        Execute each layer: See https://www.tensorflow.org/api_docs/python/tf/keras/Model

        Args:
            inputs: a keras.Input object or list of keras.Input objects.
            training (bool): One can use it to specify a different behavior in training and inference. 
        """
        x = self.all_layers[0](inputs)
        
        for hl in self.all_layers[1:]:
            x = hl(x)

        return x

def merge_two_tensor(a):
    return K.concatenate([a[0], a[1]], axis=1)

class BNN_user_weak_pde_general_dynamic(tf.keras.Model):
    """ 
    User defined general weak-pde constrained BNNs. Automatically create a sequential BNN model based on the list of layers.

    Args:
        layers_str (str): a string contains all info of layers defining the NNs.
        NUM_TRAIN_EXAMPLES (int): scale factor for the kl-loss. See more explanation: https://www.tensorflow.org/probability/api_docs/python/tfp/layers/Convolution2DFlipout
        Sigma2 (float): initial value for the variance of residual. Used in the loss.
    """

    def __init__(self, layers_str, NUM_TRAIN_EXAMPLES, Sigma2=1.0e-4):
        super(BNN_user_weak_pde_general_dynamic, self).__init__()
        isBNN = False
        if layers_str.find('Flipout') >= 0:
            isBNN = True

        self.list_of_layers_dict = _form_NN_dict_from_str(layers_str)

        self.NUM_TRAIN_EXAMPLES = NUM_TRAIN_EXAMPLES
        self.Sigma2 = tf.Variable(Sigma2, trainable=isBNN)

        tfd = tfp.distributions
        kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /  # pylint: disable=g-long-lambda
                                  tf.cast(self.NUM_TRAIN_EXAMPLES, dtype=tf.float32))

        # random 1st layer
        self.all_layers = [_build_one_layer(self.list_of_layers_dict[0])]
        ind0 = 1
        for l0 in self.list_of_layers_dict[1:]:
            self.all_layers.append(_build_one_layer(l0, kl_divergence_function))
            # print(l0)
            if l0['type'] == 'Flatten':
                self.flatten_index = ind0
            ind0 += 1

        # self.merge_layer = tf.keras.layers.Lambda(merge_two_tensor)

    def call(self, inputs, training=False):
        """ 
        Execute each layer: See https://www.tensorflow.org/api_docs/python/tf/keras/Model

        Args:
            inputs: a keras.Input object or list of keras.Input objects.
            training (bool): One can use it to specify a different behavior in training and inference. 
        """
        inp0 = inputs[0] # [Dirichlet, Neumann, Initial]
        inp1 = inputs[1] # [NN] batch_x_time

        x = self.all_layers[0](inp0)
        for hl in self.all_layers[1:self.flatten_index+1]:
            # print(hl)
            x = hl(x)
        # print('-----------------')
        # x = self.merge_layer([inp1, x])  #,
        for hl in self.all_layers[self.flatten_index+1:]:
            # print(hl)
            x = hl(x) # size of x is determined by the NN structure from config.ini file.

        # current_time is in the size of [batch, :, :, 1]
        current_time = tf.expand_dims(inp1,axis=1)
        current_time = tf.expand_dims(current_time,axis=1)
        current_time = tf.multiply(tf.ones_like(inp0[:,:,:,0:1]), current_time)

        # channels = outputs + 3*dof + 1
        return tf.concat([x, inp0, current_time], 3)


if __name__ == '__main__':

    """ example for setting up an encoder-decoder structure with deterministic layers """
    example_NN = """ 
    type=PDERandom;
    type=Conv2D | filters=8 | kernel_size=5 | activation=relu | padding=same;
    type=MaxPooling2D | pool_size=(2,2) | padding=same;
    type=Conv2D | filters=16 | kernel_size=5 | activation=relu | padding=same;
    type=MaxPooling2D | pool_size=(2,2) | padding=same;
    type=Flatten;
    type=Dense | units=64 | activation=relu;
    type=Dense | units=32 | activation=relu;
    type=Reshape | target_shape=[4,4,2];
    type=Conv2D | filters=8 | kernel_size=5 | activation=relu | padding=same;
    type=UpSampling2D | size=(2,2);
    type=Conv2D | filters=8 | kernel_size=5 | activation=relu | padding=same;
    type=Conv2D | filters=1 | kernel_size=5 | activation=relu | padding=same;
    """

    model = NN_user_general(example_NN)
    input_shape=(None, 16, 16, 1)
    model.build(input_shape) 
    model.summary()

    """ example for setting up an encoder-decoder structure with probabilistic layers """
    example_BNN = """ 
    type=PDERandom;
    type=Convolution2DFlipout | filters=8 | kernel_size=5 | activation=relu | padding=same;
    type=MaxPooling2D | pool_size=(2,2) | padding=same;
    type=Convolution2DFlipout | filters=16 | kernel_size=5 | activation=relu | padding=same;
    type=MaxPooling2D | pool_size=(2,2) | padding=same;
    type=Flatten;
    type=DenseFlipout | units=64 | activation=relu;
    type=DenseFlipout | units=32 | activation=relu;
    type=Reshape | target_shape=[4,4,2];
    type=Convolution2DFlipout | filters=8 | kernel_size=5 | activation=relu | padding=same;
    type=UpSampling2D | size=(2,2);
    type=Convolution2DFlipout | filters=8 | kernel_size=5 | activation=relu | padding=same;
    type=Convolution2DFlipout | filters=1 | kernel_size=5 | activation=relu | padding=same;
     """
    model = BNN_user_general(example_BNN, NUM_TRAIN_EXAMPLES=16)
    input_shape=(None, 16, 16, 1)
    model.build(input_shape) 
    model.summary()


====================================================================================================
mechanoChemML\src\pde_layers.py
====================================================================================================
import numpy as np
import tensorflow as tf

def GetElementResidualMask(data):
    """ 
    Create a mask [batch, node_height, node_width, 1] for data [batch, node_height, node_width, m] where only the actual residual region is 1, the remaining part is zero.

    args: 
        data (numpy array): [batch, node_height, node_width, m] (scalar/vector)
    
    return:
        numpy array: mask [batch, elem_height, elem_width, 1] (same padding is used.)
    """
    pflag = False
    if pflag: print('data', np.shape(data))
    if pflag: print('data', data[0,:,:,0])

    mask_original = tf.where( data < 0, tf.fill(tf.shape(data), 0.0), tf.fill(tf.shape(data), 1.0))
    if pflag: print('mask (original)', mask_original[0,:,:,0])
    n1 = np.array([[0, 0], [0, 1]])
    n1 = np.expand_dims(n1, axis=2)
    n1 = np.expand_dims(n1, axis=3)
    mask_shift = tf.nn.conv2d(mask_original[:,:,:,0:1], n1, [1,1,1,1], 'SAME')
    if pflag: print('mask (shift)', mask_shift[0,:,:,0])
    mask = tf.multiply(mask_original[:,:,:,0:1], mask_shift)
    if pflag: print('mask (final)', mask[0,:,:,0])
    return mask


def ComputeBoundaryMaskNodalData(data_input, dof, opt=1):
    """ 
    Create Dirichlet mask or Neumann mask based on the inputs, where only the boundary part is 0.0, margin and the body part is 1.0.

    args:
        data_input (numpy array): size of [batch, node_height, node_width, dof*2]
        dof (int): dof per node
        opt (int): Dirichlet Mask (opt=1), Neumann mask (opt=2)
    return:
        numpy array: boundary mask with size of [batch, node_height, node_width, dof]

    todo:
        make this function to work with (1S, 1V), 2S, 1V1S, 3S, 2V, etc.
    """
    if dof == 1 :
        pflag = False
        # data_input = tf.convert_to_tensor(data_input, dtype=tf.float32)
        if pflag: print('data_input', np.shape(data_input))
        #---------------- Dirichlet BCs-----------------
        dirichlet_data = data_input[:,:,:,0:1]
        if pflag: print('dirichlet_data', dirichlet_data[0,:,:,0])
        dirichlet_reverse_mask = tf.where( dirichlet_data < 0, tf.fill(tf.shape(dirichlet_data), 1.0), tf.fill(tf.shape(dirichlet_data), 0.0))
        if pflag: print('dirichlet_reverse_mask', dirichlet_reverse_mask[0,:,:,0])

        #---------------- Neumann BCs-----------------
        neumann_data = data_input[:,:,:,1:2]
        if pflag:  print('neumann_data', neumann_data[0,:,:,0])
        if pflag:  print('attention, NM should not be scaled ')
        neumann_reverse_mask = tf.where( neumann_data > 0.0, tf.fill(tf.shape(neumann_data), 0.0), tf.fill(tf.shape(neumann_data), 1.0))
        if pflag:  print('neumann_reverse_mask', neumann_reverse_mask[0,:,:,0])

        # bc_mask = tf.multiply(dirichlet_reverse_mask, neumann_reverse_mask)
        bc_mask = dirichlet_reverse_mask
        # re-index the y-axis to make sure the bcs look correct on the plot
        # bc_mask = tf.reverse(bc_mask, [1]) # check on 2020-07-14: seems wrong
        if opt == 1:
          return dirichlet_reverse_mask
        elif opt == 2:
          return neumann_reverse_mask 
    elif dof == 2 :
        """ 
        input: feature inputs with Dirichlet and Neumann BCs
        output: Dirichlet mask, and Neumann mask
        """
        pflag = False
        # data_input = tf.convert_to_tensor(data_input, dtype=tf.float32)
        if pflag: print('data_input', np.shape(data_input))
        #---------------- Dirichlet BCs-----------------
        dirichlet_x_data = data_input[:,:,:,0:1]
        dirichlet_y_data = data_input[:,:,:,1:2]
        if pflag: print('dirichlet_x_data', dirichlet_x_data[0,:,:,0])
        if pflag: print('dirichlet_y_data', dirichlet_y_data[0,:,:,0])
        dirichlet_x_reverse_mask = tf.where( dirichlet_x_data < 0, tf.fill(tf.shape(dirichlet_x_data), 1.0), tf.fill(tf.shape(dirichlet_x_data), 0.0))
        dirichlet_y_reverse_mask = tf.where( dirichlet_y_data < 0, tf.fill(tf.shape(dirichlet_y_data), 1.0), tf.fill(tf.shape(dirichlet_y_data), 0.0))
        if pflag: print('dirichlet_x_reverse_mask', dirichlet_x_reverse_mask[0,:,:,0])
        if pflag: print('dirichlet_y_reverse_mask', dirichlet_y_reverse_mask[0,:,:,0])

        #---------------- Neumann BCs-----------------
        neumann_x_data = data_input[:,:,:,2:3]
        neumann_y_data = data_input[:,:,:,3:4]
        if pflag: print('neumann_x_data', neumann_x_data[0,:,:,0])
        if pflag: print('neumann_y_data', neumann_y_data[0,:,:,0])
        if pflag: print('attention, NM should not be scaled ')
        neumann_x_reverse_mask = tf.where( neumann_x_data > 0.0, tf.fill(tf.shape(neumann_x_data), 0.0), tf.fill(tf.shape(neumann_x_data), 1.0))
        neumann_y_reverse_mask = tf.where( neumann_y_data > 0.0, tf.fill(tf.shape(neumann_y_data), 0.0), tf.fill(tf.shape(neumann_y_data), 1.0))
        if pflag: print('neumann_x_reverse_mask', neumann_x_reverse_mask[0,:,:,0])
        if pflag: print('neumann_y_reverse_mask', neumann_y_reverse_mask[0,:,:,0])
        dirichlet_reverse_mask = tf.concat([dirichlet_x_reverse_mask, dirichlet_y_reverse_mask], axis=3)
        neumann_reverse_mask = tf.concat([neumann_x_reverse_mask, neumann_y_reverse_mask], axis=3)
        # bc_mask = tf.multiply(dirichlet_reverse_mask, neumann_reverse_mask)
        bc_mask = dirichlet_reverse_mask
        # bc_mask = tf.reverse(bc_mask, [1])
        # if pflag: print('bc_mask_x :', np.shape(bc_mask), bc_mask[0,:,:,0])
        # if pflag: print('bc_mask_y :', np.shape(bc_mask), bc_mask[0,:,:,1])
        # return bc_mask # disabled on 2020-07-22

        if opt == 1:
          return dirichlet_reverse_mask
        elif opt == 2:
          return neumann_reverse_mask 
    elif dof > 2 :
        """ 
        input: feature inputs with Dirichlet and Neumann BCs
        output: Dirichlet mask, and Neumann mask
        """
        pflag = False
        # data_input = tf.convert_to_tensor(data_input, dtype=tf.float32)
        if pflag: print('data_input', np.shape(data_input))
        #---------------- Dirichlet BCs-----------------
        dirichlet_data = data_input[:,:,:,0:dof]
        dirichlet_reverse_mask = tf.where( dirichlet_data < 0, tf.fill(tf.shape(dirichlet_data), 1.0), tf.fill(tf.shape(dirichlet_data), 0.0))
        if pflag: 
            for d0 in range(0, dof):
                print(' dirichlet_reverse_mask ' + str(d0) + ':', dirichlet_reverse_mask[0,:,:,d0])

        #---------------- Neumann BCs-----------------
        neumann_data = data_input[:,:,:,dof:2*dof]
        neumann_reverse_mask = tf.where( neumann_data > 0.0, tf.fill(tf.shape(neumann_data), 0.0), tf.fill(tf.shape(neumann_data), 1.0))
        if pflag: 
            for d0 in range(0, dof):
                print(' neumann_reverse_mask ' + str(d0) + ':', neumann_reverse_mask[0,:,:,d0])

        if opt == 1:
          return dirichlet_reverse_mask
        elif opt == 2:
          return neumann_reverse_mask 

        raise ValueError("Please check dof = ", dof, " if it is implemented correct or not in pde_layers.ComputeBoundaryMaskNodalData()!")

def ComputeNeumannBoundaryResidualNodalData(data_input, dh, dof, padding='SAME'):
    """ 
    Compute the residual on the Neumann BCs.  The implementation is based on Neumann BCs is scaled between (-1, 1), and Neumann condition should be always > 0 in the domain region. Raise value error if negative value is detected

    args:
        data_input (numpy array): size of [batch, node_height, node_width, dof*2]
        dof (int): dof per node
        dh (float): element size
    return:
        numpy array: nodal Neumann residual value with size of [batch, node_height, node_width, dof]

    todo:
        make this function to work with (1S, 1V), 2S, 1V1S, 3S, 2V, etc.

        loop over each dof, instead of implementing different dof opt.
    """
    # pflag = True
    pflag = False
    if (dof > 4):
        raise ValueError(" dof = ", dof, " is not implemented! Only dof = 1 or 2 or 3 or 4 is coded!")

    data_input = tf.convert_to_tensor(data_input, dtype=tf.float32)
    if pflag: print('data_input', np.shape(data_input))

    # --------------- Dirichlet data------------
    dirichlet_data = data_input[:,:,:,0:dof]
    if pflag: print('dirichlet_data', dirichlet_data[0,:,:,0])

    #--------------- Domain mask ------------------
    # change actual dirichlet BCs to -2, the all -2 will be the domain
    domain_mask = tf.where( dirichlet_data >= 0.0, tf.fill(tf.shape(dirichlet_data), -2.0), dirichlet_data)
    # print('domain_mask', domain_mask[0,:,:,0])
    domain_mask = tf.where( domain_mask < -1.0, tf.fill(tf.shape(domain_mask), 1.0), tf.fill(tf.shape(domain_mask), 0.0))
    if pflag: print('domain_mask', domain_mask[0,:,:,0])

    #--------------- Neumann BCs ------------------
    Neumann_max = 1.0
    Neumann_min = -1.0
    neumann_data = data_input[:,:,:,dof:dof+dof]
    if pflag: print('neumann_data', neumann_data[0,:,:,0])
    if pflag: print('neumann_data(dof-1)', neumann_data[0,:,:,dof-1])

    # #--------------- check Neumann BCs------------ !!!! should be checked at the very beginning of data.
    # # is not allowed during tensor running/training 
    # check_neumann = tf.multiply(neumann_data, domain_mask)
    # if pflag: print('---check_neumann', check_neumann[0,:,:,0])
    # if (tf.reduce_sum(check_neumann) == 0) :
        # print("WARNING: no Neumann BCs is detected!")
    # check_neumann = tf.where( check_neumann < 0.0, tf.fill(tf.shape(check_neumann), 1.0), tf.fill(tf.shape(check_neumann), 0.0))
    # if pflag: print('check_neumann', check_neumann[0,:,:,0])
    # if (tf.reduce_sum(check_neumann) < 0) :
        # raise ValueError("Neumann BCs should NOT be smaller than zero (< 0). Consider use diffusivity or elastic constant to scale the data!")

    #---------------- Neumann BCs-----------------
    # Should consider the scaling as well.  Any mask will not work, as Neumann BCs can be smaller than 0.0
    # Solution: Neumann BCs is not allowed to be smaller than 0 in the input data.

    neumann_mask = tf.where( neumann_data <= 0.0, tf.fill(tf.shape(neumann_data), 0.0), tf.fill(tf.shape(neumann_data), 1.0))
    if pflag: print('neumann_mask', neumann_mask[0,:,:,0])
    if pflag: print('neumann_mask(dof-1)', neumann_mask[0,:,:,dof-1])
    neumann_data = tf.multiply( neumann_data, neumann_mask)
    if pflag: print('neumann_data', neumann_data[0,:,:,0])
    if pflag: print('neumann_data(dof-1)', neumann_data[0,:,:,dof-1])

    # The main idea is to form a element connection as for the bulk residual case.
    # However, if the neumman BCs is location dependent, has both positive and negative 
    # values, then, it is very challenging to make the following to work stably. 
    # On the other hand, if we only consider the bulk region, then the neumman BCs is
    # literally enforced through the residual form, but not explicitly. 
    # Still, the following might still work.

    # form dof-1 level horizontal element
    # the padding zeros will helps to keep the location of surface node unchanged for the bottom edges
    n1 = np.array([[1, 0], [0, 0]])
    n1 = np.expand_dims(n1, axis=2)
    n1 = np.expand_dims(n1, axis=3)
    n2 = np.array([[0, 1], [0, 0]])
    n2 = np.expand_dims(n2, axis=2)
    n2 = np.expand_dims(n2, axis=3)

    # form dof-1 level vertical element:
    n3 = np.array([[1, 0], [0, 0]])
    n3 = np.expand_dims(n3, axis=2)
    n3 = np.expand_dims(n3, axis=3)
    n4 = np.array([[0, 0], [1, 0]])
    n4 = np.expand_dims(n4, axis=2)
    n4 = np.expand_dims(n4, axis=3)


    # create surface elements
    if dof == 1:
        # horizontal element
        c_n1 = tf.nn.conv2d(neumann_data, n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data, n2, [1,1,1,1], padding)
        elem_y = tf.concat([c_n1, c_n2], 3)

        # vertical element
        c_n3 = tf.nn.conv2d(neumann_data, n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data, n4, [1,1,1,1], padding)
        elem_x = tf.concat([c_n3, c_n4], 3)

        if pflag: print('elem_x', np.shape(elem_x))
        if pflag: print('elem_x 1: ', elem_x[0,:,:,0])
        if pflag: print('elem_x 2: ', elem_x[0,:,:,1])

        if pflag: print('elem_y', np.shape(elem_y))
        if pflag: print('elem_y 1: ', elem_y[0,:,:,0])
        if pflag: print('elem_y 2: ', elem_y[0,:,:,1])
    elif dof == 2:
        neumann_data_1 = neumann_data[:,:,:,0:1]
        neumann_data_2 = neumann_data[:,:,:,1:2]

        c_n1 = tf.nn.conv2d(neumann_data_1, n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data_1, n2, [1,1,1,1], padding)
        elem_y_1 = tf.concat([c_n1, c_n2], 3)

        c_n1 = tf.nn.conv2d(neumann_data_2, n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data_2, n2, [1,1,1,1], padding)
        elem_y_2 = tf.concat([c_n1, c_n2], 3)

        c_n3 = tf.nn.conv2d(neumann_data_1, n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data_1, n4, [1,1,1,1], padding)
        elem_x_1 = tf.concat([c_n3, c_n4], 3)

        c_n3 = tf.nn.conv2d(neumann_data_2, n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data_2, n4, [1,1,1,1], padding)
        elem_x_2 = tf.concat([c_n3, c_n4], 3)

        if pflag: print('elem_y_1 ', np.shape(elem_y_1))
        if pflag: print('elem_y_1 1: ', elem_y_1[0,:,:,0])
        if pflag: print('elem_y_1 2: ', elem_y_1[0,:,:,1])
        if pflag: print('elem_y_2 ', np.shape(elem_y_2))
        if pflag: print('elem_y_2 1: ', elem_y_2[0,:,:,0])
        if pflag: print('elem_y_2 2: ', elem_y_2[0,:,:,1])

        if pflag: print('elem_x_1 ', np.shape(elem_x_1))
        if pflag: print('elem_x_1 1: ', elem_x_1[0,:,:,0])
        if pflag: print('elem_x_1 2: ', elem_x_1[0,:,:,1])
        if pflag: print('elem_x_2 ', np.shape(elem_x_2))
        if pflag: print('elem_x_2 1: ', elem_x_2[0,:,:,0])
        if pflag: print('elem_x_2 2: ', elem_x_2[0,:,:,1])
    elif dof == 3:
        neumann_data_1 = neumann_data[:,:,:,0:1]
        neumann_data_2 = neumann_data[:,:,:,1:2]
        neumann_data_3 = neumann_data[:,:,:,2:3]

        c_n1 = tf.nn.conv2d(neumann_data_1, n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data_1, n2, [1,1,1,1], padding)
        elem_y_1 = tf.concat([c_n1, c_n2], 3)

        c_n1 = tf.nn.conv2d(neumann_data_2, n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data_2, n2, [1,1,1,1], padding)
        elem_y_2 = tf.concat([c_n1, c_n2], 3)

        c_n1 = tf.nn.conv2d(neumann_data_3, n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data_3, n2, [1,1,1,1], padding)
        elem_y_3 = tf.concat([c_n1, c_n2], 3)

        c_n3 = tf.nn.conv2d(neumann_data_1, n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data_1, n4, [1,1,1,1], padding)
        elem_x_1 = tf.concat([c_n3, c_n4], 3)

        c_n3 = tf.nn.conv2d(neumann_data_2, n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data_2, n4, [1,1,1,1], padding)
        elem_x_2 = tf.concat([c_n3, c_n4], 3)

        c_n3 = tf.nn.conv2d(neumann_data_3, n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data_3, n4, [1,1,1,1], padding)
        elem_x_3 = tf.concat([c_n3, c_n4], 3)

        if pflag: print('elem_y_1 ', np.shape(elem_y_1))
        if pflag: print('elem_y_1 1: ', elem_y_1[0,:,:,0])
        if pflag: print('elem_y_1 2: ', elem_y_1[0,:,:,1])
        if pflag: print('elem_y_2 ', np.shape(elem_y_2))
        if pflag: print('elem_y_2 1: ', elem_y_2[0,:,:,0])
        if pflag: print('elem_y_2 2: ', elem_y_2[0,:,:,1])
        if pflag: print('elem_y_3 ', np.shape(elem_y_3))
        if pflag: print('elem_y_3 1: ', elem_y_3[0,:,:,0])
        if pflag: print('elem_y_3 2: ', elem_y_3[0,:,:,1])

        if pflag: print('elem_x_1 ', np.shape(elem_x_1))
        if pflag: print('elem_x_1 1: ', elem_x_1[0,:,:,0])
        if pflag: print('elem_x_1 2: ', elem_x_1[0,:,:,1])
        if pflag: print('elem_x_2 ', np.shape(elem_x_2))
        if pflag: print('elem_x_2 1: ', elem_x_2[0,:,:,0])
        if pflag: print('elem_x_2 2: ', elem_x_2[0,:,:,1])
        if pflag: print('elem_x_3 ', np.shape(elem_x_3))
        if pflag: print('elem_x_3 1: ', elem_x_3[0,:,:,0])
        if pflag: print('elem_x_3 2: ', elem_x_3[0,:,:,1])

    elif dof == 4:
        neumann_data_1 = neumann_data[:,:,:,0:1]
        neumann_data_2 = neumann_data[:,:,:,1:2]
        neumann_data_3 = neumann_data[:,:,:,2:3]
        neumann_data_4 = neumann_data[:,:,:,2:3]

        c_n1 = tf.nn.conv2d(neumann_data_1, n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data_1, n2, [1,1,1,1], padding)
        elem_y_1 = tf.concat([c_n1, c_n2], 3)

        c_n1 = tf.nn.conv2d(neumann_data_2, n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data_2, n2, [1,1,1,1], padding)
        elem_y_2 = tf.concat([c_n1, c_n2], 3)

        c_n1 = tf.nn.conv2d(neumann_data_3, n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data_3, n2, [1,1,1,1], padding)
        elem_y_3 = tf.concat([c_n1, c_n2], 3)

        c_n1 = tf.nn.conv2d(neumann_data_4, n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data_4, n2, [1,1,1,1], padding)
        elem_y_4 = tf.concat([c_n1, c_n2], 3)

        c_n3 = tf.nn.conv2d(neumann_data_1, n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data_1, n4, [1,1,1,1], padding)
        elem_x_1 = tf.concat([c_n3, c_n4], 3)

        c_n3 = tf.nn.conv2d(neumann_data_2, n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data_2, n4, [1,1,1,1], padding)
        elem_x_2 = tf.concat([c_n3, c_n4], 3)

        c_n3 = tf.nn.conv2d(neumann_data_3, n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data_3, n4, [1,1,1,1], padding)
        elem_x_3 = tf.concat([c_n3, c_n4], 3)

        c_n3 = tf.nn.conv2d(neumann_data_4, n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data_4, n4, [1,1,1,1], padding)
        elem_x_4 = tf.concat([c_n3, c_n4], 3)



    if dof == 1:
        # create a mask to delete data that are not properly aligned
        c_n1_mask = tf.nn.conv2d(neumann_mask, n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask, n2, [1,1,1,1], padding)
        elem_y_mask = tf.multiply(c_n1_mask, c_n2_mask)
        if pflag: print('c_n1_mask: ', c_n1_mask[0,:,:,0])
        if pflag: print('c_n2_mask: ', c_n2_mask[0,:,:,0])
        if pflag: print('elem_y_mask: ', elem_y_mask[0,:,:,0])
    
        # create a mask to delete data that are not properly aligned
        c_n3_mask = tf.nn.conv2d(neumann_mask, n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask, n4, [1,1,1,1], padding)
        elem_x_mask = tf.multiply(c_n3_mask, c_n4_mask)
        if pflag: print('c_n3_mask: ', c_n3_mask[0,:,:,0])
        if pflag: print('c_n4_mask: ', c_n4_mask[0,:,:,0])
        if pflag: print('elem_x_mask: ', elem_x_mask[0,:,:,0])
        # exit(0)
    elif dof == 2:
        # For the 3D case, it would be impossible to perform task like this.
        # Thus, how to come up with a 3D implementation, or sparse pattern 
        # would be extremely useful.

        # create a mask to delete data that are not properly aligned
        neumann_mask_1 = neumann_mask[:,:,:,0:1]
        neumann_mask_2 = neumann_mask[:,:,:,1:2]

        c_n1_mask = tf.nn.conv2d(neumann_mask_1, n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask_1, n2, [1,1,1,1], padding)
        elem_y_mask_1 = tf.multiply(c_n1_mask, c_n2_mask)

        if pflag: print('c_n1_mask: ', c_n1_mask[0,:,:,0])
        if pflag: print('c_n2_mask: ', c_n2_mask[0,:,:,0])
        if pflag: print('elem_y_mask_1: ', elem_y_mask_1[0,:,:,0])

        c_n1_mask = tf.nn.conv2d(neumann_mask_2, n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask_2, n2, [1,1,1,1], padding)
        elem_y_mask_2 = tf.multiply(c_n1_mask, c_n2_mask)

        if pflag: print('c_n1_mask: ', c_n1_mask[0,:,:,0])
        if pflag: print('c_n2_mask: ', c_n2_mask[0,:,:,0])
        if pflag: print('elem_y_mask_2: ', elem_y_mask_2[0,:,:,0])

    
        # create a mask to delete data that are not properly aligned
        c_n3_mask = tf.nn.conv2d(neumann_mask_1, n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask_1, n4, [1,1,1,1], padding)
        elem_x_mask_1 = tf.multiply(c_n3_mask, c_n4_mask)
        if pflag: print('c_n3_mask: ', c_n3_mask[0,:,:,0])
        if pflag: print('c_n4_mask: ', c_n4_mask[0,:,:,0])
        if pflag: print('elem_x_mask_1: ', elem_x_mask_1[0,:,:,0])

        c_n3_mask = tf.nn.conv2d(neumann_mask_2, n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask_2, n4, [1,1,1,1], padding)
        elem_x_mask_2 = tf.multiply(c_n3_mask, c_n4_mask)
        if pflag: print('c_n3_mask: ', c_n3_mask[0,:,:,0])
        if pflag: print('c_n4_mask: ', c_n4_mask[0,:,:,0])
        if pflag: print('elem_x_mask_2: ', elem_x_mask_2[0,:,:,0])

    elif dof == 3:

        # create a mask to delete data that are not properly aligned
        neumann_mask_1 = neumann_mask[:,:,:,0:1]
        neumann_mask_2 = neumann_mask[:,:,:,1:2]
        neumann_mask_3 = neumann_mask[:,:,:,2:3]

        c_n1_mask = tf.nn.conv2d(neumann_mask_1, n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask_1, n2, [1,1,1,1], padding)
        elem_y_mask_1 = tf.multiply(c_n1_mask, c_n2_mask)

        if pflag: print('c_n1_mask: ', c_n1_mask[0,:,:,0])
        if pflag: print('c_n2_mask: ', c_n2_mask[0,:,:,0])
        if pflag: print('elem_y_mask_1: ', elem_y_mask_1[0,:,:,0])

        c_n1_mask = tf.nn.conv2d(neumann_mask_2, n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask_2, n2, [1,1,1,1], padding)
        elem_y_mask_2 = tf.multiply(c_n1_mask, c_n2_mask)

        if pflag: print('c_n1_mask: ', c_n1_mask[0,:,:,0])
        if pflag: print('c_n2_mask: ', c_n2_mask[0,:,:,0])
        if pflag: print('elem_y_mask_2: ', elem_y_mask_2[0,:,:,0])

        c_n1_mask = tf.nn.conv2d(neumann_mask_3, n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask_3, n2, [1,1,1,1], padding)
        elem_y_mask_3 = tf.multiply(c_n1_mask, c_n2_mask)

        if pflag: print('c_n1_mask: ', c_n1_mask[0,:,:,0])
        if pflag: print('c_n2_mask: ', c_n2_mask[0,:,:,0])
        if pflag: print('elem_y_mask_3: ', elem_y_mask_3[0,:,:,0])

    
        # create a mask to delete data that are not properly aligned
        c_n3_mask = tf.nn.conv2d(neumann_mask_1, n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask_1, n4, [1,1,1,1], padding)
        elem_x_mask_1 = tf.multiply(c_n3_mask, c_n4_mask)
        if pflag: print('c_n3_mask: ', c_n3_mask[0,:,:,0])
        if pflag: print('c_n4_mask: ', c_n4_mask[0,:,:,0])
        if pflag: print('elem_x_mask_1: ', elem_x_mask_1[0,:,:,0])

        c_n3_mask = tf.nn.conv2d(neumann_mask_2, n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask_2, n4, [1,1,1,1], padding)
        elem_x_mask_2 = tf.multiply(c_n3_mask, c_n4_mask)
        if pflag: print('c_n3_mask: ', c_n3_mask[0,:,:,0])
        if pflag: print('c_n4_mask: ', c_n4_mask[0,:,:,0])
        if pflag: print('elem_x_mask_2: ', elem_x_mask_2[0,:,:,0])

        c_n3_mask = tf.nn.conv2d(neumann_mask_3, n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask_3, n4, [1,1,1,1], padding)
        elem_x_mask_3 = tf.multiply(c_n3_mask, c_n4_mask)
        if pflag: print('c_n3_mask: ', c_n3_mask[0,:,:,0])
        if pflag: print('c_n4_mask: ', c_n4_mask[0,:,:,0])
        if pflag: print('elem_x_mask_3: ', elem_x_mask_3[0,:,:,0])

    elif dof == 4:

        # create a mask to delete data that are not properly aligned
        neumann_mask_1 = neumann_mask[:,:,:,0:1]
        neumann_mask_2 = neumann_mask[:,:,:,1:2]
        neumann_mask_3 = neumann_mask[:,:,:,2:3]
        neumann_mask_4 = neumann_mask[:,:,:,2:3]

        c_n1_mask = tf.nn.conv2d(neumann_mask_1, n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask_1, n2, [1,1,1,1], padding)
        elem_y_mask_1 = tf.multiply(c_n1_mask, c_n2_mask)

        c_n1_mask = tf.nn.conv2d(neumann_mask_2, n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask_2, n2, [1,1,1,1], padding)
        elem_y_mask_2 = tf.multiply(c_n1_mask, c_n2_mask)

        c_n1_mask = tf.nn.conv2d(neumann_mask_3, n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask_3, n2, [1,1,1,1], padding)
        elem_y_mask_3 = tf.multiply(c_n1_mask, c_n2_mask)

        c_n1_mask = tf.nn.conv2d(neumann_mask_4, n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask_4, n2, [1,1,1,1], padding)
        elem_y_mask_4 = tf.multiply(c_n1_mask, c_n2_mask)

    
        # create a mask to delete data that are not properly aligned
        c_n3_mask = tf.nn.conv2d(neumann_mask_1, n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask_1, n4, [1,1,1,1], padding)
        elem_x_mask_1 = tf.multiply(c_n3_mask, c_n4_mask)

        c_n3_mask = tf.nn.conv2d(neumann_mask_2, n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask_2, n4, [1,1,1,1], padding)
        elem_x_mask_2 = tf.multiply(c_n3_mask, c_n4_mask)

        c_n3_mask = tf.nn.conv2d(neumann_mask_3, n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask_3, n4, [1,1,1,1], padding)
        elem_x_mask_3 = tf.multiply(c_n3_mask, c_n4_mask)

        c_n3_mask = tf.nn.conv2d(neumann_mask_4, n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask_4, n4, [1,1,1,1], padding)
        elem_x_mask_4 = tf.multiply(c_n3_mask, c_n4_mask)
    
    


    if dof == 1:
        # Scale the Neumann BC value back to the original one
        # original scale in VtuDataGenerateFixedc.py: 
        #   - data = (data + (self.upperlimit - self.lowerlimit) * 0.5 ) * 0.5
        elem_x = 2.0 * elem_x - (Neumann_max - Neumann_min) * 0.5
        elem_y = 2.0 * elem_y - (Neumann_max - Neumann_min) * 0.5

        clean_elem_y = tf.multiply(elem_y, elem_y_mask)
        clean_elem_x = tf.multiply(elem_x, elem_x_mask)
        if pflag: print('clean_elem_y (node1, 2)', np.shape(clean_elem_y), clean_elem_y[0,:,:,0], clean_elem_y[0,:,:,1])
        if pflag: print('clean_elem_x (node1, 2)', np.shape(clean_elem_x), clean_elem_x[0,:,:,0], clean_elem_x[0,:,:,1])
    elif dof == 2:
        elem_x_1 = 2.0 * elem_x_1 - (Neumann_max - Neumann_min) * 0.5
        elem_y_1 = 2.0 * elem_y_1 - (Neumann_max - Neumann_min) * 0.5
        elem_x_2 = 2.0 * elem_x_2 - (Neumann_max - Neumann_min) * 0.5
        elem_y_2 = 2.0 * elem_y_2 - (Neumann_max - Neumann_min) * 0.5

        clean_elem_y_1 = tf.multiply(elem_y_1, elem_y_mask_1)
        clean_elem_x_1 = tf.multiply(elem_x_1, elem_x_mask_1)
        clean_elem_y_2 = tf.multiply(elem_y_2, elem_y_mask_2)
        clean_elem_x_2 = tf.multiply(elem_x_2, elem_x_mask_2)

        if pflag: print('clean_elem_y_1 (node1, 2)', np.shape(clean_elem_y_1), clean_elem_y_1[0,:,:,0], clean_elem_y_1[0,:,:,1])
        if pflag: print('clean_elem_x_1 (node1, 2)', np.shape(clean_elem_x_1), clean_elem_x_1[0,:,:,0], clean_elem_x_1[0,:,:,1])
        if pflag: print('clean_elem_y_2 (node1, 2)', np.shape(clean_elem_y_2), clean_elem_y_2[0,:,:,0], clean_elem_y_2[0,:,:,1])
        if pflag: print('clean_elem_x_2 (node1, 2)', np.shape(clean_elem_x_2), clean_elem_x_2[0,:,:,0], clean_elem_x_2[0,:,:,1])
    elif dof == 3:
        elem_x_1 = 2.0 * elem_x_1 - (Neumann_max - Neumann_min) * 0.5
        elem_y_1 = 2.0 * elem_y_1 - (Neumann_max - Neumann_min) * 0.5
        elem_x_2 = 2.0 * elem_x_2 - (Neumann_max - Neumann_min) * 0.5
        elem_y_2 = 2.0 * elem_y_2 - (Neumann_max - Neumann_min) * 0.5
        elem_x_3 = 2.0 * elem_x_3 - (Neumann_max - Neumann_min) * 0.5
        elem_y_3 = 2.0 * elem_y_3 - (Neumann_max - Neumann_min) * 0.5

        clean_elem_y_1 = tf.multiply(elem_y_1, elem_y_mask_1)
        clean_elem_x_1 = tf.multiply(elem_x_1, elem_x_mask_1)
        clean_elem_y_2 = tf.multiply(elem_y_2, elem_y_mask_2)
        clean_elem_x_2 = tf.multiply(elem_x_2, elem_x_mask_2)
        clean_elem_y_3 = tf.multiply(elem_y_3, elem_y_mask_3)
        clean_elem_x_3 = tf.multiply(elem_x_3, elem_x_mask_3)

        if pflag: print('clean_elem_y_1 (node1, 2)', np.shape(clean_elem_y_1), clean_elem_y_1[0,:,:,0], clean_elem_y_1[0,:,:,1])
        if pflag: print('clean_elem_x_1 (node1, 2)', np.shape(clean_elem_x_1), clean_elem_x_1[0,:,:,0], clean_elem_x_1[0,:,:,1])
        if pflag: print('clean_elem_y_2 (node1, 2)', np.shape(clean_elem_y_2), clean_elem_y_2[0,:,:,0], clean_elem_y_2[0,:,:,1])
        if pflag: print('clean_elem_x_2 (node1, 2)', np.shape(clean_elem_x_2), clean_elem_x_2[0,:,:,0], clean_elem_x_2[0,:,:,1])
        if pflag: print('clean_elem_y_3 (node1, 2)', np.shape(clean_elem_y_3), clean_elem_y_3[0,:,:,0], clean_elem_y_3[0,:,:,1])
        if pflag: print('clean_elem_x_3 (node1, 2)', np.shape(clean_elem_x_3), clean_elem_x_3[0,:,:,0], clean_elem_x_3[0,:,:,1])

    elif dof == 4:
        elem_x_1 = 2.0 * elem_x_1 - (Neumann_max - Neumann_min) * 0.5
        elem_y_1 = 2.0 * elem_y_1 - (Neumann_max - Neumann_min) * 0.5
        elem_x_2 = 2.0 * elem_x_2 - (Neumann_max - Neumann_min) * 0.5
        elem_y_2 = 2.0 * elem_y_2 - (Neumann_max - Neumann_min) * 0.5
        elem_x_3 = 2.0 * elem_x_3 - (Neumann_max - Neumann_min) * 0.5
        elem_y_3 = 2.0 * elem_y_3 - (Neumann_max - Neumann_min) * 0.5
        elem_x_4 = 2.0 * elem_x_4 - (Neumann_max - Neumann_min) * 0.5
        elem_y_4 = 2.0 * elem_y_4 - (Neumann_max - Neumann_min) * 0.5

        clean_elem_y_1 = tf.multiply(elem_y_1, elem_y_mask_1)
        clean_elem_x_1 = tf.multiply(elem_x_1, elem_x_mask_1)
        clean_elem_y_2 = tf.multiply(elem_y_2, elem_y_mask_2)
        clean_elem_x_2 = tf.multiply(elem_x_2, elem_x_mask_2)
        clean_elem_y_3 = tf.multiply(elem_y_3, elem_y_mask_3)
        clean_elem_x_3 = tf.multiply(elem_x_3, elem_x_mask_3)
        clean_elem_y_4 = tf.multiply(elem_y_4, elem_y_mask_4)
        clean_elem_x_4 = tf.multiply(elem_x_4, elem_x_mask_4)



    if dof == 1 :
        shape=elem_x.get_shape()[0:].as_list()    
        new_shape = shape[1:3]
        if pflag: print('new_shape:', new_shape)
    elif dof == 2 :
        shape=elem_x_1.get_shape()[0:].as_list()    
        new_shape = shape[1:3]
        if pflag: print('new_shape:', new_shape)
    elif dof == 3 :
        shape=elem_x_1.get_shape()[0:].as_list()    
        new_shape = shape[1:3]
        if pflag: print('new_shape:', new_shape)
    elif dof == 4 :
        shape=elem_x_1.get_shape()[0:].as_list()    
        new_shape = shape[1:3]


    # get the 1D info, and then perform a N h calculation
    # and then unfold everything to the nodal value
    # 
    N, B, jxw = Get1DGaussPointInfo(dh=dh, GPs=2, dof=1)
    if pflag: print("N", np.shape(N))
    if pflag: print("B", np.shape(B))
    if pflag: print("jxw", jxw)

    if dof == 1:
        elem_x2 = tf.reshape(clean_elem_x,[-1, 2])
        elem_y2 = tf.reshape(clean_elem_y,[-1, 2])

        if pflag: print('elem_x2', np.shape(elem_x2), elem_x2)
        if pflag: print('elem_y2', np.shape(elem_y2), elem_y2)
        # exit(0)

    elif dof == 2:
        elem_x2_1 = tf.reshape(clean_elem_x_1,[-1, 2])
        elem_y2_1 = tf.reshape(clean_elem_y_1,[-1, 2])
        elem_x2_2 = tf.reshape(clean_elem_x_2,[-1, 2])
        elem_y2_2 = tf.reshape(clean_elem_y_2,[-1, 2])

        if pflag: print('elem_x2_1', np.shape(elem_x2_1), elem_x2_1)
        if pflag: print('elem_y2_1', np.shape(elem_y2_1), elem_y2_1)
        if pflag: print('elem_x2_2', np.shape(elem_x2_2), elem_x2_2)
        if pflag: print('elem_y2_2', np.shape(elem_y2_2), elem_y2_2)
    elif dof == 3:
        elem_x2_1 = tf.reshape(clean_elem_x_1,[-1, 2])
        elem_y2_1 = tf.reshape(clean_elem_y_1,[-1, 2])
        elem_x2_2 = tf.reshape(clean_elem_x_2,[-1, 2])
        elem_y2_2 = tf.reshape(clean_elem_y_2,[-1, 2])
        elem_x2_3 = tf.reshape(clean_elem_x_3,[-1, 2])
        elem_y2_3 = tf.reshape(clean_elem_y_3,[-1, 2])

        if pflag: print('elem_x2_1', np.shape(elem_x2_1), elem_x2_1)
        if pflag: print('elem_y2_1', np.shape(elem_y2_1), elem_y2_1)
        if pflag: print('elem_x2_2', np.shape(elem_x2_2), elem_x2_2)
        if pflag: print('elem_y2_2', np.shape(elem_y2_2), elem_y2_2)
        if pflag: print('elem_x2_3', np.shape(elem_x2_3), elem_x2_3)
        if pflag: print('elem_y2_3', np.shape(elem_y2_3), elem_y2_3)
    elif dof == 4:
        elem_x2_1 = tf.reshape(clean_elem_x_1,[-1, 2])
        elem_y2_1 = tf.reshape(clean_elem_y_1,[-1, 2])
        elem_x2_2 = tf.reshape(clean_elem_x_2,[-1, 2])
        elem_y2_2 = tf.reshape(clean_elem_y_2,[-1, 2])
        elem_x2_3 = tf.reshape(clean_elem_x_3,[-1, 2])
        elem_y2_3 = tf.reshape(clean_elem_y_3,[-1, 2])
        elem_x2_4 = tf.reshape(clean_elem_x_4,[-1, 2])
        elem_y2_4 = tf.reshape(clean_elem_y_4,[-1, 2])


    if dof == 1:
        # int(N^T h) dA: hnodal valueshape fcn, scale 
        # Calculate the hbar at the GPs based on nodal info.
        # GP1
        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2, N[0,:]) 
        # GP2
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2, N[1,:]) 
        # GP1
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2, N[0,:]) 
        # GP2
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])

        # if pflag: print('elem_x2_hbar_gp1', np.shape(elem_x2_hbar_gp1),tf.reshape(elem_x2_hbar_gp1, new_shape)) # work for [1, 16, 16, 1], but not [8, 16, 16, 1]
        # if pflag: print('elem_x2_hbar_gp2', np.shape(elem_x2_hbar_gp2),tf.reshape(elem_x2_hbar_gp2, new_shape))
        # if pflag: print('elem_y2_hbar_gp1', np.shape(elem_y2_hbar_gp1),tf.reshape(elem_y2_hbar_gp1, new_shape))
        # if pflag: print('elem_y2_hbar_gp2', np.shape(elem_y2_hbar_gp2),tf.reshape(elem_y2_hbar_gp2, new_shape))

        if pflag: print("N1", N[0,:])
        if pflag: print("N2", N[1,:])

        #-------------------- WARNING --------------------------
        # Since here we start to distinguish x-, y- traction/flux, if the residual contains
        # the gradient term, then we can use different B function for either x-direction
        # or reversed y-direction as the operator to calculate the residual on the edge.
        # For now, we are good.
        #
        #---------------- END OF WARNING -----------------------
        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        # print('Rx1', np.shape(Rx1), Rx1*jxw)
        # print('Rx2', np.shape(Rx2), Rx2*jxw)
        # print('Ry1', np.shape(Ry1), Ry1*jxw)
        # print('Ry2', np.shape(Ry2), Ry2*jxw)
        Rx = jxw * (Rx1 + Rx2)
        Ry = jxw * (Ry1 + Ry2)

        if pflag: print('jxw', jxw)
        if pflag: print('elem_x2_hbar_gp1', elem_x2_hbar_gp1)
        # if pflag: print(N[0:1, :])

        # element level residual for traction in either x or y direction
        Rx = tf.reshape(Rx, [-1, new_shape[0], new_shape[1], 2])
        Ry = tf.reshape(Ry, [-1, new_shape[0], new_shape[1], 2])

        if pflag: print('Rx1', np.shape(Rx), Rx[0,:,:,0])
        if pflag: print('Rx2', np.shape(Rx), Rx[0,:,:,1])
        if pflag: print('Ry1', np.shape(Ry), Ry[0,:,:,0])
        if pflag: print('Ry2', np.shape(Ry), Ry[0,:,:,1])
    elif dof == 2:
        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2_1, N[0,:]) 
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2_1, N[1,:]) 
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2_1, N[0,:]) 
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2_1, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])
        if pflag: print('elem_x2_hbar_gp1', np.shape(elem_x2_hbar_gp1),tf.reshape(elem_x2_hbar_gp1, new_shape))
        if pflag: print('elem_x2_hbar_gp2', np.shape(elem_x2_hbar_gp2),tf.reshape(elem_x2_hbar_gp2, new_shape))
        if pflag: print('elem_y2_hbar_gp1', np.shape(elem_y2_hbar_gp1),tf.reshape(elem_y2_hbar_gp1, new_shape))
        if pflag: print('elem_y2_hbar_gp2', np.shape(elem_y2_hbar_gp2),tf.reshape(elem_y2_hbar_gp2, new_shape))

        if pflag: print("N1", N[0,:])
        if pflag: print("N2", N[1,:])

        #-------------------- WARNING --------------------------
        # Since here we start to distinguish x-, y- traction/flux, if the residual contains
        # the gradient term, then we can use different B function for either x-direction
        # or reversed y-direction as the operator to calculate the residual on the edge.
        # For now, we are good.
        #
        #---------------- END OF WARNING -----------------------
        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        Rx_1 = jxw * (Rx1 + Rx2)
        Ry_1 = jxw * (Ry1 + Ry2)

        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2_2, N[0,:]) 
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2_2, N[1,:]) 
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2_2, N[0,:]) 
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2_2, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])

        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        Rx_2 = jxw * (Rx1 + Rx2)
        Ry_2 = jxw * (Ry1 + Ry2)

        # element level residual for traction in either x or y direction
        Rx_1 = tf.reshape(Rx_1, [-1, new_shape[0], new_shape[1], 2])
        Ry_1 = tf.reshape(Ry_1, [-1, new_shape[0], new_shape[1], 2])
        Rx_2 = tf.reshape(Rx_2, [-1, new_shape[0], new_shape[1], 2])
        Ry_2 = tf.reshape(Ry_2, [-1, new_shape[0], new_shape[1], 2])

        if pflag: print('Rx1_1', np.shape(Rx_1), Rx_1[0,:,:,0])
        if pflag: print('Rx2_1', np.shape(Rx_1), Rx_1[0,:,:,1])
        if pflag: print('Ry1_1', np.shape(Ry_1), Ry_1[0,:,:,0])
        if pflag: print('Ry2_1', np.shape(Ry_1), Ry_1[0,:,:,1])
        if pflag: print('Rx1_2', np.shape(Rx_2), Rx_2[0,:,:,0])
        if pflag: print('Rx2_2', np.shape(Rx_2), Rx_2[0,:,:,1])
        if pflag: print('Ry1_2', np.shape(Ry_2), Ry_2[0,:,:,0])
        if pflag: print('Ry2_2', np.shape(Ry_2), Ry_2[0,:,:,1])
    elif dof == 3:
        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2_1, N[0,:]) 
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2_1, N[1,:]) 
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2_1, N[0,:]) 
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2_1, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])
        if pflag: print('elem_x2_hbar_gp1', np.shape(elem_x2_hbar_gp1),tf.reshape(elem_x2_hbar_gp1, new_shape))
        if pflag: print('elem_x2_hbar_gp2', np.shape(elem_x2_hbar_gp2),tf.reshape(elem_x2_hbar_gp2, new_shape))
        if pflag: print('elem_y2_hbar_gp1', np.shape(elem_y2_hbar_gp1),tf.reshape(elem_y2_hbar_gp1, new_shape))
        if pflag: print('elem_y2_hbar_gp2', np.shape(elem_y2_hbar_gp2),tf.reshape(elem_y2_hbar_gp2, new_shape))

        if pflag: print("N1", N[0,:])
        if pflag: print("N2", N[1,:])

        #-------------------- WARNING --------------------------
        # Since here we start to distinguish x-, y- traction/flux, if the residual contains
        # the gradient term, then we can use different B function for either x-direction
        # or reversed y-direction as the operator to calculate the residual on the edge.
        # For now, we are good.
        #
        #---------------- END OF WARNING -----------------------
        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        Rx_1 = jxw * (Rx1 + Rx2)
        Ry_1 = jxw * (Ry1 + Ry2)

        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2_2, N[0,:]) 
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2_2, N[1,:]) 
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2_2, N[0,:]) 
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2_2, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])

        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        Rx_2 = jxw * (Rx1 + Rx2)
        Ry_2 = jxw * (Ry1 + Ry2)

        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2_3, N[0,:]) 
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2_3, N[1,:]) 
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2_3, N[0,:]) 
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2_3, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])

        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        Rx_3 = jxw * (Rx1 + Rx2)
        Ry_3 = jxw * (Ry1 + Ry2)


        # element level residual for traction in either x or y direction
        Rx_1 = tf.reshape(Rx_1, [-1, new_shape[0], new_shape[1], 2])
        Ry_1 = tf.reshape(Ry_1, [-1, new_shape[0], new_shape[1], 2])
        Rx_2 = tf.reshape(Rx_2, [-1, new_shape[0], new_shape[1], 2])
        Ry_2 = tf.reshape(Ry_2, [-1, new_shape[0], new_shape[1], 2])
        Rx_3 = tf.reshape(Rx_3, [-1, new_shape[0], new_shape[1], 2])
        Ry_3 = tf.reshape(Ry_3, [-1, new_shape[0], new_shape[1], 2])

        if pflag: print('Rx1_1', np.shape(Rx_1), Rx_1[0,:,:,0])
        if pflag: print('Rx2_1', np.shape(Rx_1), Rx_1[0,:,:,1])
        if pflag: print('Ry1_1', np.shape(Ry_1), Ry_1[0,:,:,0])
        if pflag: print('Ry2_1', np.shape(Ry_1), Ry_1[0,:,:,1])
        if pflag: print('Rx1_2', np.shape(Rx_2), Rx_2[0,:,:,0])
        if pflag: print('Rx2_2', np.shape(Rx_2), Rx_2[0,:,:,1])
        if pflag: print('Ry1_2', np.shape(Ry_2), Ry_2[0,:,:,0])
        if pflag: print('Ry2_2', np.shape(Ry_2), Ry_2[0,:,:,1])
        if pflag: print('Rx1_3', np.shape(Rx_3), Rx_3[0,:,:,0])
        if pflag: print('Rx2_3', np.shape(Rx_3), Rx_3[0,:,:,1])
        if pflag: print('Ry1_3', np.shape(Ry_3), Ry_3[0,:,:,0])
        if pflag: print('Ry2_3', np.shape(Ry_3), Ry_3[0,:,:,1])
    elif dof == 4:
        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2_1, N[0,:]) 
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2_1, N[1,:]) 
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2_1, N[0,:]) 
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2_1, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])

        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        Rx_1 = jxw * (Rx1 + Rx2)
        Ry_1 = jxw * (Ry1 + Ry2)

        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2_2, N[0,:]) 
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2_2, N[1,:]) 
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2_2, N[0,:]) 
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2_2, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])

        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        Rx_2 = jxw * (Rx1 + Rx2)
        Ry_2 = jxw * (Ry1 + Ry2)

        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2_3, N[0,:]) 
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2_3, N[1,:]) 
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2_3, N[0,:]) 
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2_3, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])

        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        Rx_3 = jxw * (Rx1 + Rx2)
        Ry_3 = jxw * (Ry1 + Ry2)

        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2_4, N[0,:]) 
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2_4, N[1,:]) 
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2_4, N[0,:]) 
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2_4, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])

        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        Rx_4 = jxw * (Rx1 + Rx2)
        Ry_4 = jxw * (Ry1 + Ry2)

        # element level residual for traction in either x or y direction
        Rx_1 = tf.reshape(Rx_1, [-1, new_shape[0], new_shape[1], 2])
        Ry_1 = tf.reshape(Ry_1, [-1, new_shape[0], new_shape[1], 2])
        Rx_2 = tf.reshape(Rx_2, [-1, new_shape[0], new_shape[1], 2])
        Ry_2 = tf.reshape(Ry_2, [-1, new_shape[0], new_shape[1], 2])
        Rx_3 = tf.reshape(Rx_3, [-1, new_shape[0], new_shape[1], 2])
        Ry_3 = tf.reshape(Ry_3, [-1, new_shape[0], new_shape[1], 2])
        Rx_4 = tf.reshape(Rx_4, [-1, new_shape[0], new_shape[1], 2])
        Ry_4 = tf.reshape(Ry_4, [-1, new_shape[0], new_shape[1], 2])



    if dof == 1:
        c_x1 = Rx[:,:,:,0:1]
        c_x2 = tf.roll(Rx[:,:,:,1:2], [1], [1])

        # on 2020-07-16, was not sure, why this is not shift to the row axis to get nodal value. Right now, it's still nodal information 

        c_y1 = Ry[:,:,:,0:1]
        c_y2 = tf.roll(Ry[:,:,:,1:2], [1], [2])
        if pflag: print('Rx 1 (before): ', Rx[0,:,:,0])
        if pflag: print('Rx 1 (after ): ', c_x1[0,:,:,0])
        if pflag: print('Rx 2 (before): ', Rx[0,:,:,1])
        if pflag: print('Rx 2 (after ): ', c_x2[0,:,:,0])

        if pflag: print('Ry 1 (before): ', Ry[0,:,:,0])
        if pflag: print('Ry 1 (after ): ', c_y1[0,:,:,0])
        if pflag: print('Ry 2 (before): ', Ry[0,:,:,1])
        if pflag: print('Ry 2 (after ): ', c_y2[0,:,:,0])

        Rx = c_x1 + c_x2
        Ry = c_y1 + c_y2
        # Add on 2020-07-16. Note on 2020-07-17, not working well
        # Rx = tf.roll(Rx[:,:,:,0:1], [1], [2])
        # Ry = tf.roll(Ry[:,:,:,0:1], [1], [1])
        #---------------------
        if pflag: print('Pay attention to potential errors here')
        if pflag: print('Rx : ', Rx[0,:,:,0])
        if pflag: print('Ry : ', Ry[0,:,:,0])
    elif dof == 2:
        c_x1 = Rx_1[:,:,:,0:1]
        c_x2 = tf.roll(Rx_1[:,:,:,1:2], [1], [1])

        c_y1 = Ry_1[:,:,:,0:1]
        c_y2 = tf.roll(Ry_1[:,:,:,1:2], [1], [2])

        Rx_1 = c_x1 + c_x2
        Ry_1 = c_y1 + c_y2
        if pflag: print('Rx_1 : ', Rx_1[0,:,:,0])
        if pflag: print('Ry_1 : ', Ry_1[0,:,:,0])

        c_x1 = Rx_2[:,:,:,0:1]
        c_x2 = tf.roll(Rx_2[:,:,:,1:2], [1], [1])

        c_y1 = Ry_2[:,:,:,0:1]
        c_y2 = tf.roll(Ry_2[:,:,:,1:2], [1], [2])

        Rx_2 = c_x1 + c_x2
        Ry_2 = c_y1 + c_y2
        if pflag: print('Rx_2 : ', Rx_2[0,:,:,0])
        if pflag: print('Ry_2 : ', Ry_2[0,:,:,0])
        if pflag: print('Pay attention to potential errors here')

    elif dof == 3:
        c_x1 = Rx_1[:,:,:,0:1]
        c_x2 = tf.roll(Rx_1[:,:,:,1:2], [1], [1])

        c_y1 = Ry_1[:,:,:,0:1]
        c_y2 = tf.roll(Ry_1[:,:,:,1:2], [1], [2])

        Rx_1 = c_x1 + c_x2
        Ry_1 = c_y1 + c_y2
        if pflag: print('Rx_1 : ', Rx_1[0,:,:,0])
        if pflag: print('Ry_1 : ', Ry_1[0,:,:,0])

        c_x1 = Rx_2[:,:,:,0:1]
        c_x2 = tf.roll(Rx_2[:,:,:,1:2], [1], [1])

        c_y1 = Ry_2[:,:,:,0:1]
        c_y2 = tf.roll(Ry_2[:,:,:,1:2], [1], [2])

        Rx_2 = c_x1 + c_x2
        Ry_2 = c_y1 + c_y2
        if pflag: print('Rx_2 : ', Rx_2[0,:,:,0])
        if pflag: print('Ry_2 : ', Ry_2[0,:,:,0])
        if pflag: print('Pay attention to potential errors here')

        c_x1 = Rx_3[:,:,:,0:1]
        c_x2 = tf.roll(Rx_3[:,:,:,1:2], [1], [1])

        c_y1 = Ry_3[:,:,:,0:1]
        c_y2 = tf.roll(Ry_3[:,:,:,1:2], [1], [2])

        Rx_3 = c_x1 + c_x2
        Ry_3 = c_y1 + c_y2
        if pflag: print('Rx_3 : ', Rx_3[0,:,:,0])
        if pflag: print('Ry_3 : ', Ry_3[0,:,:,0])
        if pflag: print('Pay attention to potential errors here')
    elif dof == 4:
        c_x1 = Rx_1[:,:,:,0:1]
        c_x2 = tf.roll(Rx_1[:,:,:,1:2], [1], [1])

        c_y1 = Ry_1[:,:,:,0:1]
        c_y2 = tf.roll(Ry_1[:,:,:,1:2], [1], [2])

        Rx_1 = c_x1 + c_x2
        Ry_1 = c_y1 + c_y2

        c_x1 = Rx_2[:,:,:,0:1]
        c_x2 = tf.roll(Rx_2[:,:,:,1:2], [1], [1])

        c_y1 = Ry_2[:,:,:,0:1]
        c_y2 = tf.roll(Ry_2[:,:,:,1:2], [1], [2])

        Rx_2 = c_x1 + c_x2
        Ry_2 = c_y1 + c_y2

        c_x1 = Rx_3[:,:,:,0:1]
        c_x2 = tf.roll(Rx_3[:,:,:,1:2], [1], [1])

        c_y1 = Ry_3[:,:,:,0:1]
        c_y2 = tf.roll(Ry_3[:,:,:,1:2], [1], [2])

        Rx_3 = c_x1 + c_x2
        Ry_3 = c_y1 + c_y2

        c_x1 = Rx_4[:,:,:,0:1]
        c_x2 = tf.roll(Rx_4[:,:,:,1:2], [1], [1])

        c_y1 = Ry_4[:,:,:,0:1]
        c_y2 = tf.roll(Ry_4[:,:,:,1:2], [1], [2])

        Rx_4 = c_x1 + c_x2
        Ry_4 = c_y1 + c_y2


    if dof == 1 :
        R = Rx + Ry
        R = tf.multiply(R, neumann_mask) # remove other edge left R due to conv operation: do test edge (1,3) and (2,4)
        if pflag: print('R: ', np.shape(R), R[0,:,:,0])
        # R = tf.reverse(R, [1]) # disabled on 2020-07-16
        if pflag: print('R: ', np.shape(R), R[0,:,:,0])
    elif dof == 2:
        # R for dof=x
        R_1 = Rx_1 + Ry_1
        if pflag: print('R_1: ', np.shape(R_1), R_1[0,:,:,0])
        # R for dof=y
        R_2 = Rx_2 + Ry_2
        if pflag: print('R_2: ', np.shape(R_2), R_2[0,:,:,0])

        R = tf.concat([R_1, R_2], axis=3)
        R = tf.multiply(R, neumann_mask) # remove other edge left R due to conv operation: do test edge (1,3) and (2,4)
        # R = tf.reverse(R, [1]) # disabled on 2020-07-22
        if pflag: print('R: ', np.shape(R), R[0,:,:,0], R[0,:,:,1])
    elif dof == 3:
        # R for dof=x
        R_1 = Rx_1 + Ry_1
        if pflag: print('R_1: ', np.shape(R_1), R_1[0,:,:,0])
        # R for dof=y
        R_2 = Rx_2 + Ry_2
        if pflag: print('R_2: ', np.shape(R_2), R_2[0,:,:,0])

        R_3 = Rx_3 + Ry_3
        if pflag: print('R_3: ', np.shape(R_3), R_3[0,:,:,0])

        R = tf.concat([R_1, R_2, R_3], axis=3)
        R = tf.multiply(R, neumann_mask) # remove other edge left R due to conv operation: do test edge (1,3) and (2,4)
        # R = tf.reverse(R, [1]) # disabled on 2020-07-22
        if pflag: print('R: ', np.shape(R), R[0,:,:,0], R[0,:,:,1], R[0,:,:,2])
    elif dof == 4:
        # R for dof=x
        R_1 = Rx_1 + Ry_1
        # R for dof=y
        R_2 = Rx_2 + Ry_2

        R_3 = Rx_3 + Ry_3

        R_4 = Rx_4 + Ry_4
        R = tf.concat([R_1, R_2, R_3, R_4], axis=3)
        R = tf.multiply(R, neumann_mask) # remove other edge left R due to conv operation: do test edge (1,3) and (2,4)
        if pflag: print('R: ', np.shape(R), R[0,:,:,0], R[0,:,:,1], R[0,:,:,2], R[0,:,:,3])

    # exit(0)

    return R

def ComputeNeumannBoundaryResidualNodalDataNew(data_input, dh, dof, padding='SAME'):
    """ 
    Compute the residual on the Neumann BCs.  The implementation is based on Neumann BCs is scaled between (-1, 1), and Neumann condition should be always > 0 in the domain region. Raise value error if negative value is detected

    args:
        data_input (numpy array): size of [batch, node_height, node_width, dof*3]
        dof (int): dof per node
        dh (float): element size
    return:
        numpy array: nodal Neumann residual value with size of [batch, node_height, node_width, dof]

    todo:
        make this function to work with (1S, 1V), 2S, 1V1S, 3S, 2V, etc.

        loop over each dof, instead of implementing different dof opt.
    """
    # pflag = True
    pflag = False
    if (dof > 4):
        raise ValueError(" dof = ", dof, " is not implemented! Only dof = 1 or 2 or 3 or 4 is coded!")

    data_input = tf.convert_to_tensor(data_input, dtype=tf.float32)
    if pflag: print('data_input', np.shape(data_input))

    # --------------- Dirichlet data------------
    dirichlet_data = data_input[:,:,:,0:dof]
    if pflag: print('dirichlet_data', dirichlet_data[0,:,:,0])

    #--------------- Domain mask ------------------
    # change actual dirichlet BCs to -2, the all -2 will be the domain
    domain_mask = tf.where( dirichlet_data >= 0.0, tf.fill(tf.shape(dirichlet_data), -2.0), dirichlet_data)
    # print('domain_mask', domain_mask[0,:,:,0])
    domain_mask = tf.where( domain_mask < -1.0, tf.fill(tf.shape(domain_mask), 1.0), tf.fill(tf.shape(domain_mask), 0.0))
    if pflag: print('domain_mask', domain_mask[0,:,:,0])

    #--------------- Neumann BCs ------------------
    Neumann_max = 1.0
    Neumann_min = -1.0
    neumann_data = data_input[:,:,:,dof:dof+dof+dof]
    if pflag: print('neumann_data', neumann_data[0,:,:,0])
    if pflag: print('neumann_data(dof-1)', neumann_data[0,:,:,1])

    # #--------------- check Neumann BCs------------ !!!! should be checked at the very beginning of data.
    # # is not allowed during tensor running/training 
    # check_neumann = tf.multiply(neumann_data, domain_mask)
    # if pflag: print('---check_neumann', check_neumann[0,:,:,0])
    # if (tf.reduce_sum(check_neumann) == 0) :
        # print("WARNING: no Neumann BCs is detected!")
    # check_neumann = tf.where( check_neumann < 0.0, tf.fill(tf.shape(check_neumann), 1.0), tf.fill(tf.shape(check_neumann), 0.0))
    # if pflag: print('check_neumann', check_neumann[0,:,:,0])
    # if (tf.reduce_sum(check_neumann) < 0) :
        # raise ValueError("Neumann BCs should NOT be smaller than zero (< 0). Consider use diffusivity or elastic constant to scale the data!")

    #---------------- Neumann BCs-----------------
    # Should consider the scaling as well.  Any mask will not work, as Neumann BCs can be smaller than 0.0
    # Solution: Neumann BCs is not allowed to be smaller than 0 in the input data.

    neumann_mask = tf.where( neumann_data <= 0.0, tf.fill(tf.shape(neumann_data), 0.0), tf.fill(tf.shape(neumann_data), 1.0))
    if pflag: print('neumann_mask', neumann_mask[0,:,:,0])
    if pflag: print('neumann_mask(dof-1)', neumann_mask[0,:,:,1])
    neumann_data = tf.multiply( neumann_data, neumann_mask)
    if pflag: print('neumann_data', neumann_data[0,:,:,0])
    if pflag: print('neumann_data(dof-1)', neumann_data[0,:,:,1])

    # The main idea is to form a element connection as for the bulk residual case.
    # However, if the neumman BCs is location dependent, has both positive and negative 
    # values, then, it is very challenging to make the following to work stably. 
    # On the other hand, if we only consider the bulk region, then the neumman BCs is
    # literally enforced through the residual form, but not explicitly. 
    # Still, the following might still work.

    # form dof-1 level horizontal element
    # the padding zeros will helps to keep the location of surface node unchanged for the bottom edges
    # n1    n2    -> t_y
    # 1 0   0 1
    # 0 0   0 0
    n1 = np.array([[1, 0], [0, 0]])
    n1 = np.expand_dims(n1, axis=2)
    n1 = np.expand_dims(n1, axis=3)
    n2 = np.array([[0, 1], [0, 0]])
    n2 = np.expand_dims(n2, axis=2)
    n2 = np.expand_dims(n2, axis=3)

    # n3   n4     -> t_x
    # 1 0  0 0
    # 0 0  1 0
    # form dof-1 level vertical element:
    n3 = np.array([[1, 0], [0, 0]])
    n3 = np.expand_dims(n3, axis=2)
    n3 = np.expand_dims(n3, axis=3)
    n4 = np.array([[0, 0], [1, 0]])
    n4 = np.expand_dims(n4, axis=2)
    n4 = np.expand_dims(n4, axis=3)


    if (dof != 1):
        raise ValueError(" t_x and t_y might need to be reversed for dof>1, was not tested in the following implementation!")

    # create surface elements
    if dof == 1:
        # horizontal element
        c_n1 = tf.nn.conv2d(neumann_data[:,:,:,1:2], n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data[:,:,:,1:2], n2, [1,1,1,1], padding)
        elem_y = tf.concat([c_n1, c_n2], 3)

        # vertical element
        c_n3 = tf.nn.conv2d(neumann_data[:,:,:,0:1], n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data[:,:,:,0:1], n4, [1,1,1,1], padding)
        elem_x = tf.concat([c_n3, c_n4], 3)

        if pflag: print('elem_x', np.shape(elem_x))
        if pflag: print('elem_x 1: ', elem_x[0,:,:,0])
        if pflag: print('elem_x 2: ', elem_x[0,:,:,1])

        if pflag: print('elem_y', np.shape(elem_y))
        if pflag: print('elem_y 1: ', elem_y[0,:,:,0])
        if pflag: print('elem_y 2: ', elem_y[0,:,:,1])
    elif dof == 2:
        neumann_data_1 = neumann_data[:,:,:,0:2]
        neumann_data_2 = neumann_data[:,:,:,2:4]

        c_n1 = tf.nn.conv2d(neumann_data_1[:,:,:,0:1], n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data_1[:,:,:,0:1], n2, [1,1,1,1], padding)
        elem_y_1 = tf.concat([c_n1, c_n2], 3)

        c_n1 = tf.nn.conv2d(neumann_data_2[:,:,:,0:1], n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data_2[:,:,:,0:1], n2, [1,1,1,1], padding)
        elem_y_2 = tf.concat([c_n1, c_n2], 3)

        c_n3 = tf.nn.conv2d(neumann_data_1[:,:,:,1:2], n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data_1[:,:,:,1:2], n4, [1,1,1,1], padding)
        elem_x_1 = tf.concat([c_n3, c_n4], 3)

        c_n3 = tf.nn.conv2d(neumann_data_2[:,:,:,1:2], n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data_2[:,:,:,1:2], n4, [1,1,1,1], padding)
        elem_x_2 = tf.concat([c_n3, c_n4], 3)

        if pflag: print('elem_y_1 ', np.shape(elem_y_1))
        if pflag: print('elem_y_1 1: ', elem_y_1[0,:,:,0])
        if pflag: print('elem_y_1 2: ', elem_y_1[0,:,:,1])
        if pflag: print('elem_y_2 ', np.shape(elem_y_2))
        if pflag: print('elem_y_2 1: ', elem_y_2[0,:,:,0])
        if pflag: print('elem_y_2 2: ', elem_y_2[0,:,:,1])

        if pflag: print('elem_x_1 ', np.shape(elem_x_1))
        if pflag: print('elem_x_1 1: ', elem_x_1[0,:,:,0])
        if pflag: print('elem_x_1 2: ', elem_x_1[0,:,:,1])
        if pflag: print('elem_x_2 ', np.shape(elem_x_2))
        if pflag: print('elem_x_2 1: ', elem_x_2[0,:,:,0])
        if pflag: print('elem_x_2 2: ', elem_x_2[0,:,:,1])
    elif dof == 3:
        neumann_data_1 = neumann_data[:,:,:,0:2]
        neumann_data_2 = neumann_data[:,:,:,2:4]
        neumann_data_3 = neumann_data[:,:,:,4:6]

        c_n1 = tf.nn.conv2d(neumann_data_1[:,:,:,0:1], n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data_1[:,:,:,0:1], n2, [1,1,1,1], padding)
        elem_y_1 = tf.concat([c_n1, c_n2], 3)

        c_n1 = tf.nn.conv2d(neumann_data_2[:,:,:,0:1], n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data_2[:,:,:,0:1], n2, [1,1,1,1], padding)
        elem_y_2 = tf.concat([c_n1, c_n2], 3)

        c_n1 = tf.nn.conv2d(neumann_data_3[:,:,:,0:1], n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data_3[:,:,:,0:1], n2, [1,1,1,1], padding)
        elem_y_3 = tf.concat([c_n1, c_n2], 3)

        c_n3 = tf.nn.conv2d(neumann_data_1[:,:,:,1:2], n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data_1[:,:,:,1:2], n4, [1,1,1,1], padding)
        elem_x_1 = tf.concat([c_n3, c_n4], 3)

        c_n3 = tf.nn.conv2d(neumann_data_2[:,:,:,1:2], n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data_2[:,:,:,1:2], n4, [1,1,1,1], padding)
        elem_x_2 = tf.concat([c_n3, c_n4], 3)

        c_n3 = tf.nn.conv2d(neumann_data_3[:,:,:,1:2], n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data_3[:,:,:,1:2], n4, [1,1,1,1], padding)
        elem_x_3 = tf.concat([c_n3, c_n4], 3)

        if pflag: print('elem_y_1 ', np.shape(elem_y_1))
        if pflag: print('elem_y_1 1: ', elem_y_1[0,:,:,0])
        if pflag: print('elem_y_1 2: ', elem_y_1[0,:,:,1])
        if pflag: print('elem_y_2 ', np.shape(elem_y_2))
        if pflag: print('elem_y_2 1: ', elem_y_2[0,:,:,0])
        if pflag: print('elem_y_2 2: ', elem_y_2[0,:,:,1])
        if pflag: print('elem_y_3 ', np.shape(elem_y_3))
        if pflag: print('elem_y_3 1: ', elem_y_3[0,:,:,0])
        if pflag: print('elem_y_3 2: ', elem_y_3[0,:,:,1])

        if pflag: print('elem_x_1 ', np.shape(elem_x_1))
        if pflag: print('elem_x_1 1: ', elem_x_1[0,:,:,0])
        if pflag: print('elem_x_1 2: ', elem_x_1[0,:,:,1])
        if pflag: print('elem_x_2 ', np.shape(elem_x_2))
        if pflag: print('elem_x_2 1: ', elem_x_2[0,:,:,0])
        if pflag: print('elem_x_2 2: ', elem_x_2[0,:,:,1])
        if pflag: print('elem_x_3 ', np.shape(elem_x_3))
        if pflag: print('elem_x_3 1: ', elem_x_3[0,:,:,0])
        if pflag: print('elem_x_3 2: ', elem_x_3[0,:,:,1])

    elif dof == 4:
        neumann_data_1 = neumann_data[:,:,:,0:2]
        neumann_data_2 = neumann_data[:,:,:,2:4]
        neumann_data_3 = neumann_data[:,:,:,4:6]
        neumann_data_4 = neumann_data[:,:,:,6:8]

        c_n1 = tf.nn.conv2d(neumann_data_1[:,:,:,0:1], n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data_1[:,:,:,0:1], n2, [1,1,1,1], padding)
        elem_y_1 = tf.concat([c_n1, c_n2], 3)

        c_n1 = tf.nn.conv2d(neumann_data_2[:,:,:,0:1], n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data_2[:,:,:,0:1], n2, [1,1,1,1], padding)
        elem_y_2 = tf.concat([c_n1, c_n2], 3)

        c_n1 = tf.nn.conv2d(neumann_data_3[:,:,:,0:1], n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data_3[:,:,:,0:1], n2, [1,1,1,1], padding)
        elem_y_3 = tf.concat([c_n1, c_n2], 3)

        c_n1 = tf.nn.conv2d(neumann_data_4[:,:,:,0:1], n1, [1,1,1,1], padding )
        c_n2 = tf.nn.conv2d(neumann_data_4[:,:,:,0:1], n2, [1,1,1,1], padding)
        elem_y_4 = tf.concat([c_n1, c_n2], 3)

        c_n3 = tf.nn.conv2d(neumann_data_1[:,:,:,1:2], n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data_1[:,:,:,1:2], n4, [1,1,1,1], padding)
        elem_x_1 = tf.concat([c_n3, c_n4], 3)

        c_n3 = tf.nn.conv2d(neumann_data_2[:,:,:,1:2], n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data_2[:,:,:,1:2], n4, [1,1,1,1], padding)
        elem_x_2 = tf.concat([c_n3, c_n4], 3)

        c_n3 = tf.nn.conv2d(neumann_data_3[:,:,:,1:2], n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data_3[:,:,:,1:2], n4, [1,1,1,1], padding)
        elem_x_3 = tf.concat([c_n3, c_n4], 3)

        c_n3 = tf.nn.conv2d(neumann_data_4[:,:,:,1:2], n3, [1,1,1,1], padding )
        c_n4 = tf.nn.conv2d(neumann_data_4[:,:,:,1:2], n4, [1,1,1,1], padding)
        elem_x_4 = tf.concat([c_n3, c_n4], 3)



    if dof == 1:
        # channel 0:1 == channel 1:2
        # create a mask to delete data that are not properly aligned
        c_n1_mask = tf.nn.conv2d(neumann_mask[:,:,:,1:2], n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask[:,:,:,1:2], n2, [1,1,1,1], padding)
        elem_y_mask = tf.multiply(c_n1_mask, c_n2_mask)
        if pflag: print('c_n1_mask: ', c_n1_mask[0,:,:,0])
        if pflag: print('c_n2_mask: ', c_n2_mask[0,:,:,0])
        if pflag: print('elem_y_mask: ', elem_y_mask[0,:,:,0])
    
        # create a mask to delete data that are not properly aligned
        c_n3_mask = tf.nn.conv2d(neumann_mask[:,:,:,0:1], n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask[:,:,:,0:1], n4, [1,1,1,1], padding)
        elem_x_mask = tf.multiply(c_n3_mask, c_n4_mask)
        if pflag: print('c_n3_mask: ', c_n3_mask[0,:,:,0])
        if pflag: print('c_n4_mask: ', c_n4_mask[0,:,:,0])
        if pflag: print('elem_x_mask: ', elem_x_mask[0,:,:,0])
    elif dof == 2:
        # For the 3D case, it would be impossible to perform task like this.
        # Thus, how to come up with a 3D implementation, or sparse pattern 
        # would be extremely useful.

        # create a mask to delete data that are not properly aligned
        neumann_mask_1 = neumann_mask[:,:,:,0:1] # 0:1 = 1:2
        neumann_mask_2 = neumann_mask[:,:,:,2:3]

        c_n1_mask = tf.nn.conv2d(neumann_mask_1, n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask_1, n2, [1,1,1,1], padding)
        elem_y_mask_1 = tf.multiply(c_n1_mask, c_n2_mask)

        if pflag: print('c_n1_mask: ', c_n1_mask[0,:,:,0])
        if pflag: print('c_n2_mask: ', c_n2_mask[0,:,:,0])
        if pflag: print('elem_y_mask_1: ', elem_y_mask_1[0,:,:,0])

        c_n1_mask = tf.nn.conv2d(neumann_mask_2, n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask_2, n2, [1,1,1,1], padding)
        elem_y_mask_2 = tf.multiply(c_n1_mask, c_n2_mask)

        if pflag: print('c_n1_mask: ', c_n1_mask[0,:,:,0])
        if pflag: print('c_n2_mask: ', c_n2_mask[0,:,:,0])
        if pflag: print('elem_y_mask_2: ', elem_y_mask_2[0,:,:,0])

    
        # create a mask to delete data that are not properly aligned
        c_n3_mask = tf.nn.conv2d(neumann_mask_1, n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask_1, n4, [1,1,1,1], padding)
        elem_x_mask_1 = tf.multiply(c_n3_mask, c_n4_mask)
        if pflag: print('c_n3_mask: ', c_n3_mask[0,:,:,0])
        if pflag: print('c_n4_mask: ', c_n4_mask[0,:,:,0])
        if pflag: print('elem_x_mask_1: ', elem_x_mask_1[0,:,:,0])

        c_n3_mask = tf.nn.conv2d(neumann_mask_2, n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask_2, n4, [1,1,1,1], padding)
        elem_x_mask_2 = tf.multiply(c_n3_mask, c_n4_mask)
        if pflag: print('c_n3_mask: ', c_n3_mask[0,:,:,0])
        if pflag: print('c_n4_mask: ', c_n4_mask[0,:,:,0])
        if pflag: print('elem_x_mask_2: ', elem_x_mask_2[0,:,:,0])

    elif dof == 3:

        # create a mask to delete data that are not properly aligned
        neumann_mask_1 = neumann_mask[:,:,:,0:1]
        neumann_mask_2 = neumann_mask[:,:,:,2:3]
        neumann_mask_3 = neumann_mask[:,:,:,4:5]

        c_n1_mask = tf.nn.conv2d(neumann_mask_1, n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask_1, n2, [1,1,1,1], padding)
        elem_y_mask_1 = tf.multiply(c_n1_mask, c_n2_mask)

        if pflag: print('c_n1_mask: ', c_n1_mask[0,:,:,0])
        if pflag: print('c_n2_mask: ', c_n2_mask[0,:,:,0])
        if pflag: print('elem_y_mask_1: ', elem_y_mask_1[0,:,:,0])

        c_n1_mask = tf.nn.conv2d(neumann_mask_2, n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask_2, n2, [1,1,1,1], padding)
        elem_y_mask_2 = tf.multiply(c_n1_mask, c_n2_mask)

        if pflag: print('c_n1_mask: ', c_n1_mask[0,:,:,0])
        if pflag: print('c_n2_mask: ', c_n2_mask[0,:,:,0])
        if pflag: print('elem_y_mask_2: ', elem_y_mask_2[0,:,:,0])

        c_n1_mask = tf.nn.conv2d(neumann_mask_3, n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask_3, n2, [1,1,1,1], padding)
        elem_y_mask_3 = tf.multiply(c_n1_mask, c_n2_mask)

        if pflag: print('c_n1_mask: ', c_n1_mask[0,:,:,0])
        if pflag: print('c_n2_mask: ', c_n2_mask[0,:,:,0])
        if pflag: print('elem_y_mask_3: ', elem_y_mask_3[0,:,:,0])

    
        # create a mask to delete data that are not properly aligned
        c_n3_mask = tf.nn.conv2d(neumann_mask_1, n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask_1, n4, [1,1,1,1], padding)
        elem_x_mask_1 = tf.multiply(c_n3_mask, c_n4_mask)
        if pflag: print('c_n3_mask: ', c_n3_mask[0,:,:,0])
        if pflag: print('c_n4_mask: ', c_n4_mask[0,:,:,0])
        if pflag: print('elem_x_mask_1: ', elem_x_mask_1[0,:,:,0])

        c_n3_mask = tf.nn.conv2d(neumann_mask_2, n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask_2, n4, [1,1,1,1], padding)
        elem_x_mask_2 = tf.multiply(c_n3_mask, c_n4_mask)
        if pflag: print('c_n3_mask: ', c_n3_mask[0,:,:,0])
        if pflag: print('c_n4_mask: ', c_n4_mask[0,:,:,0])
        if pflag: print('elem_x_mask_2: ', elem_x_mask_2[0,:,:,0])

        c_n3_mask = tf.nn.conv2d(neumann_mask_3, n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask_3, n4, [1,1,1,1], padding)
        elem_x_mask_3 = tf.multiply(c_n3_mask, c_n4_mask)
        if pflag: print('c_n3_mask: ', c_n3_mask[0,:,:,0])
        if pflag: print('c_n4_mask: ', c_n4_mask[0,:,:,0])
        if pflag: print('elem_x_mask_3: ', elem_x_mask_3[0,:,:,0])

    elif dof == 4:

        # create a mask to delete data that are not properly aligned
        neumann_mask_1 = neumann_mask[:,:,:,0:1]
        neumann_mask_2 = neumann_mask[:,:,:,2:3]
        neumann_mask_3 = neumann_mask[:,:,:,4:5]
        neumann_mask_4 = neumann_mask[:,:,:,6:7]

        c_n1_mask = tf.nn.conv2d(neumann_mask_1, n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask_1, n2, [1,1,1,1], padding)
        elem_y_mask_1 = tf.multiply(c_n1_mask, c_n2_mask)

        c_n1_mask = tf.nn.conv2d(neumann_mask_2, n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask_2, n2, [1,1,1,1], padding)
        elem_y_mask_2 = tf.multiply(c_n1_mask, c_n2_mask)

        c_n1_mask = tf.nn.conv2d(neumann_mask_3, n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask_3, n2, [1,1,1,1], padding)
        elem_y_mask_3 = tf.multiply(c_n1_mask, c_n2_mask)

        c_n1_mask = tf.nn.conv2d(neumann_mask_4, n1, [1,1,1,1], padding )
        c_n2_mask = tf.nn.conv2d(neumann_mask_4, n2, [1,1,1,1], padding)
        elem_y_mask_4 = tf.multiply(c_n1_mask, c_n2_mask)

    
        # create a mask to delete data that are not properly aligned
        c_n3_mask = tf.nn.conv2d(neumann_mask_1, n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask_1, n4, [1,1,1,1], padding)
        elem_x_mask_1 = tf.multiply(c_n3_mask, c_n4_mask)

        c_n3_mask = tf.nn.conv2d(neumann_mask_2, n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask_2, n4, [1,1,1,1], padding)
        elem_x_mask_2 = tf.multiply(c_n3_mask, c_n4_mask)

        c_n3_mask = tf.nn.conv2d(neumann_mask_3, n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask_3, n4, [1,1,1,1], padding)
        elem_x_mask_3 = tf.multiply(c_n3_mask, c_n4_mask)

        c_n3_mask = tf.nn.conv2d(neumann_mask_4, n3, [1,1,1,1], padding )
        c_n4_mask = tf.nn.conv2d(neumann_mask_4, n4, [1,1,1,1], padding)
        elem_x_mask_4 = tf.multiply(c_n3_mask, c_n4_mask)
    
    


    if dof == 1:
        # Scale the Neumann BC value back to the original one
        # original scale in VtuDataGenerateFixedc.py: 
        #   - data = (data + (self.upperlimit - self.lowerlimit) * 0.5 ) * 0.5
        elem_x = 2.0 * elem_x - (Neumann_max - Neumann_min) * 0.5
        elem_y = 2.0 * elem_y - (Neumann_max - Neumann_min) * 0.5

        clean_elem_y = tf.multiply(elem_y, elem_y_mask)
        clean_elem_x = tf.multiply(elem_x, elem_x_mask)
        if pflag: print('clean_elem_y (node1, 2)', np.shape(clean_elem_y), clean_elem_y[0,:,:,0], clean_elem_y[0,:,:,1])
        if pflag: print('clean_elem_x (node1, 2)', np.shape(clean_elem_x), clean_elem_x[0,:,:,0], clean_elem_x[0,:,:,1])

    elif dof == 2:
        elem_x_1 = 2.0 * elem_x_1 - (Neumann_max - Neumann_min) * 0.5
        elem_y_1 = 2.0 * elem_y_1 - (Neumann_max - Neumann_min) * 0.5
        elem_x_2 = 2.0 * elem_x_2 - (Neumann_max - Neumann_min) * 0.5
        elem_y_2 = 2.0 * elem_y_2 - (Neumann_max - Neumann_min) * 0.5

        clean_elem_y_1 = tf.multiply(elem_y_1, elem_y_mask_1)
        clean_elem_x_1 = tf.multiply(elem_x_1, elem_x_mask_1)
        clean_elem_y_2 = tf.multiply(elem_y_2, elem_y_mask_2)
        clean_elem_x_2 = tf.multiply(elem_x_2, elem_x_mask_2)

        if pflag: print('clean_elem_y_1 (node1, 2)', np.shape(clean_elem_y_1), clean_elem_y_1[0,:,:,0], clean_elem_y_1[0,:,:,1])
        if pflag: print('clean_elem_x_1 (node1, 2)', np.shape(clean_elem_x_1), clean_elem_x_1[0,:,:,0], clean_elem_x_1[0,:,:,1])
        if pflag: print('clean_elem_y_2 (node1, 2)', np.shape(clean_elem_y_2), clean_elem_y_2[0,:,:,0], clean_elem_y_2[0,:,:,1])
        if pflag: print('clean_elem_x_2 (node1, 2)', np.shape(clean_elem_x_2), clean_elem_x_2[0,:,:,0], clean_elem_x_2[0,:,:,1])
    elif dof == 3:
        elem_x_1 = 2.0 * elem_x_1 - (Neumann_max - Neumann_min) * 0.5
        elem_y_1 = 2.0 * elem_y_1 - (Neumann_max - Neumann_min) * 0.5
        elem_x_2 = 2.0 * elem_x_2 - (Neumann_max - Neumann_min) * 0.5
        elem_y_2 = 2.0 * elem_y_2 - (Neumann_max - Neumann_min) * 0.5
        elem_x_3 = 2.0 * elem_x_3 - (Neumann_max - Neumann_min) * 0.5
        elem_y_3 = 2.0 * elem_y_3 - (Neumann_max - Neumann_min) * 0.5

        clean_elem_y_1 = tf.multiply(elem_y_1, elem_y_mask_1)
        clean_elem_x_1 = tf.multiply(elem_x_1, elem_x_mask_1)
        clean_elem_y_2 = tf.multiply(elem_y_2, elem_y_mask_2)
        clean_elem_x_2 = tf.multiply(elem_x_2, elem_x_mask_2)
        clean_elem_y_3 = tf.multiply(elem_y_3, elem_y_mask_3)
        clean_elem_x_3 = tf.multiply(elem_x_3, elem_x_mask_3)

        if pflag: print('clean_elem_y_1 (node1, 2)', np.shape(clean_elem_y_1), clean_elem_y_1[0,:,:,0], clean_elem_y_1[0,:,:,1])
        if pflag: print('clean_elem_x_1 (node1, 2)', np.shape(clean_elem_x_1), clean_elem_x_1[0,:,:,0], clean_elem_x_1[0,:,:,1])
        if pflag: print('clean_elem_y_2 (node1, 2)', np.shape(clean_elem_y_2), clean_elem_y_2[0,:,:,0], clean_elem_y_2[0,:,:,1])
        if pflag: print('clean_elem_x_2 (node1, 2)', np.shape(clean_elem_x_2), clean_elem_x_2[0,:,:,0], clean_elem_x_2[0,:,:,1])
        if pflag: print('clean_elem_y_3 (node1, 2)', np.shape(clean_elem_y_3), clean_elem_y_3[0,:,:,0], clean_elem_y_3[0,:,:,1])
        if pflag: print('clean_elem_x_3 (node1, 2)', np.shape(clean_elem_x_3), clean_elem_x_3[0,:,:,0], clean_elem_x_3[0,:,:,1])

    elif dof == 4:
        elem_x_1 = 2.0 * elem_x_1 - (Neumann_max - Neumann_min) * 0.5
        elem_y_1 = 2.0 * elem_y_1 - (Neumann_max - Neumann_min) * 0.5
        elem_x_2 = 2.0 * elem_x_2 - (Neumann_max - Neumann_min) * 0.5
        elem_y_2 = 2.0 * elem_y_2 - (Neumann_max - Neumann_min) * 0.5
        elem_x_3 = 2.0 * elem_x_3 - (Neumann_max - Neumann_min) * 0.5
        elem_y_3 = 2.0 * elem_y_3 - (Neumann_max - Neumann_min) * 0.5
        elem_x_4 = 2.0 * elem_x_4 - (Neumann_max - Neumann_min) * 0.5
        elem_y_4 = 2.0 * elem_y_4 - (Neumann_max - Neumann_min) * 0.5

        clean_elem_y_1 = tf.multiply(elem_y_1, elem_y_mask_1)
        clean_elem_x_1 = tf.multiply(elem_x_1, elem_x_mask_1)
        clean_elem_y_2 = tf.multiply(elem_y_2, elem_y_mask_2)
        clean_elem_x_2 = tf.multiply(elem_x_2, elem_x_mask_2)
        clean_elem_y_3 = tf.multiply(elem_y_3, elem_y_mask_3)
        clean_elem_x_3 = tf.multiply(elem_x_3, elem_x_mask_3)
        clean_elem_y_4 = tf.multiply(elem_y_4, elem_y_mask_4)
        clean_elem_x_4 = tf.multiply(elem_x_4, elem_x_mask_4)



    if dof == 1 :
        shape=elem_x.get_shape()[0:].as_list()    
        new_shape = shape[1:3]
        if pflag: print('new_shape:', new_shape)
    elif dof == 2 :
        shape=elem_x_1.get_shape()[0:].as_list()    
        new_shape = shape[1:3]
        if pflag: print('new_shape:', new_shape)
    elif dof == 3 :
        shape=elem_x_1.get_shape()[0:].as_list()    
        new_shape = shape[1:3]
        if pflag: print('new_shape:', new_shape)
    elif dof == 4 :
        shape=elem_x_1.get_shape()[0:].as_list()    
        new_shape = shape[1:3]


    # get the 1D info, and then perform a N h calculation
    # and then unfold everything to the nodal value
    # 
    N, B, jxw = Get1DGaussPointInfo(dh=dh, GPs=2, dof=1)
    if pflag: print("N", np.shape(N))
    if pflag: print("B", np.shape(B))
    if pflag: print("jxw", jxw)

    if dof == 1:
        elem_x2 = tf.reshape(clean_elem_x,[-1, 2])
        elem_y2 = tf.reshape(clean_elem_y,[-1, 2])

        if pflag: print('elem_x2', np.shape(elem_x2), elem_x2)
        if pflag: print('elem_y2', np.shape(elem_y2), elem_y2)

    elif dof == 2:
        elem_x2_1 = tf.reshape(clean_elem_x_1,[-1, 2])
        elem_y2_1 = tf.reshape(clean_elem_y_1,[-1, 2])
        elem_x2_2 = tf.reshape(clean_elem_x_2,[-1, 2])
        elem_y2_2 = tf.reshape(clean_elem_y_2,[-1, 2])

        if pflag: print('elem_x2_1', np.shape(elem_x2_1), elem_x2_1)
        if pflag: print('elem_y2_1', np.shape(elem_y2_1), elem_y2_1)
        if pflag: print('elem_x2_2', np.shape(elem_x2_2), elem_x2_2)
        if pflag: print('elem_y2_2', np.shape(elem_y2_2), elem_y2_2)
    elif dof == 3:
        elem_x2_1 = tf.reshape(clean_elem_x_1,[-1, 2])
        elem_y2_1 = tf.reshape(clean_elem_y_1,[-1, 2])
        elem_x2_2 = tf.reshape(clean_elem_x_2,[-1, 2])
        elem_y2_2 = tf.reshape(clean_elem_y_2,[-1, 2])
        elem_x2_3 = tf.reshape(clean_elem_x_3,[-1, 2])
        elem_y2_3 = tf.reshape(clean_elem_y_3,[-1, 2])

        if pflag: print('elem_x2_1', np.shape(elem_x2_1), elem_x2_1)
        if pflag: print('elem_y2_1', np.shape(elem_y2_1), elem_y2_1)
        if pflag: print('elem_x2_2', np.shape(elem_x2_2), elem_x2_2)
        if pflag: print('elem_y2_2', np.shape(elem_y2_2), elem_y2_2)
        if pflag: print('elem_x2_3', np.shape(elem_x2_3), elem_x2_3)
        if pflag: print('elem_y2_3', np.shape(elem_y2_3), elem_y2_3)
    elif dof == 4:
        elem_x2_1 = tf.reshape(clean_elem_x_1,[-1, 2])
        elem_y2_1 = tf.reshape(clean_elem_y_1,[-1, 2])
        elem_x2_2 = tf.reshape(clean_elem_x_2,[-1, 2])
        elem_y2_2 = tf.reshape(clean_elem_y_2,[-1, 2])
        elem_x2_3 = tf.reshape(clean_elem_x_3,[-1, 2])
        elem_y2_3 = tf.reshape(clean_elem_y_3,[-1, 2])
        elem_x2_4 = tf.reshape(clean_elem_x_4,[-1, 2])
        elem_y2_4 = tf.reshape(clean_elem_y_4,[-1, 2])


    if dof == 1:
        # int(N^T h) dA: hnodal valueshape fcn, scale 
        # Calculate the hbar at the GPs based on nodal info.
        # GP1
        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2, N[0,:]) 
        # GP2
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2, N[1,:]) 
        # GP1
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2, N[0,:]) 
        # GP2
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])

        # if pflag: print('elem_x2_hbar_gp1', np.shape(elem_x2_hbar_gp1),tf.reshape(elem_x2_hbar_gp1, new_shape)) # work for [1, 16, 16, 1], but not [8, 16, 16, 1]
        # if pflag: print('elem_x2_hbar_gp2', np.shape(elem_x2_hbar_gp2),tf.reshape(elem_x2_hbar_gp2, new_shape))
        # if pflag: print('elem_y2_hbar_gp1', np.shape(elem_y2_hbar_gp1),tf.reshape(elem_y2_hbar_gp1, new_shape))
        # if pflag: print('elem_y2_hbar_gp2', np.shape(elem_y2_hbar_gp2),tf.reshape(elem_y2_hbar_gp2, new_shape))

        if pflag: print("N1", N[0,:])
        if pflag: print("N2", N[1,:])

        #-------------------- WARNING --------------------------
        # Since here we start to distinguish x-, y- traction/flux, if the residual contains
        # the gradient term, then we can use different B function for either x-direction
        # or reversed y-direction as the operator to calculate the residual on the edge.
        # For now, we are good.
        #
        #---------------- END OF WARNING -----------------------
        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        # print('Rx1', np.shape(Rx1), Rx1*jxw)
        # print('Rx2', np.shape(Rx2), Rx2*jxw)
        # print('Ry1', np.shape(Ry1), Ry1*jxw)
        # print('Ry2', np.shape(Ry2), Ry2*jxw)
        Rx = jxw * (Rx1 + Rx2)
        Ry = jxw * (Ry1 + Ry2)

        if pflag: print('jxw', jxw)
        if pflag: print('elem_x2_hbar_gp1', elem_x2_hbar_gp1)
        # if pflag: print(N[0:1, :])

        # element level residual for traction in either x or y direction
        Rx = tf.reshape(Rx, [-1, new_shape[0], new_shape[1], 2])
        Ry = tf.reshape(Ry, [-1, new_shape[0], new_shape[1], 2])

        if pflag: print('Rx1', np.shape(Rx), Rx[0,:,:,0])
        if pflag: print('Rx2', np.shape(Rx), Rx[0,:,:,1])
        if pflag: print('Ry1', np.shape(Ry), Ry[0,:,:,0])
        if pflag: print('Ry2', np.shape(Ry), Ry[0,:,:,1])

    elif dof == 2:
        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2_1, N[0,:]) 
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2_1, N[1,:]) 
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2_1, N[0,:]) 
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2_1, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])
        if pflag: print('elem_x2_hbar_gp1', np.shape(elem_x2_hbar_gp1),tf.reshape(elem_x2_hbar_gp1, new_shape))
        if pflag: print('elem_x2_hbar_gp2', np.shape(elem_x2_hbar_gp2),tf.reshape(elem_x2_hbar_gp2, new_shape))
        if pflag: print('elem_y2_hbar_gp1', np.shape(elem_y2_hbar_gp1),tf.reshape(elem_y2_hbar_gp1, new_shape))
        if pflag: print('elem_y2_hbar_gp2', np.shape(elem_y2_hbar_gp2),tf.reshape(elem_y2_hbar_gp2, new_shape))

        if pflag: print("N1", N[0,:])
        if pflag: print("N2", N[1,:])

        #-------------------- WARNING --------------------------
        # Since here we start to distinguish x-, y- traction/flux, if the residual contains
        # the gradient term, then we can use different B function for either x-direction
        # or reversed y-direction as the operator to calculate the residual on the edge.
        # For now, we are good.
        #
        #---------------- END OF WARNING -----------------------
        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        Rx_1 = jxw * (Rx1 + Rx2)
        Ry_1 = jxw * (Ry1 + Ry2)

        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2_2, N[0,:]) 
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2_2, N[1,:]) 
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2_2, N[0,:]) 
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2_2, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])

        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        Rx_2 = jxw * (Rx1 + Rx2)
        Ry_2 = jxw * (Ry1 + Ry2)

        # element level residual for traction in either x or y direction
        Rx_1 = tf.reshape(Rx_1, [-1, new_shape[0], new_shape[1], 2])
        Ry_1 = tf.reshape(Ry_1, [-1, new_shape[0], new_shape[1], 2])
        Rx_2 = tf.reshape(Rx_2, [-1, new_shape[0], new_shape[1], 2])
        Ry_2 = tf.reshape(Ry_2, [-1, new_shape[0], new_shape[1], 2])

        if pflag: print('Rx1_1', np.shape(Rx_1), Rx_1[0,:,:,0])
        if pflag: print('Rx2_1', np.shape(Rx_1), Rx_1[0,:,:,1])
        if pflag: print('Ry1_1', np.shape(Ry_1), Ry_1[0,:,:,0])
        if pflag: print('Ry2_1', np.shape(Ry_1), Ry_1[0,:,:,1])
        if pflag: print('Rx1_2', np.shape(Rx_2), Rx_2[0,:,:,0])
        if pflag: print('Rx2_2', np.shape(Rx_2), Rx_2[0,:,:,1])
        if pflag: print('Ry1_2', np.shape(Ry_2), Ry_2[0,:,:,0])
        if pflag: print('Ry2_2', np.shape(Ry_2), Ry_2[0,:,:,1])
    elif dof == 3:
        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2_1, N[0,:]) 
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2_1, N[1,:]) 
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2_1, N[0,:]) 
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2_1, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])
        if pflag: print('elem_x2_hbar_gp1', np.shape(elem_x2_hbar_gp1),tf.reshape(elem_x2_hbar_gp1, new_shape))
        if pflag: print('elem_x2_hbar_gp2', np.shape(elem_x2_hbar_gp2),tf.reshape(elem_x2_hbar_gp2, new_shape))
        if pflag: print('elem_y2_hbar_gp1', np.shape(elem_y2_hbar_gp1),tf.reshape(elem_y2_hbar_gp1, new_shape))
        if pflag: print('elem_y2_hbar_gp2', np.shape(elem_y2_hbar_gp2),tf.reshape(elem_y2_hbar_gp2, new_shape))

        if pflag: print("N1", N[0,:])
        if pflag: print("N2", N[1,:])

        #-------------------- WARNING --------------------------
        # Since here we start to distinguish x-, y- traction/flux, if the residual contains
        # the gradient term, then we can use different B function for either x-direction
        # or reversed y-direction as the operator to calculate the residual on the edge.
        # For now, we are good.
        #
        #---------------- END OF WARNING -----------------------
        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        Rx_1 = jxw * (Rx1 + Rx2)
        Ry_1 = jxw * (Ry1 + Ry2)

        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2_2, N[0,:]) 
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2_2, N[1,:]) 
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2_2, N[0,:]) 
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2_2, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])

        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        Rx_2 = jxw * (Rx1 + Rx2)
        Ry_2 = jxw * (Ry1 + Ry2)

        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2_3, N[0,:]) 
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2_3, N[1,:]) 
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2_3, N[0,:]) 
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2_3, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])

        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        Rx_3 = jxw * (Rx1 + Rx2)
        Ry_3 = jxw * (Ry1 + Ry2)


        # element level residual for traction in either x or y direction
        Rx_1 = tf.reshape(Rx_1, [-1, new_shape[0], new_shape[1], 2])
        Ry_1 = tf.reshape(Ry_1, [-1, new_shape[0], new_shape[1], 2])
        Rx_2 = tf.reshape(Rx_2, [-1, new_shape[0], new_shape[1], 2])
        Ry_2 = tf.reshape(Ry_2, [-1, new_shape[0], new_shape[1], 2])
        Rx_3 = tf.reshape(Rx_3, [-1, new_shape[0], new_shape[1], 2])
        Ry_3 = tf.reshape(Ry_3, [-1, new_shape[0], new_shape[1], 2])

        if pflag: print('Rx1_1', np.shape(Rx_1), Rx_1[0,:,:,0])
        if pflag: print('Rx2_1', np.shape(Rx_1), Rx_1[0,:,:,1])
        if pflag: print('Ry1_1', np.shape(Ry_1), Ry_1[0,:,:,0])
        if pflag: print('Ry2_1', np.shape(Ry_1), Ry_1[0,:,:,1])
        if pflag: print('Rx1_2', np.shape(Rx_2), Rx_2[0,:,:,0])
        if pflag: print('Rx2_2', np.shape(Rx_2), Rx_2[0,:,:,1])
        if pflag: print('Ry1_2', np.shape(Ry_2), Ry_2[0,:,:,0])
        if pflag: print('Ry2_2', np.shape(Ry_2), Ry_2[0,:,:,1])
        if pflag: print('Rx1_3', np.shape(Rx_3), Rx_3[0,:,:,0])
        if pflag: print('Rx2_3', np.shape(Rx_3), Rx_3[0,:,:,1])
        if pflag: print('Ry1_3', np.shape(Ry_3), Ry_3[0,:,:,0])
        if pflag: print('Ry2_3', np.shape(Ry_3), Ry_3[0,:,:,1])
    elif dof == 4:
        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2_1, N[0,:]) 
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2_1, N[1,:]) 
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2_1, N[0,:]) 
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2_1, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])

        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        Rx_1 = jxw * (Rx1 + Rx2)
        Ry_1 = jxw * (Ry1 + Ry2)

        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2_2, N[0,:]) 
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2_2, N[1,:]) 
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2_2, N[0,:]) 
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2_2, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])

        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        Rx_2 = jxw * (Rx1 + Rx2)
        Ry_2 = jxw * (Ry1 + Ry2)

        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2_3, N[0,:]) 
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2_3, N[1,:]) 
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2_3, N[0,:]) 
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2_3, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])

        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        Rx_3 = jxw * (Rx1 + Rx2)
        Ry_3 = jxw * (Ry1 + Ry2)

        elem_x2_hbar_gp1 = tf.linalg.matvec(elem_x2_4, N[0,:]) 
        elem_x2_hbar_gp2 = tf.linalg.matvec(elem_x2_4, N[1,:]) 
        elem_y2_hbar_gp1 = tf.linalg.matvec(elem_y2_4, N[0,:]) 
        elem_y2_hbar_gp2 = tf.linalg.matvec(elem_y2_4, N[1,:]) 

        elem_x2_hbar_gp1 = tf.reshape(elem_x2_hbar_gp1,[-1, 1])
        elem_x2_hbar_gp2 = tf.reshape(elem_x2_hbar_gp2,[-1, 1])
        elem_y2_hbar_gp1 = tf.reshape(elem_y2_hbar_gp1,[-1, 1])
        elem_y2_hbar_gp2 = tf.reshape(elem_y2_hbar_gp2,[-1, 1])

        Rx1 = tf.matmul(elem_x2_hbar_gp1, N[0:1,:]) 
        Rx2 = tf.matmul(elem_x2_hbar_gp2, N[1:2,:]) 
        Ry1 = tf.matmul(elem_y2_hbar_gp1, N[0:1,:]) 
        Ry2 = tf.matmul(elem_y2_hbar_gp2, N[1:2,:]) 

        Rx_4 = jxw * (Rx1 + Rx2)
        Ry_4 = jxw * (Ry1 + Ry2)

        # element level residual for traction in either x or y direction
        Rx_1 = tf.reshape(Rx_1, [-1, new_shape[0], new_shape[1], 2])
        Ry_1 = tf.reshape(Ry_1, [-1, new_shape[0], new_shape[1], 2])
        Rx_2 = tf.reshape(Rx_2, [-1, new_shape[0], new_shape[1], 2])
        Ry_2 = tf.reshape(Ry_2, [-1, new_shape[0], new_shape[1], 2])
        Rx_3 = tf.reshape(Rx_3, [-1, new_shape[0], new_shape[1], 2])
        Ry_3 = tf.reshape(Ry_3, [-1, new_shape[0], new_shape[1], 2])
        Rx_4 = tf.reshape(Rx_4, [-1, new_shape[0], new_shape[1], 2])
        Ry_4 = tf.reshape(Ry_4, [-1, new_shape[0], new_shape[1], 2])



    if dof == 1:
        c_x1 = Rx[:,:,:,0:1]
        c_x2 = tf.roll(Rx[:,:,:,1:2], [1], [1])

        # on 2020-07-16, was not sure, why this is not shift to the row axis to get nodal value. Right now, it's still nodal information 

        c_y1 = Ry[:,:,:,0:1]
        c_y2 = tf.roll(Ry[:,:,:,1:2], [1], [2])
        if pflag: print('Rx 1 (before): ', Rx[0,:,:,0])
        if pflag: print('Rx 1 (after ): ', c_x1[0,:,:,0])
        if pflag: print('Rx 2 (before): ', Rx[0,:,:,1])
        if pflag: print('Rx 2 (after ): ', c_x2[0,:,:,0])

        if pflag: print('Ry 1 (before): ', Ry[0,:,:,0])
        if pflag: print('Ry 1 (after ): ', c_y1[0,:,:,0])
        if pflag: print('Ry 2 (before): ', Ry[0,:,:,1])
        if pflag: print('Ry 2 (after ): ', c_y2[0,:,:,0])

        Rx = c_x1 + c_x2
        Ry = c_y1 + c_y2
        # Add on 2020-07-16. Note on 2020-07-17, not working well
        # Rx = tf.roll(Rx[:,:,:,0:1], [1], [2])
        # Ry = tf.roll(Ry[:,:,:,0:1], [1], [1])
        #---------------------
        if pflag: print('Pay attention to potential errors here')
        if pflag: print('Rx : ', Rx[0,:,:,0])
        if pflag: print('Ry : ', Ry[0,:,:,0])

    elif dof == 2:
        c_x1 = Rx_1[:,:,:,0:1]
        c_x2 = tf.roll(Rx_1[:,:,:,1:2], [1], [1])

        c_y1 = Ry_1[:,:,:,0:1]
        c_y2 = tf.roll(Ry_1[:,:,:,1:2], [1], [2])

        Rx_1 = c_x1 + c_x2
        Ry_1 = c_y1 + c_y2
        if pflag: print('Rx_1 : ', Rx_1[0,:,:,0])
        if pflag: print('Ry_1 : ', Ry_1[0,:,:,0])

        c_x1 = Rx_2[:,:,:,0:1]
        c_x2 = tf.roll(Rx_2[:,:,:,1:2], [1], [1])

        c_y1 = Ry_2[:,:,:,0:1]
        c_y2 = tf.roll(Ry_2[:,:,:,1:2], [1], [2])

        Rx_2 = c_x1 + c_x2
        Ry_2 = c_y1 + c_y2
        if pflag: print('Rx_2 : ', Rx_2[0,:,:,0])
        if pflag: print('Ry_2 : ', Ry_2[0,:,:,0])
        if pflag: print('Pay attention to potential errors here')

    elif dof == 3:
        c_x1 = Rx_1[:,:,:,0:1]
        c_x2 = tf.roll(Rx_1[:,:,:,1:2], [1], [1])

        c_y1 = Ry_1[:,:,:,0:1]
        c_y2 = tf.roll(Ry_1[:,:,:,1:2], [1], [2])

        Rx_1 = c_x1 + c_x2
        Ry_1 = c_y1 + c_y2
        if pflag: print('Rx_1 : ', Rx_1[0,:,:,0])
        if pflag: print('Ry_1 : ', Ry_1[0,:,:,0])

        c_x1 = Rx_2[:,:,:,0:1]
        c_x2 = tf.roll(Rx_2[:,:,:,1:2], [1], [1])

        c_y1 = Ry_2[:,:,:,0:1]
        c_y2 = tf.roll(Ry_2[:,:,:,1:2], [1], [2])

        Rx_2 = c_x1 + c_x2
        Ry_2 = c_y1 + c_y2
        if pflag: print('Rx_2 : ', Rx_2[0,:,:,0])
        if pflag: print('Ry_2 : ', Ry_2[0,:,:,0])
        if pflag: print('Pay attention to potential errors here')

        c_x1 = Rx_3[:,:,:,0:1]
        c_x2 = tf.roll(Rx_3[:,:,:,1:2], [1], [1])

        c_y1 = Ry_3[:,:,:,0:1]
        c_y2 = tf.roll(Ry_3[:,:,:,1:2], [1], [2])

        Rx_3 = c_x1 + c_x2
        Ry_3 = c_y1 + c_y2
        if pflag: print('Rx_3 : ', Rx_3[0,:,:,0])
        if pflag: print('Ry_3 : ', Ry_3[0,:,:,0])
        if pflag: print('Pay attention to potential errors here')
    elif dof == 4:
        c_x1 = Rx_1[:,:,:,0:1]
        c_x2 = tf.roll(Rx_1[:,:,:,1:2], [1], [1])

        c_y1 = Ry_1[:,:,:,0:1]
        c_y2 = tf.roll(Ry_1[:,:,:,1:2], [1], [2])

        Rx_1 = c_x1 + c_x2
        Ry_1 = c_y1 + c_y2

        c_x1 = Rx_2[:,:,:,0:1]
        c_x2 = tf.roll(Rx_2[:,:,:,1:2], [1], [1])

        c_y1 = Ry_2[:,:,:,0:1]
        c_y2 = tf.roll(Ry_2[:,:,:,1:2], [1], [2])

        Rx_2 = c_x1 + c_x2
        Ry_2 = c_y1 + c_y2

        c_x1 = Rx_3[:,:,:,0:1]
        c_x2 = tf.roll(Rx_3[:,:,:,1:2], [1], [1])

        c_y1 = Ry_3[:,:,:,0:1]
        c_y2 = tf.roll(Ry_3[:,:,:,1:2], [1], [2])

        Rx_3 = c_x1 + c_x2
        Ry_3 = c_y1 + c_y2

        c_x1 = Rx_4[:,:,:,0:1]
        c_x2 = tf.roll(Rx_4[:,:,:,1:2], [1], [1])

        c_y1 = Ry_4[:,:,:,0:1]
        c_y2 = tf.roll(Ry_4[:,:,:,1:2], [1], [2])

        Rx_4 = c_x1 + c_x2
        Ry_4 = c_y1 + c_y2


    if dof == 1 :
        #----------------- first version of implementation ----------- 
        # works fine for large dataset problem because of mask contains both x- and y- direction component. However, 
        # the 2nd implementation is less error-prone, in case channel has value, channel 1 remains empty, then
        # mask can mask out everything.
        # R = Rx + Ry
        # R = tf.multiply(R, neumann_mask[:,:,:,0:1])
        #-------------------------------------------------------------

        R = tf.multiply(Rx, neumann_mask[:,:,:,0:1]) + tf.multiply(Ry, neumann_mask[:,:,:,1:2])
        if pflag: print('R: ', np.shape(R), R[0,:,:,0])
    elif dof == 2:
        print("Not fully tested! Exit... Please test this part to enable the code")
        exit(0)
        R_1 = tf.multiply(Rx_1, neumann_mask[:,:,:,0:1]) + tf.multiply(Ry_1, neumann_mask[:,:,:,1:2]) 
        if pflag: print('R_1: ', np.shape(R_1), R_1[0,:,:,0])
        R_2 = tf.multiply(Rx_2, neumann_mask[:,:,:,2:3]) + tf.multiply(Ry_2, neumann_mask[:,:,:,3:4])  
        if pflag: print('R_2: ', np.shape(R_2), R_2[0,:,:,0])
        R = tf.concat([R_1, R_2], axis=3)
        if pflag: print('R: ', np.shape(R), R[0,:,:,0], R[0,:,:,1])
    elif dof == 3:
        print("Not fully tested! Exit... Please test this part to enable the code")
        exit(0)
        R_1 = tf.multiply(Rx_1, neumann_mask[:,:,:,0:1]) + tf.multiply(Ry_1, neumann_mask[:,:,:,1:2]) 
        if pflag: print('R_1: ', np.shape(R_1), R_1[0,:,:,0])
        R_2 = tf.multiply(Rx_2, neumann_mask[:,:,:,2:3]) + tf.multiply(Ry_2, neumann_mask[:,:,:,3:4]) 
        if pflag: print('R_2: ', np.shape(R_2), R_2[0,:,:,0])
        R_3 = tf.multiply(Rx_3, neumann_mask[:,:,:,4:5]) + tf.multiply(Ry_3, neumann_mask[:,:,:,5:6]) 
        if pflag: print('R_3: ', np.shape(R_3), R_3[0,:,:,0])

        R = tf.concat([R_1, R_2, R_3], axis=3)
        if pflag: print('R: ', np.shape(R), R[0,:,:,0], R[0,:,:,1], R[0,:,:,2])
    elif dof == 4:
        print("Not fully tested! Exit... Please test this part to enable the code")
        exit(0)
        R_1 = tf.multiply(Rx_1, neumann_mask[:,:,:,0:1]) + tf.multiply(Ry_1, neumann_mask[:,:,:,1:2]) 
        R_2 = tf.multiply(Rx_2, neumann_mask[:,:,:,2:3]) + tf.multiply(Ry_2, neumann_mask[:,:,:,3:4]) 
        R_3 = tf.multiply(Rx_3, neumann_mask[:,:,:,4:5]) + tf.multiply(Ry_3, neumann_mask[:,:,:,5:6]) 
        R_4 = tf.multiply(Rx_4, neumann_mask[:,:,:,6:7]) + tf.multiply(Ry_4, neumann_mask[:,:,:,7:8]) 
        R = tf.concat([R_1, R_2, R_3, R_4], axis=3)
        # R = tf.multiply(R, neumann_mask) # remove other edge left R due to conv operation: do test edge (1,3) and (2,4)
        if pflag: print('R: ', np.shape(R), R[0,:,:,0], R[0,:,:,1], R[0,:,:,2], R[0,:,:,3])

    # exit(0)

    return R


def Get1DGaussPointInfo(dh=1.0, GPs=2, dof=1):
    """ 
    args:
        dh (float): element size
        GPs (int): total Gauss point number 
        dof (int): dof per node

    return:
        - shape function (numpy array) with size of [GPs, Nodes=2]
        - gradient shape function (numpy array) [None] Not implemented.
        - weight per gauss point (float scalar)

    todo:
        make this function to work with (1S, 1V), 2S, 1V1S, 3S, 2V, etc.
    """

    # print ("For 1D gauss point, the flip of the index for the y-axis was not tested. Be extremely careful if B is used. But for Neumann BCs, it might be just fine as the order of the node does not matter that much.")
    if GPs == 2 :
        #
        if dof == 1:
            # N (gp=2,nodes*dofs=2)
            N = tf.cast(
                    np.array(
                        [
                            # the shape function value should not be changed
                            # . x x .  . 1 2 .
                            # the closer one get value of 0.788, the further one get value of 0.211
                            [0.7886751345948129, 0.2113248654051871,], #GP1, [N1,N2]
                            [0.2113248654051871, 0.7886751345948129,], #GP2, [N1,N2]
                        ]
                        ),
            tf.float32)

            # B is disabled as the y-axis was not tested. And it is not clear how to make one B to work for 
            # both x-axis and y-axis, as it's a 1D GPs rule.
            # B = tf.cast(
                # np.array([
                    # [
                        # # GP1
                        # [0.7886751345948129], # N1, coor 3
                        # [0.2113248654051871], # N2, coor 4
                    # ],
                    # [
                        # # GP2
                        # [0.2113248654051871], # N1, coor 3
                        # [0.7886751345948129], # N2, coor 4
                    # ],
                # ]),
                # tf.float32) / dh 
        elif dof == 2:
            # N (gp=2,nodes*dofs=4)
            raise ValueError("Please disable this Error. Face GPs in is not tested for dof=2, be careful here!")
            N = tf.cast(
                    np.array(
                        [
                            [0.7886751345948129, 0.7886751345948129, 0.2113248654051871, 0.2113248654051871,], #GP1, [N1,N2]
                            [0.2113248654051871, 0.2113248654051871, 0.7886751345948129, 0.7886751345948129,], #GP2, [N1,N2]
                        ]
                        ),
            tf.float32)

            # B = tf.cast(
                # np.array([
                    # [
                        # # GP1
                        # [0.7886751345948129, 0], # N1, coor 3
                        # [0, 0.7886751345948129], # N1, coor 3
                        # [0.2113248654051871, 0], # N2, coor 4
                        # [0, 0.2113248654051871], # N2, coor 4
                    # ],
                    # [
                        # # GP2
                        # [0.2113248654051871, 0], # N1, coor 3
                        # [0, 0.2113248654051871], # N1, coor 3
                        # [0.7886751345948129, 0], # N2, coor 4
                        # [0, 0.7886751345948129], # N2, coor 4
                    # ],
                # ]),
                # tf.float32) / dh 
        else :
            raise ValueError("dof = ", dof, " is not implemented!")

        B = None

        jxw = dh*0.5
        # print('N: ', np.shape(N), '(q,n)')
        # print('q=0, N: ', N[0,:])
        # print('q=1, N: ', N[1,:])
        # print('q=2, N: ', N[2,:])
        # print('q=3, N: ', N[3,:])
        # print('B: ', np.shape(B), '(q,n,x)' )
        # print('q=0 B: ', B[0,:,:])
        # print('q=1 B: ', B[1,:,:])
        # print('q=2 B: ', B[2,:,:])
        # print('q=3 B: ', B[3,:,:])
        # print('jxw: ', jxw)

        return N, B, jxw
    else:
        raise ValueError("Only GPs == 2 is implemented, please choose a different GPs!", GPs)


def Get2DGaussPointInfo(dh=1.0, GPs=4, dof=1):
    """ 
    args:
        dh (float): element size
        GPs (int): total Gauss point number 
        dof (int): dof per node

    return:
        - shape function (numpy array) with size of [GPs, Nodes=4*dof]
        - gradient shape function (numpy array) [GPs, Nodes=4*dof, dim=2*dof] last dim: dof=1: [dc/dx, dc/dy] dof=2: [dx/dx, dx/dy, dy/dx, dy/dy]
        - weight per gauss point (float scalar)

    todo:
        make this function to work with (1S, 1V), 2S, 1V1S, 3S, 2V, etc.
    """
    if GPs == 4 :
        #
        if dof == 1:
            # N (gp=4,nodes*dofs=4)

            # check reshape [1,2,3,4] to (2,2)
            # check reshape [[1,2],[3,4]] to (4)
            N = tf.cast(
                    np.array(
                        [
                            # the shape function value should not be changed
                            # .      . .      .
                            #   x  x     1  2
                            #   x  x     3  4
                            # .      . .      .
                            # the closer one get value of 0.622, the further one get value of 0.044
                            #GP1, [N3,N4,N1,N2]
                            [0.1666666666666667, 0.04465819873852046, 0.6220084679281462, 0.1666666666666667,], 
                            # [0.6220084679281462, 0.1666666666666667, 0.1666666666666667, 0.04465819873852046,], #GP1, [N1,N2,N3,N4]

                            #GP2, [N3,N4,N1,N2]
                            [0.04465819873852046, 0.1666666666666667, 0.1666666666666667, 0.6220084679281462,], 
                            # [0.1666666666666667, 0.6220084679281462, 0.04465819873852046, 0.1666666666666667,], #GP2, [N1,N2,N3,N4]

                            #GP3, [N3,N4,N1,N2]
                            [0.6220084679281462, 0.1666666666666667, 0.1666666666666667, 0.04465819873852046,], 
                            # [0.1666666666666667, 0.04465819873852046, 0.6220084679281462, 0.1666666666666667,], #GP3, [N1,N2,N3,N4]

                            #GP4, [N3,N4,N1,N2]
                            [0.1666666666666667, 0.6220084679281462, 0.04465819873852046, 0.1666666666666667,],
                            # [0.04465819873852046, 0.1666666666666667, 0.1666666666666667, 0.6220084679281462,], #GP4, [N1,N2,N3,N4]

                        ]
                        ),
            tf.float32)

            B = tf.cast(
                np.array([
                    [
                        # GP1
                        [-0.2113248654051871, 0.7886751345948129],  # N3, coor 1
                        [0.2113248654051871, 0.2113248654051871],   # N4, coor 2
                        [-0.7886751345948129, -0.7886751345948129], # N1, coor 3
                        [0.7886751345948129, -0.2113248654051871],  # N2, coor 4
                    ],
                    [
                        # GP2
                        [-0.2113248654051871, 0.2113248654051871],  # N3, coor 1
                        [0.2113248654051871, 0.7886751345948129],   # N4, coor 2
                        [-0.7886751345948129, -0.2113248654051871], # N1, coor 3
                        [0.7886751345948129, -0.7886751345948129],  # N2, coor 4
                    ],
                    [
                        # GP3
                        [-0.7886751345948129, 0.7886751345948129],  # N3, coor 1
                        [0.7886751345948129, 0.2113248654051871],   # N4, coor 2
                        [-0.2113248654051871, -0.7886751345948129], # N1, coor 3
                        [0.2113248654051871, -0.2113248654051871],  # N2, coor 4
                    ],
                    [
                        # GP4
                        [-0.7886751345948129, 0.2113248654051871],  # N3, coor 1
                        [0.7886751345948129, 0.7886751345948129],   # N4, coor 2
                        [-0.2113248654051871, -0.2113248654051871], # N1, coor 3
                        [0.2113248654051871, -0.7886751345948129],  # N2, coor 4
                    ]
                ]),
                tf.float32) / dh 
        elif dof == 2:
            # N (gp=4,nodes*dofs=8)
            N = tf.cast(
                    np.array(
                        [
                            #GP1, [N3,N4,N1,N2]
                            [0.1666666666666667, 0.1666666666666667,  0.04465819873852046,0.04465819873852046, 0.6220084679281462,  0.6220084679281462,  0.1666666666666667,  0.1666666666666667, ], 
                            #[0.6220084679281462, 0.6220084679281462,  0.1666666666666667, 0.1666666666666667,  0.1666666666666667,  0.1666666666666667,  0.04465819873852046, 0.04465819873852046,], #GP1, [N1,N2,N3,N4]

                            #GP2, [N3,N4,N1,N2]
                            [0.04465819873852046,0.04465819873852046, 0.1666666666666667, 0.1666666666666667,  0.1666666666666667,  0.1666666666666667,  0.6220084679281462,  0.6220084679281462, ], 
                            #[0.1666666666666667, 0.1666666666666667,  0.6220084679281462, 0.6220084679281462,  0.04465819873852046, 0.04465819873852046, 0.1666666666666667,  0.1666666666666667, ], #GP2, [N1,N2,N3,N4]

                            #GP3, [N3,N4,N1,N2]
                            [0.6220084679281462, 0.6220084679281462,  0.1666666666666667, 0.1666666666666667,  0.1666666666666667,  0.1666666666666667,  0.04465819873852046, 0.04465819873852046,], 
                            #[0.1666666666666667, 0.1666666666666667,  0.04465819873852046,0.04465819873852046, 0.6220084679281462,  0.6220084679281462,  0.1666666666666667,  0.1666666666666667, ], #GP3, [N1,N2,N3,N4]

                            #GP4, [N3,N4,N1,N2]
                            [0.1666666666666667, 0.1666666666666667,  0.6220084679281462, 0.6220084679281462,  0.04465819873852046, 0.04465819873852046, 0.1666666666666667,  0.1666666666666667, ], 
                            #[0.04465819873852046,0.04465819873852046, 0.1666666666666667, 0.1666666666666667,  0.1666666666666667,  0.1666666666666667,  0.6220084679281462,  0.6220084679281462, ], #GP4, [N1,N2,N3,N4]
                        ]
                        ),
            tf.float32)

            B = tf.cast(
                np.array([
                    [
                        # GP1
                        [-0.2113248654051871, 0.7886751345948129, 0, 0],
                        [0, 0, -0.2113248654051871, 0.7886751345948129],  # N3, coor 1
                        [0.2113248654051871, 0.2113248654051871, 0, 0],
                        [0, 0, 0.2113248654051871, 0.2113248654051871],   # N4, coor 2
                        [-0.7886751345948129, -0.7886751345948129, 0, 0],
                        [0, 0, -0.7886751345948129, -0.7886751345948129], # N1, coor 3
                        [0.7886751345948129, -0.2113248654051871, 0, 0],
                        [0, 0, 0.7886751345948129, -0.2113248654051871],  # N2, coor 4
                    ],
                    [
                        # GP2
                        [-0.2113248654051871, 0.2113248654051871, 0, 0],
                        [0, 0, -0.2113248654051871, 0.2113248654051871],  # N3, coor 1
                        [0.2113248654051871, 0.7886751345948129, 0, 0],
                        [0, 0, 0.2113248654051871, 0.7886751345948129],   # N4, coor 2
                        [-0.7886751345948129, -0.2113248654051871, 0, 0],
                        [0, 0, -0.7886751345948129, -0.2113248654051871], # N1, coor 3
                        [0.7886751345948129, -0.7886751345948129, 0, 0],
                        [0, 0, 0.7886751345948129, -0.7886751345948129],  # N2, coor 4
                    ],
                    [
                        # GP3
                        [-0.7886751345948129, 0.7886751345948129, 0, 0],
                        [0, 0, -0.7886751345948129, 0.7886751345948129],  # N3, coor 1
                        [0.7886751345948129, 0.2113248654051871, 0, 0],
                        [0, 0, 0.7886751345948129, 0.2113248654051871],   # N4, coor 2
                        [-0.2113248654051871, -0.7886751345948129, 0, 0],
                        [0, 0, -0.2113248654051871, -0.7886751345948129], # N1, coor 3
                        [0.2113248654051871, -0.2113248654051871, 0, 0],
                        [0, 0, 0.2113248654051871, -0.2113248654051871],  # N2, coor 4
                    ],
                    [
                        # GP4
                        [-0.7886751345948129, 0.2113248654051871, 0, 0],
                        [0, 0, -0.7886751345948129, 0.2113248654051871],  # N3, coor 1
                        [0.7886751345948129, 0.7886751345948129, 0, 0],
                        [0, 0, 0.7886751345948129, 0.7886751345948129],   # N4, coor 2
                        [-0.2113248654051871, -0.2113248654051871, 0, 0],
                        [0, 0, -0.2113248654051871, -0.2113248654051871], # N1, coor 3
                        [0.2113248654051871, -0.7886751345948129, 0, 0],
                        [0, 0, 0.2113248654051871, -0.7886751345948129],  # N2, coor 4
                    ]
                ]),
                tf.float32) / dh 
        else :
            raise ValueError("dof = ", dof, " is not implemented!")

        jxw = dh*dh*0.25
        # print('N: ', np.shape(N), '(q,n)')
        # print('q=0, N: ', N[0,:])
        # print('q=1, N: ', N[1,:])
        # print('q=2, N: ', N[2,:])
        # print('q=3, N: ', N[3,:])
        # print('B: ', np.shape(B), '(q,n,x)' )
        # print('q=0 B: ', B[0,:,:])
        # print('q=1 B: ', B[1,:,:])
        # print('q=2 B: ', B[2,:,:])
        # print('q=3 B: ', B[3,:,:])
        # print('jxw: ', jxw)

        return N, B, jxw
    else:
        raise ValueError("Only GPs == 4 is implemented, please choose a different GPs!", GPs)


def GetNodalInfoFromElementInfo(data, residual_mask, dof, padding='SAME'):
    """ 
    reorganize data from a matrix form with 4 nodal values of elements to nodal values

    Args:
        data (numpy array/tensor): [None, elem_height, elem_width, 4*dof] (4 nodal values for 1 dof)
        residual_mask (numpy_array):  [None, elem_height, elem_width, 1] 
        dof (int): dof per node

    return:
        numpy array: output with size of [None, node_height, node_width, dof]

    todo:
        make this function to work with (1S, 1V), 2S, 1V1S, 3S, 2V, etc.
    """
    # tf.roll( input, shift, axis, name=None)
    # 't' is [0, 1, 2, 3, 4]
    # roll(t, shift=2, axis=0) ==> [3, 4, 0, 1, 2]
    
    # shifting along multiple dimensions
    # 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]
    # roll(t, shift=[1, -2], axis=[0, 1]) ==> [[7, 8, 9, 5, 6], [2, 3, 4, 0, 1]]
    
    # shifting along the same axis multiple times
    # 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]
    # roll(t, shift=[2, -3], axis=[1, 1]) ==> [[1, 2, 3, 4, 0], [6, 7, 8, 9, 5]]

    pflag = False
    if dof == 1:
        # data = tf.convert_to_tensor(data, dtype=tf.float32)
        data = tf.multiply(data, residual_mask)

        if pflag: print('data', np.shape(data))

        c_n1 = data[:,:,:,0:1]
        c_n2 = tf.roll(data[:,:,:,1:2], [1], [2])
        c_n3 = tf.roll(data[:,:,:,2:3], [1], [1])
        c_n4 = tf.roll(data[:,:,:,3:4], [1,1], [1,2])
        # print('data 1 (before): ', data[0,:,:,0])
        # print('data 1 (after ): ', c_n1[0,:,:,0])
        # print('data 2 (before): ', data[0,:,:,1])
        # print('data 2 (after ): ', c_n2[0,:,:,0])
        # print('data 3 (before): ', data[0,:,:,2])
        # print('data 3 (after ): ', c_n3[0,:,:,0])
        # print('data 4 (before): ', data[0,:,:,3])
        # print('data 4 (after ): ', c_n4[0,:,:,0])

        nodal_c = tf.concat([c_n1, c_n2, c_n3, c_n4], 3)

        nodal_val = tf.reduce_sum(nodal_c, axis=3, keepdims=True )
    elif dof == 2:
        # data = tf.convert_to_tensor(data, dtype=tf.float32)
        data = tf.multiply(data, residual_mask)
        if pflag: print('data', np.shape(data))

        x_n1 = data[:,:,:,0:1]
        y_n1 = data[:,:,:,1:2]
        x_n2 = tf.roll(data[:,:,:,2:3], [1], [2])
        y_n2 = tf.roll(data[:,:,:,3:4], [1], [2])
        x_n3 = tf.roll(data[:,:,:,4:5], [1], [1])
        y_n3 = tf.roll(data[:,:,:,5:6], [1], [1])
        x_n4 = tf.roll(data[:,:,:,6:7], [1,1], [1,2])
        y_n4 = tf.roll(data[:,:,:,7:8], [1,1], [1,2])

        if pflag: print('data 1 (before): ', data[0,:,:,0])
        if pflag: print('data 1 (after ): ', x_n1[0,:,:,0])
        if pflag: print('data 2 (before): ', data[0,:,:,1])
        if pflag: print('data 2 (after ): ', y_n1[0,:,:,0])
        if pflag: print('data 3 (before): ', data[0,:,:,2])
        if pflag: print('data 3 (after ): ', x_n2[0,:,:,0])
        if pflag: print('data 4 (before): ', data[0,:,:,3])
        if pflag: print('data 4 (after ): ', y_n2[0,:,:,0])


        nodal_x = tf.concat([x_n1, x_n2, x_n3, x_n4], 3)
        nodal_y = tf.concat([y_n1, y_n2, y_n3, y_n4], 3)
        if pflag: print('nodal_x ', np.shape(nodal_x))
        if pflag: print('nodal_y ', np.shape(nodal_y))
        # nodal_x = tf.expand_dims(nodal_x,3)
        # nodal_y = tf.expand_dims(nodal_y,3)

        nodal_x = tf.reduce_sum(nodal_x, axis=3, keepdims=True )
        nodal_y = tf.reduce_sum(nodal_y, axis=3, keepdims=True )
        nodal_val = tf.concat([nodal_x, nodal_y], 3)
    else:
        # data = tf.convert_to_tensor(data, dtype=tf.float32)
        data = tf.multiply(data, residual_mask)
        if pflag: print('data', np.shape(data))
        # use the above dof=1/2 as example to understand the following
        R_dof = []
        for i0 in range(0, dof):
            x_n1 = data[:,:,:,i0:i0+1]
            x_n2 = tf.roll(data[:,:,:,i0+dof:i0+1+dof], [1], [2])
            x_n3 = tf.roll(data[:,:,:,i0+dof*2:i0+1+2*dof], [1], [1])
            x_n4 = tf.roll(data[:,:,:,i0+dof*3:i0+1+3*dof], [1,1], [1,2])
            nodal_x = tf.concat([x_n1, x_n2, x_n3, x_n4], 3)
            nodal_x = tf.reduce_sum(nodal_x, axis=3, keepdims=True )
            R_dof.append(nodal_x)
        nodal_val = tf.concat(R_dof, 3)
        print ('Nodal value for dof = ', dof, ' is not fully tested yet!')
    if pflag: print('nodal_val ', np.shape(nodal_val))

    return nodal_val



class LayerFillRandomToBCs(tf.keras.layers.Layer):
    """ 
    A customized Keras layer to add random noise to BCs with :math:`\epsilon~\sim` N(0, stddev=0.005).

    Args:
        stddev (float): default = 0.005
    """

    def __init__(self, stddev=0.005, name='fill-random-num'):
        super(LayerFillRandomToBCs, self).__init__(name=name)
        self.stddev = stddev

    def call(self, input):
        output = input + tf.where(input > 0.0, tf.random.normal(tf.shape(input), 0, self.stddev, tf.float32), tf.fill(tf.shape(input), 0.0))
        return output

class LayerFillZeros(tf.keras.layers.Layer):
    """ 
    A customized Keras layer to generate zeros if value == -2.0
    """

    def __init__(self, name='fill-zeros'):
        super(LayerFillZeros, self).__init__(name=name)

    def call(self, input):
        output = input + tf.where( input > -1.5, tf.fill(tf.shape(input), 0.0), tf.fill(tf.shape(input), 2.0))
        return output


class LayerFillRandomNumber(tf.keras.layers.Layer):
    """ 
    A customized Keras layer to generate uniform random data (0, 1) if value == -2.0
    """

    def __init__(self, name='fill-random-num'):
        super(LayerFillRandomNumber, self).__init__(name=name)

    def call(self, input):
        output = input + tf.where(
            input > -1.5, tf.fill(tf.shape(input), 0.0),
            tf.random.uniform(tf.shape(input), minval=0.0, maxval=1.0)) + tf.where(
                input > -1.5, tf.fill(tf.shape(input), 0.0),
                tf.fill(tf.shape(input), 2.0))
        return output


class LayerBulkResidual(tf.keras.layers.Layer):
    """
    General bulk residual
    """
    # data: [batch, in_height, in_width, in_channels]
    # filter: [filter_height, filter_width, in_channels, out_channels]
    # dh is needed.

    def __init__(self, name='R_bulk_general'):
        super(LayerBulkResidual, self).__init__(name=name)

    def initialize_arrays(self):
        """
        Initialize the kernel array to transform nodal arrangement to element arrangement. Get the Gauss Point information.
        """
        self.n1 = np.array([[1, 0], [0, 0]])
        self.n1 = np.expand_dims(self.n1, axis=2)
        self.n1 = np.expand_dims(self.n1, axis=3)

        self.n2 = np.array([[0, 1], [0, 0]])
        self.n2 = np.expand_dims(self.n2, axis=2)
        self.n2 = np.expand_dims(self.n2, axis=3)

        self.n3 = np.array([[0, 0], [1, 0]])
        self.n3 = np.expand_dims(self.n3, axis=2)
        self.n3 = np.expand_dims(self.n3, axis=3)

        self.n4 = np.array([[0, 0], [0, 1]])
        self.n4 = np.expand_dims(self.n4, axis=2)
        self.n4 = np.expand_dims(self.n4, axis=3)

        self.N, self.B, self.jxw = Get2DGaussPointInfo(dh=self.dh, dof=self.dof)

    def GetElementInfo(self, input):
        """ 
        Reorganize data from nodal value to a matrix form with 4*dof nodal values 
        args:
            inputs (tensor): [batch, node_height, node_width, dof]
        return:
            tensor: data with size of [batch, elem_height, elem_width, dof*4] 


        note:
            - filter n1, n2, n3, n4: [filter_height, filter_width, in_channels, out_channels]
        """
        # It is better to stick with the 2x2 or 2x2x2 format, because the matrix form might be
        # much easier for calling the linear algebra operations in tensorflow.

        if self.dof == 1:
            c_n1 = tf.nn.conv2d(input, self.n1, [1,1,1,1], 'SAME' )
            c_n2 = tf.nn.conv2d(input, self.n2, [1,1,1,1], 'SAME' )
            c_n3 = tf.nn.conv2d(input, self.n3, [1,1,1,1], 'SAME' )
            c_n4 = tf.nn.conv2d(input, self.n4, [1,1,1,1], 'SAME' )
            # elem_c
            data = tf.concat([c_n1, c_n2, c_n3, c_n4], 3)

        elif self.dof == 2:
            c_n1x = tf.nn.conv2d(input[:,:,:,0:1], self.n1, [1,1,1,1], 'SAME' )
            c_n2x = tf.nn.conv2d(input[:,:,:,0:1], self.n2, [1,1,1,1], 'SAME' )
            c_n3x = tf.nn.conv2d(input[:,:,:,0:1], self.n3, [1,1,1,1], 'SAME' )
            c_n4x = tf.nn.conv2d(input[:,:,:,0:1], self.n4, [1,1,1,1], 'SAME' )

            c_n1y = tf.nn.conv2d(input[:,:,:,1:2], self.n1, [1,1,1,1], 'SAME' )
            c_n2y = tf.nn.conv2d(input[:,:,:,1:2], self.n2, [1,1,1,1], 'SAME' )
            c_n3y = tf.nn.conv2d(input[:,:,:,1:2], self.n3, [1,1,1,1], 'SAME' )
            c_n4y = tf.nn.conv2d(input[:,:,:,1:2], self.n4, [1,1,1,1], 'SAME' )

            data = tf.concat([c_n1x, c_n1y, c_n2x, c_n2y, c_n3x, c_n3y, c_n4x, c_n4y], 3)
        else:
            raise ValueError('dof = ', self.dof, ' is not implemented')
        return data

    def ComputeValuAtGPs(self, data):
        """
        Reshape data[:, :, :, 4*dof] to [:, 4*dof] and compute the u(unknown) at each GPs.

        args:
            data (tensor): size of [-1, 4*dof]
        return:
            tensor: valu at each GPs with size of [-1, 1*dof]
        """
        data = tf.reshape(data,[-1, 4*self.dof])
        # print(np.shape(data))
        # print(np.shape(self.N[0,:]))

        valu1 = tf.linalg.matvec(data, self.N[0,:])
        valu2 = tf.linalg.matvec(data, self.N[1,:])
        valu3 = tf.linalg.matvec(data, self.N[2,:])
        valu4 = tf.linalg.matvec(data, self.N[3,:])

        valu1 = tf.expand_dims(valu1,1)
        valu2 = tf.expand_dims(valu2,1)
        valu3 = tf.expand_dims(valu3,1)
        valu4 = tf.expand_dims(valu4,1)
        # print(np.shape(valu1))
        return valu1, valu2, valu3, valu4

    def ComputeGraduAtGPs(self, data):
        """
        Reshape data[:, :, :, 4*dof] to [:, 4*dof] and compute the Grad of u(unknown) at each GPs.

        args:
            data (tensor): size of [-1, 4*dof]
        return:
            tensor: gradu at each GPs with size of [-1, 2*dof]
        """
        data = tf.reshape(data,[-1, 4*self.dof])

        # this is du/dX at each GP
        gradu1 = tf.matmul(data, self.B[0,:,:]) 
        gradu2 = tf.matmul(data, self.B[1,:,:]) 
        gradu3 = tf.matmul(data, self.B[2,:,:]) 
        gradu4 = tf.matmul(data, self.B[3,:,:]) 
        return gradu1, gradu2, gradu3, gradu4

    def Get2ndOrderIdentityTensor(self, gradu1, domain_shape):
        """
        Get the second order identity tensor in the format of I_4[-1, 4] and I_2x2[-1, :, :, 4GPs, 2, 2]
        """

        # create 2nd order tensor
        I = np.array([1.0000001,0.0,0.0,0.99999999])
        I = tf.constant(I, tf.float32)
        I = tf.expand_dims(I,0)
        ones = tf.ones_like(gradu1)
        I4 = tf.multiply(ones, I)

        I2x2_1 = tf.reshape(I4, [-1, domain_shape[0], domain_shape[1], 1, 2, 2])
        I2x2 = tf.concat([I2x2_1, I2x2_1, I2x2_1, I2x2_1], 3) # 4 GPs, are the same.

        return I4, I2x2

    def GetFe(self, gradu1, gradu2, gradu3, gradu4, I4, domain_shape, value1, value2, value3, value4):
        """
        Compute Fe for large deformation
        """
        # this is  Fe at each GP: gradu, I4 = [None, 4=2*dof], value1 = [None, 1]
        gradu1 = (gradu1 + I4) / tf.math.pow( (value1+1.0), 1.0/3.0)
        gradu2 = (gradu2 + I4) / tf.math.pow( (value2+1.0), 1.0/3.0)
        gradu3 = (gradu3 + I4) / tf.math.pow( (value3+1.0), 1.0/3.0)
        gradu4 = (gradu4 + I4) / tf.math.pow( (value4+1.0), 1.0/3.0)

        # this is F2x2 at each GP
        gradu1 = tf.reshape(gradu1, [-1, domain_shape[0], domain_shape[1], 1, 2, 2])
        gradu2 = tf.reshape(gradu2, [-1, domain_shape[0], domain_shape[1], 1, 2, 2])
        gradu3 = tf.reshape(gradu3, [-1, domain_shape[0], domain_shape[1], 1, 2, 2])
        gradu4 = tf.reshape(gradu4, [-1, domain_shape[0], domain_shape[1], 1, 2, 2])
        # tensor/matrix form of F
        F2x2 = tf.concat([gradu1, gradu2, gradu3, gradu4], 3)

        return F2x2

    def GetF(self, gradu1, gradu2, gradu3, gradu4, I4, domain_shape):
        """
        Compute F for large deformation
        """
        # this is  F at each GP
        gradu1 = gradu1 + I4
        gradu2 = gradu2 + I4
        gradu3 = gradu3 + I4
        gradu4 = gradu4 + I4

        # this is F2x2 at each GP
        gradu1 = tf.reshape(gradu1, [-1, domain_shape[0], domain_shape[1], 1, 2, 2])
        gradu2 = tf.reshape(gradu2, [-1, domain_shape[0], domain_shape[1], 1, 2, 2])
        gradu3 = tf.reshape(gradu3, [-1, domain_shape[0], domain_shape[1], 1, 2, 2])
        gradu4 = tf.reshape(gradu4, [-1, domain_shape[0], domain_shape[1], 1, 2, 2])
        # tensor/matrix form of F
        F2x2 = tf.concat([gradu1, gradu2, gradu3, gradu4], 3)

        return F2x2

    def GetEpsilon(self, gradu1, gradu2, gradu3, gradu4, domain_shape):
        """
        Compute epsilon for small deformation
        """
        # this is epsilon at each GP
        gradu1 = tf.reshape(gradu1, [-1, domain_shape[0], domain_shape[1], 1, 2, 2])
        gradu2 = tf.reshape(gradu2, [-1, domain_shape[0], domain_shape[1], 1, 2, 2])
        gradu3 = tf.reshape(gradu3, [-1, domain_shape[0], domain_shape[1], 1, 2, 2])
        gradu4 = tf.reshape(gradu4, [-1, domain_shape[0], domain_shape[1], 1, 2, 2])

        # symmetric epsilon
        gradu1 = 0.5 * (gradu1 + tf.transpose(gradu1, perm=[0,1,2,3,5,4])) # + tf.random.uniform(tf.shape(gradu1), maxval=1e-9)
        gradu2 = 0.5 * (gradu2 + tf.transpose(gradu2, perm=[0,1,2,3,5,4])) # + tf.random.uniform(tf.shape(gradu2), maxval=1e-9)
        gradu3 = 0.5 * (gradu3 + tf.transpose(gradu3, perm=[0,1,2,3,5,4])) # + tf.random.uniform(tf.shape(gradu3), maxval=1e-9)
        gradu4 = 0.5 * (gradu4 + tf.transpose(gradu4, perm=[0,1,2,3,5,4])) # + tf.random.uniform(tf.shape(gradu4), maxval=1e-9)

        # # tensor/matrix form of epsilon
        epsilon = tf.concat([gradu1, gradu2, gradu3, gradu4], 3)
        return epsilon

    def ComputeIntTranBxP(self, P1, P2, P3, P4, domain_shape):
        """
        compute int ( B^T * P) dV

        args:
            P# (tensor): with size of [:, 4]
        """

        # B (q, n, x)
        # TransB (q, x, n)
        TransB = tf.transpose(self.B, perm=[0,2,1])

        R1 = tf.matmul(P1, TransB[0,:,:])
        R2 = tf.matmul(P2, TransB[1,:,:])
        R3 = tf.matmul(P3, TransB[2,:,:])
        R4 = tf.matmul(P4, TransB[3,:,:])

        # int ( B^T * P) dV
        R = self.jxw * (R1 + R2 + R3 + R4)

        R = tf.reshape(R, [-1, domain_shape[0], domain_shape[1], 4*self.dof])

        return R

    def ComputeIntTranNxU(self, valu1, valu2, valu3, valu4, domain_shape):
        """
        compute int ( N^T * valu) dV

        args:
            valu# (tensor): with size of [:, 1]
        """

        # N (q, n)
        R1 = tf.matmul(valu1, self.N[0:1,:])
        R2 = tf.matmul(valu2, self.N[1:2,:])
        R3 = tf.matmul(valu3, self.N[2:3,:])
        R4 = tf.matmul(valu4, self.N[3:4,:])

        # int ( N^T * valu) dV
        R = self.jxw * (R1 + R2 + R3 + R4)

        R = tf.reshape(R, [-1, domain_shape[0], domain_shape[1], 4*self.dof])

        return R

    def E_nu_to_lambda_mu(self, E, nu):
        lambda0 = (E*nu)/(1.0+nu)/(1.0-2.0*nu)
        mu0 = E/2.0/(1.0+nu)
        return lambda0, mu0


if __name__ == '__main__' :
    print('testing the main')
    # test the results for matrix that is not invertible
    # F1 = tf.constant([[1.0, 2.0], [3.0, 4.0]])
    # InvF = tf.linalg.inv(F1)
    # print('F', F1, 'InvF', InvF)

    # # lead to tensorflow.python.framework.errors_impl.InvalidArgumentError: Input is not invertible. [Op:MatrixInverse]
    # F2 = tf.constant([[1.0, 0], [0, 0.0]])
    # InvF = tf.linalg.inv(F2)
    # print('F', F2, 'InvF', InvF)

    # F3 = tf.constant([[1.0, 1.0], [1.0, 1.0]])
    # InvF = tf.linalg.inv(F3)
    # print('F', F3, 'InvF', InvF)


    # x = tf.constant([5.0, 4.8, 6.8, np.inf, np.nan])
    # print(tf.math.is_finite(x))

        # # detF_mask_finite = tf.where(detF != detF, tf.fill(tf.shape(detF), 0.0), tf.fill(tf.shape(detF), 1.0))
    # print(tf.where(x != x, tf.fill(tf.shape(x), 0.0), tf.fill(tf.shape(x), 1.0)))
    # print(tf.where(tf.math.is_finite(x), tf.fill(tf.shape(x), 1.0), tf.fill(tf.shape(x), 0.0)))

    # dh = 1.0/4
    # dof = 4
    # features = 0.0 * np.random.rand(1,5,5,8)
    # features[:,:,:,0:1] = -2
    # features[:,:,:,1:2] = -2
    # features[:,:,:,2:3] = -2
    # features[:,:,:,3:4] = -2
    
    # features[:,-1:,:,1:2] = 0.5
    # features[:,-1:,:,2:3] = 0.5
    
    # features[:,0:1,:,4:5] = 0.55
    # features[:,0:1,:,5:6] = 0.70
    # features[:,0:1,:,6:7] = 0.60
    # features[:,0:1,:,7:8] = 0.60

    # ComputeNeumannBoundaryResidualNodalData(features, dh, dof, padding='SAME')
    
    dof = 1
    dh = 1.0/7.0
    features = np.load("np-features-e4-s0-constant-0.npy")
    # print(features)
    ComputeNeumannBoundaryResidualNodalDataNew(features, dh, dof, padding='SAME')



====================================================================================================
mechanoChemML\src\stepwiseRegression.py
====================================================================================================
"""
Zhenlin Wang 2019
"""

import numpy as np
from mechanoChemML.src import LeastR as LR 
#import LeastR as LR 


class stepwiseR(object):
  def __init__(self,F_criteria=[1],F_switch=[],basis_drop_strategy='aggressive',sigma_n=1.0e-20,anchor_index=[-1],alpha_lasso=0,alpha_ridge=0, ridge_cv=[-1],threshold_d=1.0e-14,n_jobs=1):
    self.F_criteria=F_criteria
    self.F_switch=F_switch
    self.sigma_n=sigma_n
    self.anchor_index=anchor_index
    self.alpha_lasso=alpha_lasso
    self.alpha_ridge=alpha_ridge
    self.ridge_cv=ridge_cv
    self.n_jobs=n_jobs
    self.threshold_d=threshold_d
    self.basis_drop_strategy=basis_drop_strategy
    self.last_F=0
    
  def test(self):
    print('test_pass')
    
  def stepwiseR_fit_aggressive(self, theta_matrix, X_matrix):
    _,n_base_orign=theta_matrix.shape
    self.anchor=np.zeros(n_base_orign)
    if self.anchor_index[0]!=-1:
      for key in self.anchor_index:
        self.anchor[key]=1

    self.loss=np.zeros(n_base_orign)
    self.score=np.zeros(n_base_orign)
    self.F_index=np.zeros(n_base_orign)
    self.gamma_matrix=np.zeros((n_base_orign,n_base_orign))
    alpha_sum=self.alpha_lasso+self.alpha_ridge+self.ridge_cv[0]
    threshold_d=self.threshold_d
    self.best_alpha=np.zeros(n_base_orign)
    
    # local_to_global_index
    local_to_global_index=np.arange(n_base_orign)
    F_threshold=self.F_criteria[0]
    
    #########
    #first LS_regression
    #########
    num_column=0
    if(alpha_sum==-1):
        [gamma_vector,self.loss[0]]=LR.fit(theta_matrix,X_matrix)
    if(self.alpha_lasso>0):
        [gamma_vector,self.loss[0]]=LR.fit_lasso(theta_matrix,X_matrix, alpha=self.alpha_lasso)
    if(self.alpha_ridge>0):
        [gamma_vector,self.loss[0],self.score[0]]=LR.fit_ridge(theta_matrix,X_matrix, alpha=self.alpha_ridge)
    if(self.ridge_cv[0]>-0.1):
        [gamma_vector,self.loss[0],self.score[0],self.best_alpha[0]]=LR.fit_ridge_cv(theta_matrix,X_matrix, alpha=self.ridge_cv)
        
    self.gamma_matrix[local_to_global_index,num_column]=gamma_vector
    
    #########
    #stepwise
    #########
    num_column=num_column+1;
    num_canditate_basis=n_base_orign;
    frozen_index=[]
    while num_canditate_basis>1 :
      #get current F_criteria
      for i in range(len(self.F_switch)):
        if num_column>self.F_switch[i]:
          F_threshold=self.F_criteria[i+1]
        else:
          break
          
      find_flag=False 
      # put anchor index into frozen_index
      for i in range(local_to_global_index.size) :
        if self.anchor[local_to_global_index[i]]==1 :
          frozen_index.append(i)
          
      # begin to do basis reduction
      for j in range(gamma_vector.size):
        # continue if j is in the frozen_index
        if j in frozen_index:
          continue
        # calculate the min of gamma_vector except the frozen_index
        gamma_vector_min=gamma_vector;
        gamma_vector_min=np.delete(gamma_vector_min, frozen_index)
        gamma_criteria=min(abs(gamma_vector_min) )+threshold_d;
        theta_matrix_try=theta_matrix;
        
        #tentative delete the basis
        if abs(gamma_vector[j])<gamma_criteria :
          frozen_index.append(j)
          find_flag=True
          # delete the corresponding column
          theta_matrix_try=np.delete(theta_matrix_try,j,1)
            
            
          if(alpha_sum==-1):
            [gamma_vector_try,loss_try]=LR.fit(theta_matrix_try,X_matrix)
          if(self.alpha_lasso>0):
            [gamma_vector_try,loss_try]=LR.fit_lasso(theta_matrix_try,X_matrix, alpha=self.alpha_lasso)
          if(self.alpha_ridge>0):
            [gamma_vector_try,loss_try,score_tem]=LR.fit_ridge(theta_matrix_try,X_matrix, alpha=self.alpha_ridge)
          if(self.ridge_cv[0]>-0.1):
              [gamma_vector_try,loss_try,score_tem,best_alpha_tem]=LR.fit_ridge_cv(theta_matrix_try,X_matrix, alpha=self.ridge_cv)
                
          F=(loss_try-self.loss[num_column-1])/self.loss[num_column-1]*(n_base_orign-local_to_global_index.size+1)   
          if(F>self.last_F):
            self.last_F=F
                   
          # do F_test
          if F<F_threshold or loss_try<self.sigma_n:
            theta_matrix=np.delete(theta_matrix,j,1)
            local_to_global_index=np.delete(local_to_global_index,j)
            self.F_index[num_column]=F
            if(len(self.ridge_cv)>2):
                self.best_alpha[num_column]=best_alpha_tem
            self.loss[num_column]=loss_try
            gamma_vector=gamma_vector_try
            self.gamma_matrix[local_to_global_index,num_column]=gamma_vector
            num_column=num_column+1
            num_canditate_basis=num_canditate_basis-1
            frozen_index=[]
            
        # break tentative deleting basis
        if find_flag==True:
          break
      #stop the algorithm   
      if find_flag==0 or gamma_vector_min.size<1:
        break
        
    self.gamma_matrix=np.delete(self.gamma_matrix,np.arange(num_column,n_base_orign), axis=1)
    self.loss=np.delete(self.loss,np.arange(num_column,n_base_orign))   
    self.F_index=np.delete(self.F_index,np.arange(num_column,n_base_orign))   
    self.best_alpha=np.delete(self.best_alpha,np.arange(num_column,n_base_orign))  
                    
  
  def stepwiseR_fit_most_insignificant(self, theta_matrix, X_matrix):
    _,n_base_orign=theta_matrix.shape
    self.anchor=np.zeros(n_base_orign)
    if self.anchor_index[0]!=-1:
      for key in self.anchor_index:
        self.anchor[key]=1

    self.loss=np.zeros(n_base_orign)
    self.score=np.zeros(n_base_orign)
    self.F_index=np.zeros(n_base_orign)
    self.gamma_matrix=np.zeros((n_base_orign,n_base_orign))
    alpha_sum=self.alpha_lasso+self.alpha_ridge+self.ridge_cv[0]
    threshold_d=self.threshold_d
    self.best_alpha=np.zeros(n_base_orign)
    
    # local_to_global_index
    local_to_global_index=np.arange(n_base_orign)
    F_threshold=self.F_criteria[0]
    
    #########
    #first LS_regression
    #########
    num_column=0
    if(alpha_sum==-1):
        [gamma_vector,self.loss[0]]=LR.fit(theta_matrix,X_matrix)
    if(self.alpha_lasso>0):
        [gamma_vector,self.loss[0]]=LR.fit_lasso(theta_matrix,X_matrix, alpha=self.alpha_lasso)
    if(self.alpha_ridge>0):
        [gamma_vector,self.loss[0],self.score[0]]=LR.fit_ridge(theta_matrix,X_matrix, alpha=self.alpha_ridge)
    if(self.ridge_cv[0]>-0.1):
        [gamma_vector,self.loss[0],self.score[0],self.best_alpha[0]]=LR.fit_ridge_cv(theta_matrix,X_matrix, alpha=self.ridge_cv)
        
    self.gamma_matrix[local_to_global_index,num_column]=gamma_vector
    
    #########
    #stepwise
    #########
    num_column=num_column+1;
    num_canditate_basis=n_base_orign;
    frozen_index=[]
    while num_canditate_basis>1 :
      #get current F_criteria
      for i in range(len(self.F_switch)):
        if num_column>self.F_switch[i]:
          F_threshold=self.F_criteria[i+1]
        else:
          break
          
      find_flag=False 
      # put anchor index into frozen_index
      for i in range(local_to_global_index.size) :
        if self.anchor[local_to_global_index[i]]==1 :
          frozen_index.append(i)
          
      # begin to do basis reduction
      loss_tem=np.ones(gamma_vector.size)*1.0e10
      best_alpha_tem=np.zeros(gamma_vector.size)
      score_tem=np.zeros(gamma_vector.size)
      gamma_matrix_try=np.zeros((gamma_vector.size-1,gamma_vector.size))
      for j in range(gamma_vector.size):
        # continue if j is in the frozen_index
        if j in frozen_index:
          continue
        
        theta_matrix_try=np.delete(theta_matrix,j,1)
        if(alpha_sum==-1):
          [gamma_matrix_try[:,j],loss_tem[j] ]=LR.fit(theta_matrix_try,X_matrix)
        if(self.alpha_lasso>0):
          [gamma_vector_try[:,j],loss_tem[j] ]=LR.fit_lasso(theta_matrix_try,X_matrix, alpha=self.alpha_lasso)
        if(self.alpha_ridge>0):
          [gamma_matrix_try[:,j],loss_tem[j] ,score_tem[j] ]=LR.fit_ridge(theta_matrix_try,X_matrix, alpha=self.alpha_ridge)
        if(self.ridge_cv[0]>-0.1):
          [gamma_matrix_try[:,j],loss_tem[j] ,score_tem[j],best_alpha_tem[j] ]=LR.fit_ridge_cv(theta_matrix_try,X_matrix, alpha=self.ridge_cv)
        
      drop_index=np.argmin(loss_tem)  
      loss_try=loss_tem[drop_index]  
      F=(loss_try-self.loss[num_column-1])/self.loss[num_column-1]*(n_base_orign-local_to_global_index.size+1) 
      if(F>self.last_F):
        self.last_F=F
      # do F_test
      if F<F_threshold or loss_try<self.sigma_n:
        find_flag=True
        theta_matrix=np.delete(theta_matrix,drop_index,1)
        local_to_global_index=np.delete(local_to_global_index,drop_index)
        self.F_index[num_column]=F
        if(len(self.ridge_cv)>2):
          self.best_alpha[num_column]=best_alpha_tem[drop_index]
        self.loss[num_column]=loss_try
        gamma_vector=gamma_matrix_try[:,drop_index]
        self.gamma_matrix[local_to_global_index,num_column]=gamma_vector
        num_column=num_column+1
        num_canditate_basis=num_canditate_basis-1
        frozen_index=[]
            
      #stop the algorithm of no operator can be eliminated or only one operator left.
      if find_flag==False:
        break
        
    self.gamma_matrix=np.delete(self.gamma_matrix,np.arange(num_column,n_base_orign), axis=1)
    self.loss=np.delete(self.loss,np.arange(num_column,n_base_orign))   
    self.F_index=np.delete(self.F_index,np.arange(num_column,n_base_orign))   
    self.best_alpha=np.delete(self.best_alpha,np.arange(num_column,n_base_orign))  
      
    
  ###################################################            
  def stepwiseR_fit(self, theta_matrix, X_matrix):
    if self.basis_drop_strategy=='aggressive':
      self.stepwiseR_fit_aggressive(theta_matrix, X_matrix)
    elif self.basis_drop_strategy=='most_insignificant':
      self.stepwiseR_fit_most_insignificant(theta_matrix, X_matrix)
    else:
      print('basis_drop_strategy is not well defined, using most_insignificant instead')
      self.stepwiseR_fit_most_insignificant(theta_matrix, X_matrix)


    

    
    
      
    
    

====================================================================================================
mechanoChemML\src\texify.py
====================================================================================================
# Import python modules
import os,sys,copy,itertools,re
import numpy as np
DELIMITER = '__'



def groupsort(groups,args):
	'''
	Get unique args from groups and sort in same order as in args
	'''

	sort = [v for v in args]
	sets = {u:[v for v in groups if groups[v] == u] for u in set([groups[v] for v in groups])}
	groups = list(sorted(sets,key=lambda u: min([sort.index(v) if v in sort else len(sort)+list(sets).index(u) 
												 for v in sets[u]])))



	return groups



def icombinations(iterable,n,unique=False):
	''' 
	Get all combinations of p number of non-negative integers that sum up to at most n
	Args:
		iterable (iterable): Number of integers or iterable of length p
		n (int,iterable): Maximum number of elements, or allowed number of elements
		unique (bool): Return unique combinations of integers and q = choose(p+n,n) else q = (p^(n+1)-1)/(p-1)
	Returns:
		combinations (list): All combinations of iterable with q list of lists of length up to n, or lengths in n
	'''
	iterable = list(iterable)
	p = len(iterable)
	n = range(n+1) if isinstance(n,(int,np.integer)) else n
	combinations = []
	for i in n:
		combos = list((tuple(sorted(j,key=lambda i:iterable.index(i))) for j in itertools.product(iterable,repeat=i)))
		if unique:
			combos = sorted(set(combos),key=lambda i:combos.index(i))
		combinations.extend(combos)
	return combinations



def findstring(string,strings,types,prefixes,labels,replacements,default=None,regex=False,usetex=True):
	'''
	Check if string in strings dictionary
	Args:
		string(str): String to be checked if in strings dictionary
		strings(dict): Dictionary of string keys with replacement strings values
		types(dict): Dictionary of variable types that strings may start with, with template values in order to determine how to render strings 
		prefixes(dict): Dictionary of prefix types that strings may start with, with template values in order to determine how to render strings 
		labels(dict): Dictionary of label prefixes for string templates to render strings
		default (str,None): default return value if string not in strings
		regex (bool): perform regex processing to find string pattern in strings
	Returns:
		Returned string based on logic of whether string is in strings
	'''
	if default is None:
		default = string

	latex = None
	func = lambda string,latex,iloc,labels,strings:latex
	iloc = None

	funcs = []
	ilocs = []
	prefixs = []
	_string = string
	isprefix = any([string.startswith('%s%s'%(prefix,DELIMITER)) for prefix in prefixes])


	# print('finding',string,isprefix)
	while(isprefix):
		for prefix in prefixes:			
			if string.startswith(prefix):
				func = prefixes[prefix]
				# print('startswith',prefix,func)
				if DELIMITER in string:
					iloc = string.split(DELIMITER)[1]
					string = DELIMITER.join(string.split(DELIMITER)[2:])
				else:
					iloc = ''
				#print('modified',string,iloc)
				prefixs.append(prefix)
				funcs.append(func)
				ilocs.append(iloc)
		isprefix = any([string.startswith('%s%s'%(prefix,DELIMITER)) for prefix in prefixes])

	
	prefixs = prefixs[::-1]
	funcs = funcs[::-1]
	ilocs = ilocs[::-1]

	# print(ilocs)

	for typed in types:
		for label in types[typed]:
			# print('Trying',string,typed,label,string in types[typed][label])
			if string in types[typed][label] and (any([p==typed or '%ss'%(p)==typed for p in prefixs]) or (len(prefixs)==0) or all([(p not in types) and ('%ss'%(p) not in types) for p in prefixs])):
				latex = types[typed][label][string]	
				# print('found latex',typed,label,latex)

			if latex is not None:
				break
		if latex is not None:
				break

	if latex is None:
		latex = strings.get(string,string)
		typed = None
		label = None

	#print('final latex',latex)


	for func,iloc in zip(funcs,ilocs):
		latex = func(string,latex,iloc,labels,strings)

	_latex = latex
	for t in replacements:
		latex = latex.replace(t,replacements[t])

	#print('returning',latex)

	if not regex:
		return latex

	for string in strings:
		restring = re.compile(r'%s'%(string))
		restring = restring.search(string)
		try:
			restring = restring.group(0)			
		except:
			continue
		try: 
			parser = strings.get(string)				
			if not callable(parser):
				replacement = parser
				parser = lambda s:s
			else:
				replacement = string
			return parser(re.sub(string,result,string))
		except:
			return string
	return default


def isnumber(s):
	'''
	Check if object is a float or integer number
	Args:
		s(object): Object to be checked as number
	Returns:
		Boolean of whether object s is a number
	'''
	try:
		s = float(s)
		return True
	except:
		try:
			s = int(s)
			return True
		except:
			return False




def scinotation(number,decimals=2,base=10,order=2,zero=True,scilimits=[-1,1],usetex=True):
	'''
	Put number into scientific notation string
	Args:
		number (str,int,float): Number to be processed
		decimals (int): Number of decimals in base part of number
		base (int): Base of scientific notation
		order (int): Max power of number allowed for rounding
		zero (bool): Make numbers that equal 0 be the int representation
		scilimits (list): Limits on where not to represent with scientific notation
		usetex (bool): Render string with Latex
	
	Returns:
		String with scientific notation format for number

	'''
	if not isnumber(number):
		return str(number)
	try:
		number = int(number) if int(number) == float(number) else float(number)
	except:
		string = number
		return string

	maxnumber = base**order
	if number > maxnumber:
		number = number/maxnumber
		if int(number) == number:
			number = int(number)
		string = str(number)
	
	if zero and number == 0:
		string = '%d'%(number)
	
	elif isinstance(number,(int,np.integer)):
		string = str(number)
		# if usetex:
		# 	string = r'\textrm{%s}'%(string)
	
	elif isinstance(number,(float,np.float)):		
		string = '%0.*e'%(decimals,number)
		string = string.split('e')
		basechange = np.log(10)/np.log(base)
		basechange = int(basechange) if int(basechange) == basechange else basechange
		flt = string[0]
		exp = str(int(string[1])*basechange)
		if int(exp) in range(*scilimits):
			flt = '%0.*f'%(decimals,float(flt)/(base**(-int(exp))))
			string = r'%s'%(flt)
		else:
			string = r'%s%s'%(flt,r'\cdot %d^{%s}'%(base,exp) if exp!= '0' else '')
	if usetex:
		string = r'%s'%(string.replace('$',''))
	else:
		string = string.replace('$','')
	return string

# Texify strings
class Texify(object):
	'''
	Render strings as Latex strings

	Args:
		texstrings(dict): Dictionary of text strings with Latex string values
		texargs(dict): Dictionary of arguments and settings used in Latex rendering
		texlabels(dict): Dictionary of modifier strings for Latex rendering
		labels(dict): Dictionary of label prefixes for string templates for Latex rendering
		usetex(bool): Render as Latex strings, or render as text strings
	'''
	def __init__(self,texstrings={},texargs={},texlabels={},usetex=True):


		args_default = {
			'bases':{'monomial':1,'polynomial':0,'taylorseries':1,'derivative':1,'expansion':1},
			'order':2,
			'basis':3,
			'selection':tuple(range(1+1)),
			'iloc':[0,None],
			'unique':True,
			'operators':['partial','delta','Delta','d'],
			'weights':['stencil'],			
			'inputs': {'%s%d'%(x,i):r'{%s_{%d}}'%(x,i) for x in ['x'] for i in range(5)},
			'outputs': {'%s%d'%(x,i):r'{%s_{%d}}'%(x,i) for x in ['y'] for i in range(3)},
			'terms': {'%s%d'%(x,i):r'{%s_{%d}}'%(x,i) for x in ['x'] for i in range(3)},
			# 'groups':{**{'%s%d'%(x,i):r'{%s_{%d}}'%(x,i) for x in ['x'] for i in range(5)}},
			'constants':{},
			'texreplacements':{r'\\\\':'\\\\'},
			'replacements':{DELIMITER:'',r'\\\\':'','\\':'','textrm':'','_':r'\_','^{}':'','^':r'\^~',
							r'frac':'',r'}{{delta':r'}/{{delta',r'}{{partial':r'}/{{partial','abs':''},
		}

		args_special = {'inputs':{},'outputs':{},'constants':{},'order':1,'basis':1}
		args_keyed = ['iloc']
		args_dependent = {'groups': lambda args:groupsort(args.get('groups',{x: args['inputs'].get(x) for x in args['inputs']}),args['inputs']),
						  'selection': lambda args: tuple(range(args['order'] if not isinstance(args.get('selection'),(int,np.integer)) else args.get('selection')+1)) if not isinstance(args.get('selection'),(np.ndarray,list)) else args['selection'] }
		


		args = {}
		args.update(texargs)
		args.update({k:v for k,v in args_default.items() if k not in args})
		args.update({k:v for k,v in args_special.items() if args.get(k) is None})
		args.update({k: list(set([i for u in (args.get(k,{}) if isinstance(args.get(k),dict) else ([i for i in args.get(k,[])] if isinstance(args.get(k),list) else [args.get(k)]))
									for i in ((args.get(k,{}).get(u) if (
										isinstance(args.get(k,{}).get(u),(list,tuple,np.ndarray))) else (
										[args.get(k,{}).get(u)])) if isinstance(args.get(k),dict) else ([i for i in args.get(k,[])] if isinstance(args.get(k),list) else [args.get(k)]))]))
					for k in args_keyed})


		for k in args_dependent:
			args[k] = args_dependent[k](args)

		# Modifying
		labels = {
				'abs': r'{\abs{%s}}',	
				'bar': r'{\bar{%s}}',
				'tilde': r'{\tilde{%s}}',
				'hat': r'{\hat{%s}}',
				'brackets':r'{(%s)}',
				'leftrightbrackets':r'{\left({%s}\right)}',
				'squarebrackets':r'{[{%s}]}',
				'leftrightsquarebrackets':r'{\left[{%s}\right]}',
				'curlybrackets':r'{\{{%s}\}}',
				'leftrightcurlybrackets':r'{\left\{{%s}\right\}}',				
				'superscript':r'{%s}^{%s}',			
				'subscript':r'{%s}_{%s}',			
				'supersubscript':r'{%s}_{%s}^{%s}',			
				'partial':r'\partial',
				'func':r'%s(%s)',
				'd': r'd',
				'delta':r'\delta',
				'Delta':r'\Delta',
				'gamma':r'\gamma',				
				'derivative':r'{\frac{{%s}^{%s}%s}{%s {%s}^{%s}}}',
				'nderivative':r'{\frac{{%s}^{%s}%s}{%s}}',
				'coefficient': r'{%s}^{%s}',
				'factorial': r'{%s!}'
		}
		labels.update(texlabels)


		prefixes = {
			'variable': lambda string,latex,iloc,labels,strings: latex,
			'constant': lambda string,latex,iloc,labels,strings: labels['func']%(latex,'%s'%(','.join(['{%s}_{%s}'%(v,str(iloc) if iloc not in [None,'None',str(None)] else '') for v in args['groups']]))),
			'subscript': lambda string,latex,iloc,labels,strings: r'{%s}_{%s}'%(latex,str(iloc)),
			'superscript': lambda string,latex,iloc,labels,strings: r'{%s^{%s}'%(latex,str(iloc)),
			'monomial': lambda string,latex,iloc,labels,strings: r'{%s}'%(latex),
			'polynomial': lambda string,latex,iloc,labels,strings: r'{%s}'%(latex),
			'chebyshev': lambda string,latex,iloc,labels,strings: r'{%s}'%(latex),
			'legendre': lambda string,latex,iloc,labels,strings: r'{%s}'%(latex),
			'hermite': lambda string,latex,iloc,labels,strings: r'{%s}'%(latex),
			'expansion': lambda string,latex,iloc,labels,strings: r'{%s}'%(latex.replace('True',str(iloc) if iloc not in [None,'None',str(None)] else '')),
			'taylorseries': lambda string,latex,iloc,labels,strings: r'{%s}'%(latex.replace('True',str(iloc) if iloc not in [None,'None',str(None)] else '')),
			'derivative': lambda string,latex,iloc,labels,strings: r'{%s}'%(latex),
			'difference': lambda string,latex,iloc,labels,strings: r'{%s}'%(latex),
			'deltavar': lambda string,latex,iloc,labels,strings: r'%s{%s}'%(labels['delta'],latex),
			'Deltavar': lambda string,latex,iloc,labels,strings: r'%s{%s}'%(labels['Delta'],latex),
			'coefficient': lambda string,latex,iloc,labels,strings: labels['coefficient']%(labels['gamma'],latex),
			'factorial': lambda string,latex,iloc,labels,strings: labels['factorial']%(str(iloc)),
			'iteration': lambda string,latex,iloc,labels,strings: r'{%s}^{(%s)}'%(latex,iloc),
			}

		# Types
		def variables(variable,strings,args,labels,usetex):
			'''
			Variable string patterns
			Args:
				variable(str,list): Type of variable for variables (inputs,outputs,terms)			
				strings(dict): String patterns with text string keys and Latex string values
				args(dict): Dictionary of arguments and settings used in Latex rendering
				labels(dict): Dictionary of label prefixes for string templates for Latex rendering
			Returns:
				Dictionary of rendered string patterns
			'''					


			if isinstance(variable,str):
				variables = {x: strings.get(x,args[variable][x]) for x in args[variable]}
			else:
				variables = {x: strings.get(x,args[v][x]) for v in variable for x in args[v]}

			func = labels['superscript']

			values = variables

			return values


		def constants(variable,strings,args,labels,usetex):
			'''
			Variable string patterns
			Args:
				variable(str,list): Type of variable for constants (inputs,outputs,terms)			
				strings(dict): String patterns with text string keys and Latex string values
				args(dict): Dictionary of arguments and settings used in Latex rendering
				labels(dict): Dictionary of modifier strings for Latex rendering
			Returns:
				Dictionary of rendered string patterns
			For example:
				strings have notation:
					prefix__iloc__variable
				with prefixes
					[constant]					
				which becomes
					variable
			'''					

			if isinstance(variable,str):
				variables = {x: strings.get(x,args[variable][x]) for x in args[variable]}
			else:
				variables = {x: strings.get(x,args[v][x]) for v in variable for x in args[v]}

			func = labels['superscript']


			values = variables
			return values


		def derivative(symbol,strings,args,labels,usetex):
			'''
			Derivative string patterns
			Args:
				symbol(str): String key for labels on which derivative symbol to use (partial,delta,Delta)
				strings(dict): String patterns with text string keys and Latex string values
				args(dict): Dictionary of arguments and settings used in Latex rendering
				labels(dict): Dictionary of modifier strings for Latex rendering
			Returns:
				Dictionary of rendered string patterns
			For example:
				strings have notation:
					operation_0__operation_1__...__operation_order-1__order__function__variable_0__variable_1__...__variable_order-1__weight_0__weight_1__...__weight_order-1
				which becomes
					\frac{operation function}{operation_0 variable_0 ... operation_order-1 variable_order-1} operation order_0 variable_0 ... operation_order-1 variable_order-1
			'''					
			inputs = {x: strings.get(x,args['inputs'][x]) for x in args['inputs']}
			outputs = {y: strings.get(y,args['outputs'][y]) for y in args['outputs']}

			delimiter = DELIMITER
			func = labels['nderivative']
			Symbol = labels[symbol]


			values = {}
			if args['bases'].get('derivative'):
				values = {
					delimiter.join([
					delimiter.join([o]*j),
					str(j),
					y,
					delimiter.join([u for u in x]),
					delimiter.join([w]*j)
					]):func%(Symbol,str(j) if j>1 else '','%s'%(outputs[y]),
							' '.join([r'%s %s'%(Symbol,inputs[v]) for v in x]) if len(set([v for v in x]))>1 else r'{%s %s}^{%s}'%(Symbol,inputs[[v for v in x][0]],str(j) if j>1 else ''),
					)if usetex else (
						'd^%s%s/%s'%(str(j) if j>1 else '',y,''.join(['d%s'%(v) for v in x]))
						)				
					for o in [o for o in args['operators'] if o in [symbol]]
					for j in range(1,args['order']+1)
					for y in outputs
					for x in icombinations(inputs,[j],unique=args['unique'])
					for w in args['weights']
					}

			return values	

		def expansion(iloc,symbol,Symbol,strings,args,labels,usetex):
			'''
			Expansion string patterns
			Args:
				iloc(int,None): location of Expansion
				symbol(str): String key for labels on which derivative symbol to use (partial,delta,Delta)
				symbol(str): String key for labels on which delta symbol to use (partial,delta,Delta)
				strings(dict): String patterns with text string keys and Latex string values
				args(dict): Dictionary of arguments and settings used in Latex rendering
				labels(dict): Dictionary of modifier strings for Latex rendering
			Returns:
				Dictionary of rendered string patterns
			For example:
				strings have notation:
					prefix__iloc__operation_0__operation_1__...__operation_order-1__order__function__variable_0__variable_1__...__variable_order-1__weight_0__weight_1__...__weight_order-1
				with prefixes
					[expansion]
				which becomes
					\frac{operation function}{operation_0 variable_0 ... operation_order-1 variable_order-1} operation order_0 variable_0 ... operation_order-1 variable_order-1
			'''					
			inputs = {x: strings.get(x,args['inputs'][x]) for x in args['inputs']}
			outputs = {y: strings.get(y,args['outputs'][y]) for y in args['outputs']}

			delimiter = DELIMITER
			func = labels['nderivative']
			symbol = labels[symbol]
			Symbol = labels[Symbol]




			values = {}
			if args['bases'].get('expansion'):

				values.update({
					delimiter.join([
					delimiter.join([o]*j),
					str(j),
					y,
					delimiter.join([u for u in x]),
					delimiter.join([w]*j)
					]):('%s'%(						
						func%(
							symbol,
							str(j) if j>1 else '',
							'%s%s'%(outputs[y],'(%s)'%(','.join(['{%s}_{%s}'%(v,str(iloc) if iloc is not None else '') for v in args['groups']]))),
							' '.join([r'{%s %s}^{%s}'%(symbol,inputs[v],str(i) if i>1 else '') for i,v in sorted([(list(x).count(v),v) for v in set([v for v in x])],key=lambda v:list(x).index(v[1]))]))		
					) if usetex else (
					'd^%s%s/%s'%(						
						str(j) if j>1 else ''),
						y,
						''.join(['d%s'%(v) for v in x]), 
					))
					for o in args['operators']
					for j in range(1,args['order']+1)
					for y in outputs
					for x in icombinations(inputs,[j],unique=args['unique'])
					for w in args['weights']
				})
				
			return values	

		def taylorseries(iloc,symbol,Symbol,strings,args,labels,usetex):
			'''
			Taylorseries string patterns
			Args:
				iloc(int,None): location of Taylorseries
				symbol(str): String key for labels on which derivative symbol to use (partial,delta,Delta)
				symbol(str): String key for labels on which delta symbol to use (partial,delta,Delta)
				strings(dict): String patterns with text string keys and Latex string values
				args(dict): Dictionary of arguments and settings used in Latex rendering
				labels(dict): Dictionary of modifier strings for Latex rendering
			Returns:
				Dictionary of rendered string patterns
			For example:
				strings have notation:
					prefix__iloc__operation_0__operation_1__...__operation_order-1__order__function__variable_0__variable_1__...__variable_order-1__weight_0__weight_1__...__weight_order-1
				with prefixes
					[taylorseries]					
				which becomes
					\frac{1}{order !} \frac{operation function}{operation_0 variable_0 ... operation_order-1 variable_order-1} operation order_0 variable_0 ... operation_order-1 variable_order-1
			'''					
			inputs = {x: strings.get(x,args['inputs'][x]) for x in args['inputs']}
			outputs = {y: strings.get(y,args['outputs'][y]) for y in args['outputs']}

			delimiter = DELIMITER
			func = labels['nderivative']
			symbol = labels[symbol]
			Symbol = labels[Symbol]

			values = {}
			
			if args['bases'].get('taylorseries'):
				values.update({
					delimiter.join([
					delimiter.join([o]*j),
					str(j),
					y,
					delimiter.join([u for u in x]),
					delimiter.join([w]*j)
					]):('%s%s%s'%(
						(r'%s%s'%(
							(labels['coefficient']%(labels['gamma'],
								''.join([r'{{%s}^{%s}}'%(inputs[v],str(i) if i>1 else '') 
									for i,v in sorted([(list(x).count(v),v) for v in set([v for v in x])],key=lambda v:list(x).index(v[1]))]))), 
							(r'\frac{1}{%s}'%(''.join([labels['factorial']%(i) if i > 1 else '' for i,v in sorted([(list(x).count(v),v) for v in set([v for v in x])],key=lambda v:list(x).index(v[1]))])
								if args['unique'] else labels['factorial']%(j))
								if (j>1 and ((not args['unique']) or any([i!=1 for i,v in sorted([(list(x).count(v),v) for v in set([v for v in x])],key=lambda v:list(x).index(v[1]))]))) else '')
							)) if j>0 else '',							
						func%(
							symbol,
							str(j) if j>1 else '',
							'%s%s'%(outputs[y],'(%s)'%(','.join(['{%s}_{%s}'%(v,str(iloc) if iloc is not None else '') for v in args['groups']]))),
							' '.join([r'{%s %s}^{%s}'%(symbol,inputs[v],str(i) if i>1 else '') for i,v in sorted([(list(x).count(v),v) for v in set([v for v in x])],key=lambda v:list(x).index(v[1]))])),		
						' '.join([r'{%s %s}^{%s}'%(Symbol,inputs[v],str(i) if i>1 else '') for i,v in sorted([(list(x).count(v),v) for v in set([v for v in x])],key=lambda v:list(x).index(v[1]))
							])
					) if usetex else (
					'%s d^%s%s/%s %s'%(
						('%s**{%s} %s'%(
								'g',
								''.join([r'{{%s}^{%s}}'%(inputs[v],str(i) if i>1 else '') 
									for i,v in sorted([(list(x).count(v),v) for v in set([v for v in x])],key=lambda v:list(x).index(v[1]))]), 
							(r'1/%s'%(''.join([labels['factorial']%(i) if i > 1 else '' for i,v in sorted([(list(x).count(v),v) for v in set([v for v in x])],key=lambda v:list(x).index(v[1]))])
								if args['unique'] else labels['factorial']%(j)) if (j>1 and ((not args['unique']) or any([i!=1 for i,v in sorted([(list(x).count(v),v) for v in set([v for v in x])],key=lambda v:list(x).index(v[1]))]))) else '') if j>0 else ''
							) if j>0 else ''),						
						str(j) if j>1 else '',
						y,
						''.join(['d%s'%(v) for v in x]), 
						' '.join(['d%s'%(v) for v in x])
					)))
					for o in args['operators']
					for j in range(1,args['order']+1)
					for y in outputs
					for x in icombinations(inputs,[j],unique=args['unique'])
					for w in args['weights']
				})


				# values.update({
				# 	delimiter.join([
				# 	delimiter.join([o]*j),
				# 	str(j),
				# 	y,
				# 	delimiter.join([u for u in x]),
				# 	delimiter.join([w]*j)
				# 	]):'%s%s%s'%(
				# 		r'%s\frac{1}{%s}'%(labels['coefficient']%(labels['gamma'],''.join([r'%s'%(inputs[v]) for v in x]) if len(set([v for v in x]))>1 else r'{%s}^{%s}'%(inputs[[v for v in x][0]],str(j) if j>1 else '')
				# 			),''.join([labels['factorial']%(i) if i > 1 else '' for i in [list(x).count(v) for v in set([v for v in x])]]) if args['unique'] else labels['factorial']%(j)) if (j>1 and ((not args['unique']) or any([i!=1 for i in [list(x).count(v) for v in set([v for v in x])]]))) else (
				# 			labels['coefficient']%(labels['gamma'],''.join([r'%s'%(inputs[v]) for v in x]) if len(set([v for v in x]))>1 else r'{%s}^{%s}'%(inputs[[v for v in x][0]],str(j) if j>1 else ''))),
				# 		func%(symbol,str(j) if j>1 else '','%s%s'%(outputs[y],'(%s)'%(','.join(['{%s}_{%s}'%(v,str(iloc) if iloc is not None else '') for v in args['groups']]))),
				# 			' '.join([r'%s %s'%(symbol,inputs[v]) for v in x]) if len(set([v for v in x]))>1 else r'{%s %s}^{%s}'%(symbol,inputs[[v for v in x][0]],str(j) if j>1 else '')),
				# 		' '.join([r'%s %s'%(Symbol,inputs[v]) for v in x]) if len(set([v for v in x]))>1 else r'{%s %s}^{%s}'%(Symbol,inputs[[v for v in x][0]],str(j) if j>1 else '')
				# 	) if usetex else (
				# 		'%s d^%s%s/%s %s'%('1/%s!'%(str(j)) if j>1 else '',str(j) if j>1 else '',y,''.join(['d%s'%(v) for v in x]), ' '.join(['d%s'%(v) for v in x]))
				# 	)				
				# 	for o in args['operators']
				# 	for j in range(1,args['order']+1)
				# 	for y in outputs
				# 	for x in icombinations(inputs,[j],unique=args['unique'])
				# 	for w in args['weights']
				# })




			return values				



		def monomials(variable,replacements,strings,args,labels,usetex):
			'''
			Monomial string patterns
			Args:
				variable(str,list): Type of variable for monomials (inputs,outputs,terms)			
				replacements(dict): Replacements for variable strings				
				strings(dict): String patterns with text string keys and Latex string values
				args(dict): Dictionary of arguments and settings used in Latex rendering
				labels(dict): Dictionary of modifier strings for Latex rendering
			Returns:
				Dictionary of rendered string patterns
			For example:
				strings have notation:
					prefix__iloc__variable_power
				with prefixes
					[monomial]					
				which becomes
					{variable}^{power}
			'''

			if isinstance(variable,str):
				variables = copy.deepcopy(list(args[variable]))
				for i,v in enumerate(variables):
					for r in replacements:
						variables[i] = variables[i].replace(r,replacements[r])
				variables = {x: strings.get(x,args[variable].get(x)) for x in variables}
			else:
				variables = copy.deepcopy({v:[x for x in args[v]] for v in variable})

				for i,v in enumerate(variables):
					for j,x in enumerate(variables[v]):
						for r in replacements:
							variables[v][j] = variables[v][j].replace(r,replacements[r])
				variables = {x: strings.get(x,args[v].get(x)) for v in variables for x in variables[v]}


			if usetex:
				func = labels['superscript']
			else:
				func = '%s^%s'

			delimiter = '_'
			constant = r''

			values = {}

			if args['bases'].get('monomial'):

				values.update({delimiter.join([x,str(j)]):func%(variables[x],str(j) if j>1 else '') if j>0 else constant
							for j in range(args['order']+1)
							for x in variables
						})


			return values

		def polynomial(variable,replacements,strings,args,labels,usetex):
			'''
			Polynomial string patterns
			Args:
				variable(str,list): Type of variable for polynomial (inputs,outputs,terms)
				replacements(dict): Replacements for variable strings
				strings(dict): String patterns with text string keys and Latex string values
				args(dict): Dictionary of arguments and settings used in Latex rendering
				labels(dict): Dictionary of modifier strings for Latex rendering
			Returns:
				Dictionary of rendered string patterns
			For example:
				strings have notation:
					prefix__iloc__variable_0_power_0-variable_1_power_1-...-variable_order-1_power_order-1
				with prefixes
					[polynomial]					
				which becomes
					{variable_0}^{power_0} {variable_1}^{power_1} ... {variable_order-1}^{power_order-1} 				
			'''			

			

			if isinstance(variable,str):
				variables = copy.deepcopy(list(args[variable]))
				for i,v in enumerate(variables):
					for r in replacements:
						variables[i] = variables[i].replace(r,replacements[r])
				variables = {x: strings.get(x,args[variable].get(x)) for x in variables}
			else:
				variables = copy.deepcopy({v:[x for x in args[v]] for v in variable})

				for i,v in enumerate(variables):
					for j,x in enumerate(variables[v]):
						for r in replacements:
							variables[v][j] = variables[v][j].replace(r,replacements[r])
				variables = {x: strings.get(x,args[v].get(x)) for v in variables for x in variables[v]}

			if usetex:
				func = labels['superscript']
			else:
				func = '%s^%s'

			if isinstance(args['selection'],tuple):
				I = itertools.product(args['selection'],repeat=len(variables))
			else:
				I = args['selection']

			if not isinstance(I,list):
				I = list(I)
			splitter = '-'
			delimiter = '_'
			constant = r'1'
			if usetex:
				separator = ''
			else:
				separator = ' '

			values = {}

			if args['bases'].get('polynomial'):
				values.update({
					splitter.join([delimiter.join([x,str(j)]) for x,j in zip(variables,i)]):
					 separator.join([func%(variables[x],str(j) if j>1 else '') if j>0 else '' for x,j in zip(variables,i)]) if sum(i)>0 else constant
					for i in I
					})


			return values				


		def chebyshev(variable,replacements,strings,args,labels,usetex):
			'''
			Chebyshev string patterns
			Args:
				variable(str,list): Type of variable for Chebyshev polynomial (inputs,outputs,terms)			
				replacements(dict): Replacements for variable strings				
				strings(dict): String patterns with text string keys and Latex string values
				args(dict): Dictionary of arguments and settings used in Latex rendering
				labels(dict): Dictionary of modifier strings for Latex rendering
			Returns:
				Dictionary of rendered string patterns
			For example:
				strings have notation:
					prefix__iloc__variable_order
				with prefixes
					[chebyshev]					
				which becomes
					T_{order}(variable)
			'''


			if isinstance(variable,str):
				variables = copy.deepcopy(list(args[variable]))
				for i,v in enumerate(variables):
					for r in replacements:
						variables[i] = variables[i].replace(r,replacements[r])
				variables = {x: strings.get(x,args[variable].get(x)) for x in variables}
			else:
				variables = copy.deepcopy({v:[x for x in args[v]] for v in variable})

				for i,v in enumerate(variables):
					for j,x in enumerate(variables[v]):
						for r in replacements:
							variables[v][j] = variables[v][j].replace(r,replacements[r])
				variables = {x: strings.get(x,args[v].get(x)) for v in variables for x in variables[v]}

			if usetex:
				func = labels['subscript']
			else:
				func = '%s_%s'
			symbol = 'T'

			delimiter = '_'
			constant = r'1'

			values = {}

			if args['bases'].get('chebyshev'):			
				values.update({delimiter.join([x,str(j)]):r'%s(%s)'%(func%(symbol,str(j)),variables[x]) if j>0 else constant
							for j in range(args['order']+1)
							for x in variables
						})

			return values

		def hermite(variable,replacements,strings,args,labels,usetex):
			'''
			Hermite string patterns
			Args:
				variable(str,list): Type of variable for Hermite polynomial (inputs,outputs,terms)			
				replacements(dict): Replacements for variable strings				
				strings(dict): String patterns with text string keys and Latex string values
				args(dict): Dictionary of arguments and settings used in Latex rendering
				labels(dict): Dictionary of modifier strings for Latex rendering
			Returns:
				Dictionary of rendered string patterns
			For example:
				strings have notation:
					prefix__iloc__variable_order
				with prefixes
					[hermite]					
				which becomes
					H_{order}(variable)				
			'''

			if isinstance(variable,str):
				variables = copy.deepcopy(list(args[variable]))
				for i,v in enumerate(variables):
					for r in replacements:
						variables[i] = variables[i].replace(r,replacements[r])
				variables = {x: strings.get(x,args[variable].get(x)) for x in variables}
			else:
				variables = copy.deepcopy({v:[x for x in args[v]] for v in variable})

				for i,v in enumerate(variables):
					for j,x in enumerate(variables[v]):
						for r in replacements:
							variables[v][j] = variables[v][j].replace(r,replacements[r])
				variables = {x: strings.get(x,args[v].get(x)) for v in variables for x in variables[v]}

			if usetex:
				func = labels['subscript']
			else:
				func = '%s_%s'
			symbol = 'H'

			delimiter = '_'
			constant = r'1'
			
			values = {}

			if args['bases'].get('hermite'):			
				values.update({delimiter.join([x,str(j)]):r'%s(%s)'%(func%(symbol,str(j)),variables[x]) if j>0 else constant
							for j in range(args['order']+1)
							for x in variables
						})

			return values		

		def legendre(variable,replacements,strings,args,labels,usetex):
			'''
			Legendre string patterns
			Args:
				variable(str,list): Type of variable for Legendre polynomial (inputs,outputs,terms)			
				replacements(dict): Replacements for variable strings				
				strings(dict): String patterns with text string keys and Latex string values
				args(dict): Dictionary of arguments and settings used in Latex rendering
				labels(dict): Dictionary of modifier strings for Latex rendering
			Returns:
				Dictionary of rendered string patterns
			For example:
				strings have notation:
					prefix__iloc__variable_order
				with prefixes
					[legendre]					
				which becomes
					L_{order}(variable)				
			'''

			if isinstance(variable,str):
				variables = copy.deepcopy(list(args[variable]))
				for i,v in enumerate(variables):
					for r in replacements:
						variables[i] = variables[i].replace(r,replacements[r])
				variables = {x: strings.get(x,args[variable].get(x)) for x in variables}
			else:
				variables = copy.deepcopy({v:[x for x in args[v]] for v in variable})

				for i,v in enumerate(variables):
					for j,x in enumerate(variables[v]):
						for r in replacements:
							variables[v][j] = variables[v][j].replace(r,replacements[r])
				variables = {x: strings.get(x,args[v].get(x)) for v in variables for x in variables[v]}

			if usetex:
				func = labels['subscript']
			else:
				func = '%s_%s'
			symbol = 'L'

			delimiter = '_'
			constant = r'1'
			
			values = {}

			if args['bases'].get('legendre'):			
				values.update({delimiter.join([x,str(j)]):r'%s(%s)'%(func%(symbol,str(j)),variables[x]) if j>0 else constant
							for j in range(args['order']+1)
							for x in variables
						})

			return values			


		texstrings.update({s.replace('partial',r):texstrings[s].replace(r'\delta',r'\%s'%r if r not in ['d'] else r) for s in texstrings for r in args['operators']})

		funcs = { 
			'misc': lambda strings,args,labels,usetex:{
				'default':{
					'complexity_':r'N_{\textrm{terms}}',
					'coef_':labels['abs']%(labels['gamma']),
					'data':r'\mathcal{D}_{\textrm{data}}',
					'intercept_':r'\beta',
					'term_':r'\frac{\partial f}{\partial x}',
					'loss':r'\textrm{Loss}', 
					'State': r'\textrm{State}',	
					'radius':r'r'},
				'strings':texstrings,	
				},			
			'variables': lambda strings,args,labels,usetex: {'%s_%s'%(k,'_'.join(list(set(list(r.values()))))): monomials(k,r,strings,args,labels,usetex) for k in ['inputs','outputs','terms',['inputs','terms']]  for r in [{'partial':x} for x in args['operators'] if x not in ['partial']]},
			'constants': lambda strings,args,labels,usetex: {'%s_%s'%(k,'_'.join(list(set(list(r.values()))))): monomials(k,r,strings,args,labels,usetex) for k in ['inputs','outputs','terms',['inputs','terms']]  for r in [{'partial':x} for x in args['operators'] if x not in ['partial']]},
			'derivative': lambda strings,args,labels,usetex: {'%s'%(k): derivative(k,strings,args,labels,usetex) for k in args['operators']},
			'expansion': lambda strings,args,labels,usetex: {'%s_%s_%s'%(str(i),k,K): expansion(i,k,K,strings,args,labels,usetex) for i in args['iloc'] for k,K in [('partial','Delta'),('delta','delta'),('Delta','Delta'),('d','Delta')]},
			'taylorseries': lambda strings,args,labels,usetex: {'%s_%s_%s'%(str(i),k,K): taylorseries(i,k,K,strings,args,labels,usetex) for i in args['iloc'] for k,K in [('partial','Delta'),('delta','delta'),('Delta','Delta'),('d','Delta')]},
			'monomials': lambda strings,args,labels,usetex: {'%s_%s'%(k,'_'.join(list(set(list(r.values()))))): monomials(k,r,strings,args,labels,usetex) for k in ['inputs','outputs','terms',['inputs','terms']] for r in [{'partial':x} for x in args['operators'] if x not in ['partial']]},
			'polynomial': lambda strings,args,labels,usetex: {'%s_%s'%(k,'_'.join(list(set(list(r.values()))))): polynomial(k,r,strings,args,labels,usetex) for k in ['inputs','outputs','terms',['inputs','terms']] for r in [{'partial':x} for x in args['operators'] if x not in ['partial']]},
			'chebyshev': lambda strings,args,labels,usetex: {'%s_%s'%(k,'_'.join(list(set(list(r.values()))))): chebyshev(k,r,strings,args,labels,usetex) for k in ['inputs','outputs','terms',['inputs','terms']] for r in [{'partial':x} for x in args['operators'] if x not in ['partial']]},
			'hermite': lambda strings,args,labels,usetex: {'%s_%s'%(k,'_'.join(list(set(list(r.values()))))): hermite(k,r,strings,args,labels,usetex) for k in ['inputs','outputs','terms',['inputs','terms']] for r in [{'partial':x} for x in args['operators'] if x not in ['partial']]},
			'legendre': lambda strings,args,labels,usetex: {'%s_%s'%(k,'_'.join(list(set(list(r.values()))))): legendre(k,r,strings,args,labels,usetex) for k in ['inputs','outputs','terms',['inputs','terms']] for r in [{'partial':x} for x in args['operators'] if x not in ['partial']]},
			}



		strings = {}
		types = {}

		strings.update(texstrings)


		for func in funcs:
			values = funcs[func](strings,args,labels,usetex)
			types[func] = values
			strings.update({s: values[k][s] for k in values for s in values[k] if s not in strings})
		


		self.args = args
		self.strings  = strings
		self.labels = labels
		self.types = types
		self.prefixes = prefixes
		self.usetex = usetex


		return
	 

	def texify(self,string,texstrings={},texargs={},texlabels={},usetex=True):
		'''
		Render string as Latex string
		Args:
			string(str): String to be rendered
			texstrings(dict): Dictionary of text strings with Latex string values
			texargs(dict): Dictionary of arguments and settings used in Latex rendering
			texlabels(dict): Dictionary of modifier strings for Latex rendering
			usetex(bool): Render as Latex strings, or render as text strings
		Returns:
			Latex rendered string
		'''			

		_string = string

		usetex = usetex if self.usetex is None else self.usetex

		String = ""
		if not isinstance(string,tuple):
			string = (string,)

		string = list(((scinotation(s,decimals=3,zero=True,usetex=usetex) for s in string)))


		for texstring in texstrings:
			self.strings['misc']['strings'].update({texstring:texstrings[texstring]})
		self.args.update(texargs)
		self.labels.update(texlabels)


		replacements = self.args['texreplacements'] if usetex else self.args['replacements']

		for s in string:

			s = scinotation(s,decimals=3,zero=True,usetex=usetex)

			if (0) and (s not in self.strings):
				#print('texify',s,s in self.strings,findstring(s,self.strings,self.types,self.prefixes,self.labels,s))
				pass
			s = findstring(s,self.strings,self.types,self.prefixes,self.labels,replacements,s,usetex=usetex)
			String = ' - '.join([String,s]) if len(String)>0 else s

		if usetex:
			String = '\n'.join([r'$%s$'%(s.replace('$','')) if len(s)>0 else s for s in String.split('\n')])
		else:
			String = '\n'.join([r'%s'%(s.replace('$','')) if len(s)>0 else s for s in String.split('\n')])


		return String    


====================================================================================================
mechanoChemML\src\transform_layer.py
====================================================================================================
from tensorflow import keras
from tensorflow.keras.layers import Lambda
import tensorflow.keras.backend as K
import marshal, base64, types
import numpy as np

#def _Transform(transforms):
#  """ Function to define a Keras transformation layer
#
#  :param transforms: A function that takes the input array x and returns a list of transformation outputs
#  :type transforms: func 
#
#  :returns: A Keras Transformation layer
#
#  """
#  
#  def func(x):
#    y = Lambda(transforms)(x)
#    return Lambda(lambda x: K.stack(x,axis=-1))(y)
#  return func


class Transform(keras.layers.Layer):
  """ Class to define a Keras transformation layer

  :param transforms: A function that takes the input array x and returns a list of transformation outputs
  :type transforms: func 

  """

  def __init__(self, transforms, **kwargs):
    super(Transform, self).__init__(**kwargs)
    self.transforms = transforms

  def call(self, inputs):
    return K.stack(self.transforms(inputs),axis=-1)

  def compute_output_shape(self, input_shape):
    return (None,len(self.transforms(np.ones((1,input_shape[1])))))

  def get_config(self):
    config = super(Transform, self).get_config()
    config.update({"transforms": base64.b64encode(marshal.dumps(self.transforms.__code__)).decode('utf-8')})
    return config

  @classmethod
  def from_config(cls, config):
    code = marshal.loads(base64.b64decode(config["transforms"]))
    config["transforms"] = types.FunctionType(code, globals(), "transforms")
    return cls(**config)


====================================================================================================
mechanoChemML\testing\__init__.py
====================================================================================================


====================================================================================================
mechanoChemML\testing\idnn_convexity_test.py
====================================================================================================
#!/usr/bin/env python

import numpy as np
import sys, os
import tensorflow as tf
from mechanoChemML.src.idnn import IDNN, find_wells

def idnn_convexity_test():
    # Set up simple IDNN with known weights
    w1 = np.array([[10,-10,11,-11,0,0,0,0],
                   [0,0,0,0,10,-10,11,-11]])
    b1 = np.array([0,0,0,0,0,0,0,0])
    w2 = np.array([[-1.15],[-1.15],[1],[1],[-1.15],[-1.15],[1],[1]])
    b2 = np.array([0])

    idnn = IDNN(2,[8],final_bias=True)
    idnn.build(input_shape=(1,2))
    idnn.dnn_layers[0].set_weights([w1,b1])
    idnn.dnn_layers[1].set_weights([w2,b2])

    # Create random selection of test points within [0,0.25]x[0,0.25]
    # Function will return points within a well
    points = 0.25*np.random.rand(100,2)
    wells = find_wells(idnn,points,rereference=False)
    
    # Points within [0,~0.10303402]x[0,~0.10303402] should be in a well,
    # all other points should not (the cutoff is not to machine precision,
    # so we cut it a little slack)
    success = True
    for well in wells:
        if well[0] > 0.10303403 or well[1] > 0.10303403:
            success = False
            
    for point in points:
        if point not in wells and point[0] < 0.10303402 and point[1] < 0.10303402:
            success = False

    print(wells)
    print(points)
    
    if success:
        print('Find wells test: passed')
    else:
        print('Find wells test: failed')


if __name__ == "__main__":
    idnn_convexity_test()


====================================================================================================
mechanoChemML\third_party\dns_wrapper\__init__.py
====================================================================================================


====================================================================================================
mechanoChemML\third_party\dns_wrapper\available_dns_modules.py
====================================================================================================
"""
This file contains a dictionary that lists all the available DNS examples with
    key = module_name
    value = module_path
"""
DNS_MODULES_PATH = {
        'diffusion_dynamic':'mechanoChemFEM/diffusion_single_phase_dynamic/',
        }


====================================================================================================
mechanoChemML\third_party\dns_wrapper\dns_wrapper.py
====================================================================================================
import numpy as np
import argparse, sys, os
import ctypes
import importlib
ctypes.CDLL("libmpi.so", mode=ctypes.RTLD_GLOBAL)
from available_dns_modules import DNS_MODULES_PATH

class DNS_Wrapper:
    """
    Example of a DNS wrapper, which provides a python interface to running direct numerical simulations written with other programming languages. 
    For example, one can use the pybind11 library to create a python module file based on the C++ code. The module will be loaded via
    self.load_example() function.
    """

    def __init__(self):
        """ initialize the class"""
        self.parse_sys_args()
        self.load_example()

    def parse_sys_args(self):
        """ parse arguments """
        parser = argparse.ArgumentParser(description='DNS python wrapper to run simulations', prog="'" + (sys.argv[0]) + "'")
        parser.add_argument('-e', '--example', type=str, default='diffusion_dynamic', choices=DNS_MODULES_PATH.keys(), help='examples to run')
        args = parser.parse_args()
        self.args = args

    def load_example(self):
        """ add the path to the python path and load the module """
        sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)) + '/' + DNS_MODULES_PATH[self.args.example])
        # load the python module 
        self.DNS_module = importlib.import_module(self.args.example)
        # related to the specific DNS software
        self.DNS_example = self.DNS_module.PYmechanoChem(['./main', 'parameters.prm'])


    def setup_problem(self):
        if self.args.example.find('IGA') >=0 :
            self.DNS_example.setup_mechanoChemIGA()
        elif self.args.example.find('FEM') >=0 :
            self.DNS_example.setup_mechanoChemFEM()
        else :
            print("***WARNING***: I cannot determine the specified physics based library.")
            exit(0)

    def simulate(self):
        """
        run simulation
        """
        self.DNS_example.simulate()

    def set_parameter(self, variable_name, variable_value):
        """
        update parameter values
        """
        self.DNS_example.setParameter(variable_name, variable_value)


if __name__ == "__main__" :
    problem = DNS_Wrapper()
    problem.setup_problem()
    problem.simulate()
    problem.set_parameter(["Diffusion reaction: one species", "dDirichletUc"], "0.2")
    problem.simulate() 


====================================================================================================
mechanoChemML\workflows\__init__.py
====================================================================================================


====================================================================================================
mechanoChemML\workflows\active_learning\__init__.py
====================================================================================================


====================================================================================================
mechanoChemML\workflows\active_learning\active_learning.py
====================================================================================================
#!/usr/bin/env python

import sys, os

import numpy as np
import shutil
from shutil import copyfile
from mechanoChemML.src.idnn import IDNN, find_wells
from mechanoChemML.src.transform_layer import Transform
from mechanoChemML.workflows.active_learning.hp_search import hyperparameterSearch

from importlib import import_module
from mechanoChemML.workflows.active_learning.data_generation_wrapper import submitCASM, compileCASMOutput, loadCASMOutput
import tensorflow as tf
from sobol_seq import i4_sobol

from tensorflow import keras
from tensorflow.keras.callbacks import CSVLogger, ReduceLROnPlateau, EarlyStopping
from configparser import ConfigParser

############ Active learning class #######

class Active_learning(object):
    """
    Class to define the active learning workflow
    used to create a deep neural network
    representation of the free energy of a system.
    """

    ########################################
    
    def __init__(self,config_path,test=False):

        self.test = test

        if not os.path.exists('training'):
            os.mkdir('training')
        if not os.path.exists('data'):
            os.mkdir('data')
        if not os.path.exists('outputFiles'):
            os.mkdir('outputFiles')

        self.read_config(config_path)
        self.seed = 1 # initial Sobol sequence seed

        # initialize IDNN
        self.hidden_units = [20,20]
        self.lr = 0.2
        self.idnn = IDNN(self.dim,
                         self.hidden_units,
                         transforms=self.IDNN_transforms(),
                         dropout=self.Dropout,
                         unique_inputs=True,
                         final_bias=True)
        self.idnn.compile(loss=['mse','mse',None],
                          loss_weights=[0.01,1,None],
                          optimizer=keras.optimizers.Adagrad(lr=self.lr))

    ########################################

    def read_config(self,config_path):

        config = ConfigParser()
        config.read(config_path)

        self.casm_project_dir = config['DNS']['CASM_project_dir']
        self.job_manager = config['DNS']['JOB_MANAGER']
        self.N_jobs = int(config['HPC']['CPUNum']) #number of processors to use
        self.N_global_pts = int(config['WORKFLOW']['N_global_pts']) #global sampling points each iteration
        self.N_rnds = int(config['WORKFLOW']['Iterations'])
        self.Epochs = int(config['NN']['Epochs'])
        self.Batch_size = int(config['NN']['Batch_size'])
        self.N_hp_sets = int(config['HYPERPARAMETERS']['N_sets'])
        self.Dropout = float(config['HYPERPARAMETERS']['Dropout'])
        self.dim = 4
        if self.job_manager == 'PC' and self.N_jobs > 1:
            self.N_jobs = 1
            self.casm_project_dir = '.'
            print("WARNING: only one processor is allowed for running on your personal computer; CPUNum is overridden by 1")
                        
    ########################################

    def create_test_set(self,N_points,dim,bounds=[0.,1.],seed=1):

        Q = 0.25*np.array([[1, 1, 1, 1],
                           [1, 1, -1, -1],
                           [1, -1, -1, 1],
                           [1, -1, 1, -1]])
        
        # Create test set
        x_test = np.zeros((N_points,dim))
        eta = np.zeros((N_points,dim))
        i = 0
        while (i < N_points):
            x_test[i],seed = i4_sobol(dim,seed)
            x_test[i] = (bounds[1] - bounds[0])*x_test[i] + bounds[0] # shift/scale according to bounds
            eta[i] = np.dot(x_test[i],Q.T).astype(np.float32)
            if eta[i,0] <= 0.25:
                i += 1

        return x_test, eta, seed

    ########################################
    
    def ideal(self,x_test):

        T = 600.
        kB = 8.61733e-5
        Q = 0.25*np.array([[1, 1, 1, 1],
                           [1, 1, -1, -1],
                           [1, -1, -1, 1],
                           [1, -1, 1, -1]])
        invQ = np.linalg.inv(Q)

        g_test = 0.25*kB*T*np.sum((x_test*np.log(x_test) + (1.-x_test)*np.log(1.-x_test)),axis=1)
        mu_test = 0.25*kB*T*np.log(x_test/(1.-x_test)).dot(invQ)

        return mu_test, g_test

    ########################################

    def global_sampling(self,rnd):
        
        # sample with sobol
        if rnd==0:
            x_bounds = [1.e-5,1-1.e-5]
        elif rnd<11:
            x_bounds = [-0.05,1.05]
        else:
            #x_bounds = [-0.02,1.02]
            x_bounds = [0.,1.]
        x_test,eta,self.seed = self.create_test_set(self.N_global_pts,
                                                    self.dim,
                                                    bounds=x_bounds,
                                                    seed=self.seed)

        # approximate mu
        if rnd==0:
            mu_test,_ = self.ideal(x_test)
        else:
            mu_test = 0.01*self.idnn.predict([eta,eta,eta])[1]
            
        # submit casm
        submitCASM(self.N_jobs,mu_test,eta,rnd,casm_project_dir=self.casm_project_dir,test=self.test,job_manager=self.job_manager)
        compileCASMOutput(rnd)

    ########################################
    
    def local_sampling(self,rnd):
        
        # local error
        eta_test, mu_test = loadCASMOutput(rnd-1,singleRnd=True)
        mu_pred = 0.01*self.idnn.predict([eta_test,eta_test,eta_test])[1]
        error = np.sum((mu_pred - mu_test)**2,axis=1)
        etaE =  eta_test[np.argsort(error)[::-1]]

        # find wells
        _,eta_test,self.seed = self.create_test_set(30*self.N_global_pts,
                                                    self.dim,
                                                    seed=self.seed)
        etaW = find_wells(self.idnn,eta_test)

        # randomly perturbed samples
        eta_a = np.repeat(etaE[:200],4,axis=0)
        eta_b = np.repeat(etaE[200:400],2,axis=0)
        eta_c = np.repeat(etaW[:400],4,axis=0)
        if self.test:
            eta_a = np.repeat(etaE[:2],3,axis=0)
            eta_b = np.repeat(etaE[2:4],2,axis=0)
            eta_c = np.repeat(etaW[:4],3,axis=0)
        eta_local = np.vstack((eta_a,eta_b,eta_c))
        eta_local += 0.25*(1.5/(2.*self.N_global_pts**(1./self.dim)))*(np.random.rand(*eta_local.shape)-0.5) #perturb points randomly
        mu_local = 0.01*self.idnn.predict([eta_local,eta_local,eta_local])[1]
        
        # submit casm
        submitCASM(self.N_jobs,mu_local,eta_local,rnd,casm_project_dir=self.casm_project_dir,test=self.test,job_manager=self.job_manager)
        compileCASMOutput(rnd)

    ########################################
        
    def hyperparameter_search(self,rnd):
        # submit
        self.hidden_units, self.lr = hyperparameterSearch(rnd,self.N_hp_sets,job_manager=self.job_manager)

    ########################################
        
    def IDNN_transforms(self):

        def transforms(x):
            h0 = x[:,0]
            h1 = 16.*x[:,1]*x[:,2]*x[:,3]
            h2 = 4.*(x[:,1]*x[:,1] + x[:,2]*x[:,2] + x[:,3]*x[:,3])
            h3 = 64.*(x[:,2]*x[:,2]*x[:,3]*x[:,3] +
                      x[:,1]*x[:,1]*x[:,3]*x[:,3] +
                      x[:,1]*x[:,1]*x[:,2]*x[:,2])
            
            return [h0,h1,h2,h3]

        return transforms

    ########################################
    
    def surrogate_training(self,rnd):
        # read in casm data
        eta_train, mu_train = loadCASMOutput(rnd)

        # shuffle the training set (otherwise, the most recent results
        # will be put in the validation set by Keras)
        inds = np.arange(eta_train.shape[0])
        np.random.shuffle(inds)
        eta_train = eta_train[inds]
        mu_train = mu_train[inds]

        # create energy dataset (zero energy at origin)
        eta_train0 = np.zeros(eta_train.shape)
        g_train0 = np.zeros((eta_train.shape[0],1))
        
        # train
        lr_decay = 0.9**rnd
        self.idnn.compile(loss=['mse','mse',None],
                          loss_weights=[0.01,1,None],
                          optimizer=keras.optimizers.Adagrad(lr=self.lr*lr_decay))
        csv_logger = CSVLogger('training/training_{}.txt'.format(rnd),append=True)
        reduceOnPlateau = ReduceLROnPlateau(factor=0.5,patience=100,min_lr=1.e-4)
        earlyStopping = EarlyStopping(patience=150)
        self.idnn.fit([eta_train0,eta_train,0*eta_train],
                      [100.*g_train0,100.*mu_train,0*mu_train],
                      validation_split=0.25,
                      epochs=self.Epochs,
                      batch_size=self.Batch_size,
                      callbacks=[csv_logger,
                                 reduceOnPlateau,
                                 earlyStopping])
        self.idnn.save('idnn_{}'.format(rnd))

    ########################################
        
    def main_workflow(self):
        """
        Main function outlining the workflow.

        - Global sampling

        - Surrogate training (including hyperparameter search)

        - Local sampling
        """

        for rnd in range(self.N_rnds):
            self.global_sampling(2*rnd)

            if rnd==1:
                self.hyperparameter_search(rnd)
                custom_objects = {'Transform': Transform(self.IDNN_transforms())}
                
                unique_inputs = self.idnn.unique_inputs
                self.idnn = keras.models.load_model('idnn_1',
                                                    custom_objects=custom_objects)
                self.idnn.unique_inputs = unique_inputs
                
            self.surrogate_training(rnd)
            self.local_sampling(2*rnd+1)


====================================================================================================
mechanoChemML\workflows\active_learning\data_generation_surrogate.py
====================================================================================================
from tensorflow import keras
import sys, os

import numpy as np
from mechanoChemML.src.idnn import IDNN
from mechanoChemML.src.transform_layer import Transform
import json

def transforms(x):

    h0 = x[:,0]
    h1 = 16.*x[:,1]*x[:,2]*x[:,3]
    h2 = 4.*(x[:,1]*x[:,1] + x[:,2]*x[:,2] + x[:,3]*x[:,3])
    h3 = 64.*(x[:,2]*x[:,2]*x[:,3]*x[:,3] +
              x[:,1]*x[:,1]*x[:,3]*x[:,3] +
              x[:,1]*x[:,1]*x[:,2]*x[:,2])
    
    return [h0,h1,h2,h3]

print('recreate model...')
hidden_layers = [70, 70]
idnn = IDNN(4,
            hidden_layers,
            transforms=transforms,
            final_bias=True)

idnn.build(input_shape=(1,4))
for i in range(len(hidden_layers)+1):
    w = np.loadtxt(os.path.dirname(__file__)+'/surrogate_weights/weights_{}.txt'.format(i),ndmin=2)
    b = np.loadtxt(os.path.dirname(__file__)+'/surrogate_weights/bias_{}.txt'.format(i),ndmin=1)
    idnn.dnn_layers[i].set_weights([w,b])

print('read input...')
# Read in the casm Monte Carlo input file
input_file = sys.argv[1]
with open(input_file) as fin:
    inputs = json.load(fin)

phi = []
kappa  = []
for comp in inputs["driver"]["conditions_list"]:
    T = comp["temperature"]
    phi.append([comp["phi"][0][0],
                comp["phi"][1][0],
                comp["phi"][2][0],
                comp["phi"][3][0]])
    kappa.append([comp["kappa"][0][0],
                  comp["kappa"][1][0],
                  comp["kappa"][2][0],
                  comp["kappa"][3][0]])

phi = np.array(phi)
eta = np.array(kappa) # Since it's just for testing the workflow, we'll take eta as kappa

print('predicting...')
pred = idnn.predict(eta)
mu = pred[1]
kappa = eta + 0.5*mu/phi # Back out what kappa would have been

#keras.backend.clear_session()

print('write output...')
# Write out a limited CASM-like results file
results = {"T": len(eta)*[T]}
for i in range(4):
    results["kappa_{}".format(i)] = kappa[:,i].tolist()
    results["phi_{}".format(i)] = phi[:,i].tolist()
    results["<op_val({})>".format(i)] = eta[:,i].tolist()

with open('results.json','w') as fout:
    json.dump(results,fout,sort_keys=True, indent=4)


====================================================================================================
mechanoChemML\workflows\active_learning\data_generation_wrapper.py
====================================================================================================
#!/usr/bin/env python

import json
import numpy as np
from subprocess import check_output, STDOUT
import os, shutil, copy
import fileinput as fin
from time import sleep
from shutil import copyfile

def submitCASM(N_jobs,mu_test,eta,rnd,casm_project_dir='.',test=False,job_manager='LSF'):

    # Calculate and write out the predicted kappa values to the CASM input files
    n = len(eta)
    phi = np.array(n*[[5.,3.,3.,3.]])
    kappa = eta + 0.5*mu_test/phi
    T = np.array(n*[[600.]])

    dataOut = np.hstack((kappa,eta,phi,T,mu_test))
    np.savetxt('data/CASMinput{}.txt'.format(rnd),
               dataOut,
               fmt='%.12f',
               header='kappa_0 kappa_1 kappa_2 kappa_3 eta_0 eta_1 eta_2 eta_3 phi_0 phi_1 phi_2 phi_3 T mu_0 mu_1 mu_2 mu_3')

    kappa = np.expand_dims(kappa,-1).tolist()

    with open(os.path.dirname(__file__)+'/monte_settings.json.tmpl','r') as tmplFile:

        tmpl = json.load(tmplFile)
        for job in range(N_jobs):
            shutil.rmtree('job_{}'.format(job+1),ignore_errors=True)
            os.mkdir('job_{}'.format(job+1))

            inputF = copy.deepcopy(tmpl)

            for i in range(job,len(kappa),N_jobs):
                inputF['driver']['conditions_list']+=[{'tolerance': 0.001,
                                                       'temperature': 600.0,
                                                       'phi': [[5.],[3.],[3.],[3.]],
                                                       'kappa': kappa[i]}]

            with open('job_{0}/monte_settings_{0}.json'.format(job+1),'w') as outFile:
                json.dump(inputF,outFile,indent=4)
    
    if test:
        # using a data-generation surrogate instead of CASM
        if job_manager == 'PC':
            command = 'cd job_1; python -u {}/data_generation_surrogate.py monte_settings_1.json; cd ../'.format(os.path.dirname(__file__))
        elif job_manager == 'LSF':
            command = ['cd job_$LSB_JOBINDEX'.format(casm_project_dir),
                       'python -u {}/data_generation_surrogate.py monte_settings_$LSB_JOBINDEX.json'.format(os.path.dirname(__file__)),
                       'cd ../'] 
        elif job_manager == 'slurm':
            command = ['cd job_$SLURM_ARRAY_TASK_ID'.format(casm_project_dir),
                       'python -u {}/data_generation_surrogate.py monte_settings_$SLURM_ARRAY_TASK_ID.json'.format(os.path.dirname(__file__)),
                       'cd ../']
    else:
        if job_manager == 'PC':
            raise Exception('JOB_MANAGER cannot be set to PC -- running CASM on PC is not supported. You can run python main_test.py to use a surrogate for data generation')
        else:
            command = ['cwd=$PWD',
                    'mv job_$LSB_JOBINDEX {}'.format(casm_project_dir),
                    'cd {}/job_$LSB_JOBINDEX'.format(casm_project_dir),
                    '$CASMPREFIX/bin/casm monte -s monte_settings_$LSB_JOBINDEX.json',
                    'cd ../',
                    'mv job_$LSB_JOBINDEX $cwd']

    if job_manager == 'PC':
        from subprocess import call
        call(command,shell=True)
    else:
        if job_manager == 'LSF':
            from mechanoChemML.workflows.active_learning.LSF_manager import submitJob, waitForAll
            specs = {'job_name':'CASM_[1-{}]'.format(N_jobs),
                    'queue': 'gpu_p100',
                    'output_folder':'outputFiles'}
            name = 'CASM*'
        elif job_manager == 'slurm':
            from mechanoChemML.workflows.active_learning.slurm_manager import submitJob, waitForAll
            specs = {'job_name':'CASM',
                    'array': '1-{}'.format(N_jobs),
                    'account': 'TG-MCH200011',
                    'walltime': '2:00:00',
                    'total_memory':'3G',
                    'output_folder':'outputFiles'}
            name = 'CASM'
            
        submitJob(command,specs)
        waitForAll(name)


def compileCASMOutput(rnd):
    kappa = []
    eta = []
    phi = []
    T = []
    for dir in os.listdir('.'):
        if 'job' in dir:
            if os.path.exists(dir+'/results.json'):
                with open(dir+'/results.json','r') as file:
                    data = json.load(file)
                    kappa += np.array([data['kappa_{}'.format(i)] for i in range(4)]).T.tolist()
                    eta += np.array([data['<op_val({})>'.format(i)] for i in range(4)]).T.tolist()
                    phi += np.array([data['phi_{}'.format(i)] for i in range(4)]).T.tolist()
                    T += np.array([data['T']]).T.tolist()

    kappa = np.array(kappa)
    eta = np.array(eta)
    phi = np.array(phi)
    T = np.array(T)
    mu = -2.*phi*(eta - kappa)
    dataOut = np.hstack((kappa,eta,phi,T,mu))
    dataOut = dataOut[~np.isnan(dataOut).any(axis=1)] #remove any rows with nan
    np.savetxt('data/results{}.txt'.format(rnd),
               dataOut,
               fmt='%.12f',
               header='kappa_0 kappa_1 kappa_2 kappa_3 eta_0 eta_1 eta_2 eta_3 phi_0 phi_1 phi_2 phi_3 T mu_0 mu_1 mu_2 mu_3')
    if rnd==0:
        copyfile('data/results{}.txt'.format(rnd),'data/allResults{}.txt'.format(rnd))
    else:
        allResults = np.loadtxt('data/allResults{}.txt'.format(rnd-1))
        allResults = np.vstack((allResults,dataOut))
        np.savetxt('data/allResults{}.txt'.format(rnd),
                   allResults,
                   fmt='%.12f',
                   header='kappa_0 kappa_1 kappa_2 kappa_3 eta_0 eta_1 eta_2 eta_3 phi_0 phi_1 phi_2 phi_3 T mu_0 mu_1 mu_2 mu_3')

def loadCASMOutput(rnd,singleRnd=False):

    if singleRnd:
        dataIn = np.genfromtxt('data/results'+str(rnd)+'.txt',dtype=np.float32)[:,[4,5,6,7,13,14,15,16]]
    else:
        dataIn = np.genfromtxt('data/allResults'+str(rnd)+'.txt',dtype=np.float32)[:,[4,5,6,7,13,14,15,16]]
    features = dataIn[:,:4]
    labels = dataIn[:,4:]

    return features, labels


====================================================================================================
mechanoChemML\workflows\active_learning\hp_search.py
====================================================================================================
#!/usr/bin/env python

import os
import fileinput as fin
import shutil
from shutil import copyfile
from operator import itemgetter

from importlib import import_module
from time import sleep

def submitHPSearch(n_sets,rnd,job_manager):
    """ A function to submit the job scripts for a each set of hyperparameters
    in the hyperparameter search in the active learning workflow.

    (Still needs to be generalized).

    :param n_sets: The number of hyperparameter sets to run.
    :type n_sets: int

    :param rnd: The current round (workflow iteration) number.
    :type rnd: int
    
    """
    if job_manager=='PC':
        from subprocess import call
    elif job_manager == 'LSF':
        from mechanoChemML.workflows.active_learning.LSF_manager import submitJob, waitForAll
        specs = {'job_name':'optimizeHParameters',
                 'queue': 'gpu_p100',
                 'output_folder':'outputFiles'}
    elif job_manager == 'slurm':
        from mechanoChemML.workflows.active_learning.slurm_manager import submitJob, waitForAll
        specs = {'job_name':'optimizeHParameters',
                 'account': 'TG-MCH200011',
                 'total_memory':'3G',
                 'output_folder':'outputFiles',
                 'queue': 'shared'}

    # Compare n_sets of random hyperparameters; choose the set that gives the lowest l2norm
    for i in range(n_sets):
        read = 0 # read in previous hyperparameters if read not zero
        if (i < 5):
            read = i+1

        command = ['python '+os.path.dirname(__file__)+'/optimize_hparameters.py '+str(i)+' '+str(read)+' '+str(rnd)]
        if job_manager=='PC':
            call(command[0],shell=True)
        else:            
            submitJob(command,specs)

    if job_manager!='PC':
        waitForAll('optimizeHParameters')


def hyperparameterSearch(rnd,N_sets,job_manager='LSF'):
    """ A function that initializes and manages the hyperparameter search in the active learning workflow.

    (Still needs to be generalized).

    :param N_sets: The number of hyperparameter sets to run.
    :type N_sets: int

    :param rnd: The current round (workflow iteration) number.
    :type rnd: int
    
    """
    
    # Submit the training sessions with various hyperparameters
    submitHPSearch(N_sets,rnd,job_manager)

    # Wait for jobs to finish
    #sleep(20)
    #while ( numCurrentJobs('optimizeHParameters') > 0):
    #    sleep(15)

    # Compare n_sets of random hyperparameters; choose the set that gives the lowest l2norm
    hparameters = []
    for i in range(N_sets):
        filename = 'hparameters_'+str(i)+'.txt'
        if os.path.isfile(filename):
            fin = open(filename,'r')
            exec (fin.read()) # execute the code snippet written as a string in the read file
            fin.close()
            os.remove('hparameters_'+str(i)+'.txt')

    # Sort by l2norm
    sortedHP = sorted(hparameters,key=itemgetter(3))

    writeHP = open('data/sortedHyperParameters_'+str(rnd)+'.txt','w')
    writeHP.write('learning_rate,hidden_units,round/set,l2norm\n')
    for set in sortedHP:
        writeHP.write(str(set[0])+','+str(set[1])+',"'+str(set[2])+'",'+str(set[3])+'\n')
    writeHP.close()

    # Clean up checkpoint files
    copyfile('training/training_{}.txt'.format(sortedHP[0][2]),'training/training_{}.txt'.format(rnd))
    shutil.rmtree('idnn_{}'.format(rnd),ignore_errors=True)
    os.rename('idnn_{}'.format(sortedHP[0][2]),'idnn_{}'.format(rnd))
    for i in range(N_sets):
        shutil.rmtree('idnn_{}_{}'.format(rnd,i),ignore_errors=True)

    return sortedHP[0][1],sortedHP[0][0] #hidden_units, learning_rate


====================================================================================================
mechanoChemML\workflows\active_learning\LSF_manager.py
====================================================================================================
#!/usr/bin/env python

import numpy as np
from subprocess import check_output, STDOUT
import os
from time import sleep

def numCurrentJobs(name):
    return len(check_output(['bjobs','-J',name],stderr=STDOUT).decode("utf-8").split('\n'))-2

def submitJob(command,specs={},is_dnsml=False):

    # Default values for LSF job script
    default = {'wall_time':'12:00',
               'n_processes':1,
               'total_memory':5000,
               'mem_per_process':1000,
               'job_name':'default',
               'output_folder':'.',
               'queue':'normal'}

    # Incoporate any changes to the defaults
    default.update(specs)

    # Write out job script
    with open('submit.lsf','w') as fout:
        fout.write('#!/bin/bash\n#\n')
        if is_dnsml:
            fout.write('#BSUB -a tbb')
        fout.write("#BSUB -W {}                  # wall time\n".format(default['wall_time']))
        fout.write("#BSUB -n {}                  # n processes\n".format(default['n_processes']))
        fout.write("#BSUB -R rusage[mem={}]       # amount of total memory in MB for all processes\n".format(default['total_memory']))
        fout.write('#BSUB -R "span[ptile={}]"        # number of processes per host\n'.format(min(20,default["n_processes"])))
        #fout.write('#BSUB -R "affinity[thread(1):cpubind=thread:distribute=pack]"   # thread(t) means "t" threads per tasks\n')
        fout.write('#BSUB -R "affinity[core(1):cpubind=core:distribute=balance]" # bind 1 core per process\n')
        fout.write('#BSUB -M {}                   # amount of memory in MB per process\n'.format(default["mem_per_process"]))
        fout.write('#BSUB -J "{}"    # job name\n'.format(default["job_name"]))
        fout.write('#BSUB -e {}/errors.%J       # error file name in which %J is replaced by the job ID \n'.format(default["output_folder"]))
        fout.write('#BSUB -o {}/output.%J       # output file name in which %J is replaced by the job ID\n'.format(default["output_folder"]))
        fout.write('#BSUB -q {}                       # choose the queue to use: gpu_p100, normal or large_memory\n\n'.format(default["queue"]))

        if isinstance(command, list):
            for item in command:
                fout.write(item)
                fout.write('\n')
        else:
            fout.write(command)

    # Submit script
    os.system('bsub < submit.lsf')
        
def waitForAll(name,interval=15):
    sleep(2*interval)
    while ( numCurrentJobs(name) > 0):
        sleep(interval)


====================================================================================================
mechanoChemML\workflows\active_learning\optimize_hparameters.py
====================================================================================================
#!/usr/bin/env python

import sys, os

import numpy as np
from mechanoChemML.src.idnn import IDNN
from mechanoChemML.workflows.active_learning.data_generation_wrapper import loadCASMOutput
import sys
from tensorflow import keras

set_i = int(sys.argv[1])
read = int(sys.argv[2])
rnd = int(sys.argv[3])

# Randomly choose hyperparameters
if (read == 0 or rnd <= 2):
    learning_rate = 5.*np.power(10.,-1.-2.*np.random.rand(1)[0],dtype=np.float32)
    n_layers = 2
    hidden_units = n_layers*[np.random.randint(20,200)]
else:
    readHP = open('data/sortedHyperParameters_'+str(rnd-1)+'.txt','r')
    HP = readHP.read().splitlines()
    exec ('hparam = ['+HP[read]+']')
    learning_rate = hparam[0]
    hidden_units = hparam[1]

def IDNN_transforms():

    def transforms(x):
        h0 = x[:,0]
        h1 = 16.*x[:,1]*x[:,2]*x[:,3]
        h2 = 4.*(x[:,1]*x[:,1] + x[:,2]*x[:,2] + x[:,3]*x[:,3])
        h3 = 64.*(x[:,2]*x[:,2]*x[:,3]*x[:,3] +
                  x[:,1]*x[:,1]*x[:,3]*x[:,3] +
                  x[:,1]*x[:,1]*x[:,2]*x[:,2])
        
        return [h0,h1,h2,h3]

    return transforms

# Callbacks for training either model
csv_logger = keras.callbacks.CSVLogger('training/training_{}_{}.txt'.format(rnd,set_i),append=True)
reduceOnPlateau = keras.callbacks.ReduceLROnPlateau(factor=0.5,
                                                    patience=100,
                                                    min_lr=1.e-4)
earlyStopping = keras.callbacks.EarlyStopping(patience=150)

# Define model(s)
idnn = IDNN(4,
            hidden_units,
            transforms=IDNN_transforms(),
            dropout=0.06,
            unique_inputs=True,
            final_bias=True)
idnn.compile(loss=['mse','mse',None],
             loss_weights=[0.01,1,None],
             optimizer=keras.optimizers.Adagrad(lr=np.float32(learning_rate)))

# read in casm data
eta_train, mu_train = loadCASMOutput(rnd)
        
# shuffle the training set (otherwise, the most recent results
# will be put in the validation set by Keras)
inds = np.arange(eta_train.shape[0])
np.random.shuffle(inds)
eta_train = eta_train[inds]
mu_train = mu_train[inds]
        
# create energy dataset (zero energy at origin)
eta_train0 = np.zeros(eta_train.shape)
g_train0 = np.zeros((eta_train.shape[0],1))
        
# train
history = idnn.fit([eta_train0,eta_train,eta_train],
                   [100.*g_train0,100.*mu_train,0*mu_train],
                   validation_split=0.25,
                   epochs=250,
                   batch_size=100,
                   callbacks=[csv_logger,
                              reduceOnPlateau,
                              earlyStopping])
idnn.save('idnn_{}_{}'.format(rnd,set_i))

valid_loss = history.history['val_loss'][-1]

# Write out hyperparameters and l2norm
if not np.isnan(valid_loss):
    fout = open('hparameters_'+str(set_i)+'.txt','w')
    fout.write('hparameters += [['+str(learning_rate)+','+str(hidden_units)+',"'+str(rnd)+'_'+str(set_i)+'",'+str(valid_loss)+']]')
    fout.close()


====================================================================================================
mechanoChemML\workflows\active_learning\save_weights.py
====================================================================================================
import numpy as np
import json
from tensorflow import keras

import sys, os

from mechanoChemML.src.idnn import IDNN
from mechanoChemML.src.transform_layer import Transform

def transforms(x):

    h0 = x[:,0]
    h1 = 16.*x[:,1]*x[:,2]*x[:,3]
    h2 = 4.*(x[:,1]*x[:,1] + x[:,2]*x[:,2] + x[:,3]*x[:,3])
    h3 = 64.*(x[:,2]*x[:,2]*x[:,3]*x[:,3] +
              x[:,1]*x[:,1]*x[:,3]*x[:,3] +
              x[:,1]*x[:,1]*x[:,2]*x[:,2])
    
    return [h0,h1,h2,h3]


rnd = 12
idnn = keras.models.load_model(f'idnn_{rnd}',
                               custom_objects={'Transform': Transform(transforms)})
i = 0
weights = []
biases = []
for layer in idnn.layers[1:]:
    w = layer.get_weights()
    if len(w)==2:
        weights.append(w[0])
        biases.append(w[1])
    elif len(w)==1:
        weights.append(w[0])

last = max(len(weights) - 1,len(biases) - 1)

for i,weight in enumerate(weights):
    if i == last:
        weight *= 0.01
    np.savetxt('weights_'+str(i)+'.txt',weight,header=str(weight.shape[0])+' '+str(weight.shape[1]))

for i,bias in enumerate(biases):
    if i == last:
        bias *= 0.01
    np.savetxt('bias_'+str(i)+'.txt',bias,header=str(bias.shape[0]))


====================================================================================================
mechanoChemML\workflows\active_learning\slurm_manager.py
====================================================================================================
#!/usr/bin/env python

import numpy as np
from subprocess import check_output, STDOUT
import os
from time import sleep

def numCurrentJobs(name):
    try:
        num = len(check_output(['squeue','-n',name],stderr=STDOUT).decode("utf-8").split('\n'))-2
    except:
        num = 1
        print('Error with numCurrentJobs: ',check_output(['squeue','-n',name],stderr=STDOUT).decode("utf-8"))
    return num

def checkPending(name):
    try:
        val = False
        jobs = check_output(['squeue','-n',name],stderr=STDOUT).decode("utf-8").split('\n')
        for job in jobs:
            if 'PD' in job:
                val = True
                break
    except:
        val = False
        print('Error with check pending: ',check_output(['squeue','-n',name],stderr=STDOUT).decode("utf-8"))
    return val


def submitJob(command,specs={},is_dnsml=False):

    # Default values for LSF job script
    default = {'wall_time':'12:00:00',
               'nodes':1,
               'ntasks-per-node':1,
               'total_memory':'1G',
               'job_name':'default',
               'output_folder':'.',
               'queue':'shared'}

    # Incoporate any changes to the defaults
    default.update(specs)

    # Write out job script
    with open('submit.slrm','w') as fout:
        fout.write('#!/bin/bash\n#\n')
        fout.write("#SBATCH -t {}                  # wall time\n".format(default['wall_time']))
        if 'account' in default:
            fout.write("#SBATCH -A {}                  # account\n".format(default['account']))
        fout.write("#SBATCH --nodes {}                  \n".format(default['nodes']))
        fout.write("#SBATCH --ntasks-per-node={}                  \n".format(default['ntasks-per-node']))
        fout.write("#SBATCH --mem={}\n".format(default['total_memory']))
        fout.write('#SBATCH -J {}    # job name\n'.format(default["job_name"]))
        if 'gpu' in default['queue']:
            fout.write('#SBATCH --gpus=1')
        if 'array' in default:
            fout.write("#SBATCH --array={}                  # job array\n".format(default['array']))
        fout.write('#SBATCH -e {}/errors.%J       # error file name in which %J is replaced by the job ID \n'.format(default["output_folder"]))
        fout.write('#SBATCH -o {}/output.%J       # output file name in which %J is replaced by the job ID\n'.format(default["output_folder"]))
        fout.write('#SBATCH -p {}                       # choose the queue (partition) to use\n\n'.format(default["queue"]))
        fout.write('#SBATCH --export=ALL\n\n')
        
        if isinstance(command, list):
            for item in command:
                fout.write(item)
                fout.write('\n')
        else:
            fout.write(command)

    # Submit script
    os.system('sbatch submit.slrm')
        
def waitForAll(name,interval=15):
    sleep(2*interval)
    while ( numCurrentJobs(name) > 0 or checkPending(name) ):
        sleep(interval)


====================================================================================================
mechanoChemML\workflows\mr_learning\__init__.py
====================================================================================================


====================================================================================================
mechanoChemML\workflows\mr_learning\mrnn_models.py
====================================================================================================
# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import regularizers
import numpy as np
import sys, os, datetime
import mechanoChemML.workflows.mr_learning.mrnn_utility as mrnn_utility

############################### learning rate #####################################
def build_learningrate(config):
    """
    Build different learning rates based on the input
    """
    LR = mrnn_utility.getlist_str(config['MODEL']['LearningRate'])
    # print('LR str: ', LR)
    if (len(LR) == 1):
        LearningRate = float(LR[0])
        # print('--Decay in mono rate: rate = ', LearningRate)
    elif (len(LR) > 1):
        LR_type = LR[0]
        if (LR_type == 'mono'):
            LearningRate = float(LR[1])
        elif (LR_type == 'decay_exp'):
            initial_learning_rate = 0.001
            decay_steps = 1000
            decay_rate = 0.96
            initial_learning_rate = float(LR[1])
            if len(LR) > 2:
                decay_steps = int(LR[2])
            if len(LR) > 3:
                decay_rate = float(LR[3])
            # print('--Decay in exponential rate: initial rate = ', initial_learning_rate, ', decay steps = ', decay_steps, ', decay_rate = ', decay_rate)

            if (mrnn_utility.get_package_version(tf.__version__)[0] == 1 and mrnn_utility.get_package_version(tf.__version__)[1] <= 13):
                global_step = tf.Variable(0, name='global_step', trainable=False)
                global_step = tf.train.get_global_step()
                LearningRate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps=decay_steps, decay_rate=decay_rate, staircase=True)
            else:
                # print("!!!! Caution: use Learning rate with care, there were occasions that tf1.13 should better performance on training. !!!")
                global_step = tf.Variable(0, name='global_step', trainable=False)
                LearningRate = tf.compat.v1.train.exponential_decay(initial_learning_rate, global_step, decay_steps=decay_steps, decay_rate=decay_rate, staircase=True)
        else:
            raise ValueError('unknown choice for learning rate (mono, decay_exp): ', LR)

    return LearningRate


###################################################################################

############################### optimizer #########################################

def build_optimizer(config):
    """
    Build different optimizers based on the input
    """

    ModelOptimizer = config['MODEL']['Optimizer']
    LearningRate = build_learningrate(config)

    # print('Avail Optimizer: ', ['adam', 'sgd', 'adadelta', 'gradientdescentoptimizer'])

    if (ModelOptimizer.lower() == "adam".lower()):
        optimizer = tf.keras.optimizers.Adam(LearningRate)
        return optimizer
    elif (ModelOptimizer.lower() == "sgd".lower()):
        optimizer = tf.keras.optimizers.SGD(LearningRate)
        return optimizer
    elif (ModelOptimizer.lower() == "adadelta".lower()):
        optimizer = tf.keras.optimizers.Adadelta(LearningRate)
        return optimizer
    elif (ModelOptimizer.lower() == "gradientdescentoptimizer".lower()):
        optimizer = tf.compat.v1.train.GradientDescentOptimizer(LearningRate)
        return optimizer
    elif (ModelOptimizer.lower() == "user".lower()):
        raise ValueError('Model optimizer = ', ModelOptimizer, ' is chosen, but is not implemented!')
    else:
        raise ValueError('Model optimizer = ', ModelOptimizer, ' is chosen, but is not implemented!')


###################################################################################


############################# call back ####################################
class callback_PrintDot(keras.callbacks.Callback):
    """
    Display training progress by printing a single dot for each completed epoch
    """
    def on_epoch_end(self, epoch, logs):
        if epoch % 100 == 0:
            print('epoch: ', epoch, 'loss: ', logs['loss'], 'val_loss: ', logs['val_loss'])

def check_point_callback(config):
    """
    Return the check point call back function
    """
    checkpoint_dir = config['RESTART']['CheckPointDir'] + config['MODEL']['ParameterID']
    checkpoint_path = checkpoint_dir + "/cp-{epoch:04d}.ckpt"  
    period = int(config['RESTART']['CheckPointPeriod'])
    verbose = int(config['MODEL']['Verbose'])
    cp_callback = tf.keras.callbacks.ModelCheckpoint(
        checkpoint_path,
        verbose=verbose,
        save_weights_only=True,
    # Save weights, every 5-epochs.
        period=period)
    return cp_callback


def tensor_board_callback(config):
    """
    Return the tensor board call back function
    """
    tensorboard_dir = config['OUTPUT']['TensorBoardDir'] + config['MODEL']['ParameterID']
    log_dir = tensorboard_dir + '/' + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    if (mrnn_utility.get_package_version(tf.__version__)[0] == 1):
        tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)
    elif (mrnn_utility.get_package_version(tf.__version__)[0] == 2):
        tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=2)
    else:
        raise ValueError("unknown tf version for tensor board callback support")
    return tb_callback


def build_callbacks(config):
    """
    Build different call back functions based on the input
    """
    callbacks = []
    callback_names = mrnn_utility.getlist_str(config['MODEL']['CallBacks'])

    if 'checkpoint' in callback_names:
        callbacks.append(check_point_callback(config))

    if 'tensorboard' in callback_names:
        callbacks.append(tensor_board_callback(config))

    if 'printdot' in callback_names:
        callbacks.append(callback_PrintDot())

    return callbacks


###############################################################################

################################## loss ############################################
def my_mse_loss():
    """
    Use defined mse loss function
    """
    def loss(y_true, y_pred):
        return tf.reduce_mean(tf.square(y_pred - y_true))
    return loss


def my_mse_loss_with_grad(BetaP=1000.0):
    """
    Use defined mse loss function with penalty term for physics-based constraint
    """
    def loss(y_true, y_pred):
        # compute P based on S
        S_NN = y_pred[:, 1:5]
        S_NN = tf.reshape(S_NN, [-1, 2, 2])

        P_DNS = y_true[:, 1:5]
        P_DNS = tf.reshape(P_DNS, [-1, 2, 2])

        F_DNS = y_true[:, 5:9]
        F_DNS = tf.reshape(F_DNS, [-1, 2, 2])

        P_NN = tf.linalg.matmul(F_DNS, S_NN)

        P_NN = tf.reshape(P_NN, [-1, 4])
        P_DNS = tf.reshape(P_DNS, [-1, 4])

        return tf.reduce_mean(tf.square(y_pred[:, 0] - y_true[:, 0])) + BetaP * tf.reduce_mean(tf.square(P_NN - P_DNS))
    # Return a function
    return loss


def build_loss(config, loss_model=None):
    """
    Build loss function based on the input
    """

    ModelLoss = config['MODEL']['Loss']

    if (ModelLoss.lower() == "mse".lower()):
        if (tf.__version__[0:1] == '1'):
            loss = tf.keras.losses.MSE()
        else:
            loss = tf.keras.losses.MeanSquaredError()
        return loss
    elif (ModelLoss.lower() == "my_mse_loss".lower()):
        loss = my_mse_loss()
        return loss
    elif (ModelLoss.lower() == "my_mse_loss_with_grad".lower()):
        loss = my_mse_loss_with_grad()
        return loss
    else:
        raise ValueError('Model loss = ', ModelLoss, ' is chosen, but is not implemented!')
###################################################################################


##################################### model ##########################################
def shift_labels(config, dataset, dataset_index, dataset_frame, data_file):
    """
    Shift label based on the trained-NN predictions
    """

    # print("---!!!!--- reach shift_labels!!!!")
    # print("---!!!!--- Remember to modify old 'std', old 'mean' for DNN based KBNN")
    trained_model_lists = mrnn_utility.getlist_str(config['KBNN']['LabelShiftingModels'])
    if len(trained_model_lists) > 0:
        # all_fields = mrnn_utility.getlist_str(config['TEST']['AllFields'])
        label_fields = mrnn_utility.getlist_str(config['TEST']['LabelFields'])

        if len(label_fields) > 1:
            # raise ValueError(
            # 'Shift labels is not working for two labels shifting yet!')
            print('Shift labels is not working for two labels shifting yet!')

        # if label_fields[0] != all_fields[-1]:
        # raise ValueError('the single label for KBNN should put at the end of all label fields!')

        # print("---!!!!---  load trained model!!!!")
        old_models = load_trained_model(trained_model_lists)
        # print("---!!!!---  after load trained model!!!!")
        key0 = label_fields[0]
        old_label_scale = mrnn_utility.getlist_float(config['KBNN']['OldShiftLabelScale'])

        # print('old shift features: ', config['KBNN']['OldShiftFeatures'])

        # to switch between vtk and other features
        if (config['KBNN']['OldShiftFeatures'].find('.vtk') >= 0):
            # """ """
            # print("--- here: vtk for label shift")
            # index should not be used anymore.
            # use base frame info to do the base free energy shifting
            dataset_old = mrnn_utility.load_data_from_vtk_for_label_shift_frame(config, dataset_frame, normalization_flag=True, verbose=0)
        elif (config['KBNN']['OldShiftFeatures'].find('.npy') >= 0):
            # """ """
            # print("--- here: npy for label shift")
            # index should not be used anymore.
            # use base frame info to do the base free energy shifting
            dataset_old = mrnn_utility.load_data_from_npy_for_label_shift_frame(config, dataset_frame, normalization_flag=True, verbose=0)
        else:
            old_feature_fields = mrnn_utility.getlist_str(config['KBNN']['OldShiftFeatures'])
            raw_dataset_old = mrnn_utility.read_csv_fields(data_file, old_feature_fields)
            dataset_old = raw_dataset_old.copy()

            if len(old_feature_fields) > 0:
                old_mean = mrnn_utility.getlist_float(config['KBNN']['OldShiftMean'])
                old_std = mrnn_utility.getlist_float(config['KBNN']['OldShiftStd'])
                old_data_norm = int(config['KBNN']['OldShiftDataNormOption'])
                if (old_data_norm != 2):
                    dataset_old = (dataset_old - old_mean) / old_std
                else:
                    dataset_old = (dataset_old - old_mean) / old_std + 0.5
                    raise ValueError("This part is not carefully checked. Please check it before you disable it.")

        if (len(old_models) > 0):
            # convert dataset_old to numpy data array in case it is not
            try:
                dataset_old = dataset_old.to_numpy()
            except:
                try:
                    dataset_old = dataset_old.numpy()
                except:
                    pass
                pass

            for model0 in old_models:
                label_shift_amount = []
                batch_size = int(config['MODEL']['BatchSize'])
                print('run...', model0)

                # use model.predict() will run it in the eager mode and evaluate the tensor properly.
                for i0 in range(0, len(dataset_old), batch_size):
                    tmp_shift = model0.predict(mrnn_utility.special_input_case(dataset_old[i0:i0 + batch_size])) / old_label_scale    # numpy type
                    label_shift_amount.extend(tmp_shift)

        for i0 in range(0, len(dataset[key0])):
            a = dataset[key0][i0] - label_shift_amount[i0]
            # tf1.13
            if (i0 % 200 == 0):
                print('--i0--', i0, 'DNS:', dataset[key0][i0], '\t', 'NN:', label_shift_amount[i0], ' key0 = ', key0, ' dataset size = ', len(dataset[key0]),
                      ' label shift size = ', len(label_shift_amount))
            # for tf2.0
            # print('--i0--',i0, 'DNS:', dataset[key0][i0],'\t', 'NN:',label_shift_amount[i0].numpy()[0], '\t', a.numpy()[0], '\t', abs(a.numpy()[0]/dataset[key0][i0])*100, 'new label', new_label[key0][i0])
            dataset[key0][i0] = dataset[key0][i0] - label_shift_amount[i0]




def load_one_model(old_config_file):
    """ 
    Based on the config file name to load pre-saved model info
    """
    # print('old_config_file:', old_config_file)
    old_config = mrnn_utility.read_config_file(old_config_file, False)
    old_data_file = old_config['TEST']['DataFile']
    if old_data_file.find('.csv') >= 0:
        dummy_train_dataset, dummy_train_labels = mrnn_utility.generate_dummy_dataset(old_config)
        # print('dummy_train_dataset:', dummy_train_dataset, 'dummy label: ', dummy_train_labels)
    elif old_data_file.find('.vtk') >= 0:
        dummy_train_dataset, dummy_train_labels, _, _, _, _, _, _ = mrnn_utility.load_data_from_vtk_database(old_config, normalization_flag=True, verbose=0)
    else:
        print('***WARNING***: unknown datafile in old_config file for KBNN, old_datafile = ', old_data_file)
        print('               could potentially lead to errors!!!!')

    old_model = build_model(old_config, dummy_train_dataset, dummy_train_labels, set_non_trainable=True)

    if (old_config['RESTART']['SavedCheckPoint'] != ''):
        saved_check_point = old_config_file[0:old_config_file.rfind('/')] + '/' + old_config['RESTART']['SavedCheckPoint']
        print('saved_check_point: ', saved_check_point)
        old_model.load_weights(saved_check_point)
        print("...loading saved model ...:", old_config['RESTART']['SavedCheckPoint'], old_config_file.rfind('/'), saved_check_point)
        return old_model
    else:
        print("You have to provided the saved check point location in the old config file. Exiting...")
        exit(0)
    return old_model


def load_trained_model(trained_model_lists):
    """
    Load trained models
    """
    old_models = []
    if (len(trained_model_lists) > 0):
        for m0 in trained_model_lists:
            model0 = load_one_model(m0)
            # print('Pre-trained Model summary: (before) ', m0, model0)
            model0.summary()
            # print('Pre-trained Model summary: (after) ', m0)
            old_models.append(model0)
    return old_models




class user_DNN_kregl1l2_gauss_grad(tf.keras.Model):
    """
    DNN with regularization layers and Gaussian noise layers.
    """
    def __init__(self, config, NodesList, Activation, train_dataset, train_labels, label_scale, train_stats):
        super(user_DNN_kregl1l2_gauss_grad, self).__init__()
        self.label_scale = label_scale
        self.train_stats_std = train_stats['std'].to_numpy()[0:3]    # E11, E12, E22

        kreg_l1 = 0.0
        kreg_l2 = 0.0
        gauss_noise = 0.0

        try:
            kreg_l2 = float(config['MODEL']['KRegL2'])
            # print('l2 regularize could potential cause the loss != mse')
        except:
            pass

        try:
            kreg_l1 = float(config['MODEL']['KRegL1'])
            # print('l1 regularize could potential cause the loss != mse')
        except:
            pass

        try:
            gauss_noise = float(config['MODEL']['GaussNoise'])
            # print('gauss noise could potential cause the loss != mse')
        except:
            pass

        if (kreg_l1 < 0 or kreg_l2 < 0 or gauss_noise < 0):
            raise ValueError('***ERR***: regularizer or guass noise are < 0! They should be > 0. kreg_l1 = ', kreg_l1, 'kreg_l2 = ', kreg_l2, 'gauss_noise = ', gauss_noise)

        self.all_layers = []

        if kreg_l2 > 0 and kreg_l1 == 0:
            self.all_layers.append(
                layers.Dense(
                    NodesList[0],
                    activation=Activation[0],
                    input_shape=[len(train_dataset.keys())],
                    name='input',
                    kernel_regularizer=regularizers.l2(kreg_l2),
                    kernel_initializer='random_uniform'))
        elif kreg_l1 > 0 and kreg_l2 == 0:
            self.all_layers.append(
                layers.Dense(
                    NodesList[0],
                    activation=Activation[0],
                    input_shape=[len(train_dataset.keys())],
                    name='input',
                    kernel_regularizer=regularizers.l1(kreg_l1),
                    kernel_initializer='random_uniform'))
        elif kreg_l1 > 0 and kreg_l2 > 0:
            raise ValueError('you can not use both l1 and l2 kernel regularizer: try just use l2')
        else:
            self.all_layers.append(layers.Dense(NodesList[0], activation=Activation[0], input_shape=[len(train_dataset.keys())], name='input', kernel_initializer='random_uniform'))

        # first hidden layer
        if gauss_noise > 0.0:
            self.all_layers.append(layers.GaussianNoise(gauss_noise))

        # remaining hidden layer
        for i0 in range(1, len(NodesList)):
            name_str = 'dense-' + str(i0)
            self.all_layers.append(layers.Dense(NodesList[i0], activation=Activation[i0], name=name_str, kernel_initializer='random_uniform'))

        # output layer
        self.all_layers.append(layers.Dense(1, name='output'))

    @tf.function(autograph=False)
    def call(self, inputs):
        with tf.GradientTape() as g:
            g.watch(inputs)
            y1 = self.all_layers[0](inputs)    #,
            for hd in self.all_layers[1:]:
                y2 = hd(y1)
                y1 = y2
        dy_dx = g.gradient(y2, inputs) / self.label_scale / self.train_stats_std

        # for penalize P
        return tf.concat([y2, dy_dx[:, 0:1], dy_dx[:, 1:2], dy_dx[:, 1:2], dy_dx[:, 2:3]], 1)



class CNN_user_supervise(tf.keras.Model):
    """ 
    similar as CNN supervise, but with the check_layer() function  
    """
    def __init__(self, input_shape, num_output, LayerName, NodesList, Activation, Padding, OutAct):
        super(CNN_user_supervise, self).__init__()

        #-------------------------------------------input layer-----------------------------------------
        Name = LayerName[0]
        Act = Activation[0]
        padding = Padding[0]
        Node = NodesList[0]
        kernel = Name.split('_')[1:]
        kernel = [int(x) for x in kernel]
        if (not (Name.lower().find('conv2d') >= 0)):
            raise ValueError('The first layer should be a conv2D layer!!!')

        self.input_layer = layers.Conv2D(Node, kernel, activation=Act, input_shape=input_shape, padding=padding)

        num_encoder = len(NodesList)

        self.encoder_layer = []
        for i0 in range(1, num_encoder):
            Name = LayerName[i0]
            Act = Activation[i0]
            padding = Padding[i0]
            Node = NodesList[i0]

            if (Name.lower().find('conv2d') >= 0):
                kernel = Name.split('_')[1:]
                kernel = [int(x) for x in kernel]
                self.encoder_layer.append(layers.Conv2D(Node, kernel, activation=Act, padding=padding))
            elif (Name.lower().find('maxpooling2d') >= 0):
                kernel = Name.split('_')[1:]
                kernel = [int(x) for x in kernel]
                self.encoder_layer.append(layers.MaxPooling2D(kernel, padding=padding))
            elif (Name.lower().find('flatten') >= 0):
                self.encoder_layer.append(layers.Flatten())
            elif (Name.lower().find('dense') >= 0):
                self.encoder_layer.append(layers.Dense(Node, activation=Act))
            else:
                raise ValueError('The layer name: ', Name, ' is not programmed!')

        self.output_layer = layers.Dense(num_output, activation=OutAct, name='output')

    def call(self, inputs, training=False):
        x = self.input_layer(inputs)
        for hl in self.encoder_layer:
            x = hl(x)
        x = self.output_layer(x)
        return x

    def check_layer(self, inputs, layer_number=100):
        count = 1

        if layer_number == 0:
            return inputs

        x = self.input_layer(inputs)
        if layer_number == count:
            return x
        count += 1

        for hl in self.encoder_layer:
            x = hl(x)
            if layer_number == count:
                return x
            count += 1

        x = self.output_layer(x)
        return x



def user_DNN_kregl1l2_gauss_grad_setup(config, train_dataset, train_labels, NodesList, Activation, train_stats):
    print('Warning Msg: user_DNN_kregl1l2_gauss_grad is hard coded with fixed label numbers = 1!')

    label_scale = float(config['TEST']['LabelScale'])
    model = user_DNN_kregl1l2_gauss_grad(config, NodesList, Activation, train_dataset, train_labels, label_scale, train_stats)
    return model


def DNN_kregl1l2_gauss(config, train_dataset, train_labels, NodesList, Activation):
    """
    Customized DNN with different regularizations
    """
    model = keras.Sequential()
    # activity_regularizer
    # bias_regularizer
    # kernel_regularizer
    kreg_l1 = 0.0
    kreg_l2 = 0.0
    gauss_noise = 0.0

    try:
        kreg_l2 = float(config['MODEL']['KRegL2'])
        # print('l2 regularize could potential cause the loss != mse')
    except:
        pass

    try:
        kreg_l1 = float(config['MODEL']['KRegL1'])
        # print('l1 regularize could potential cause the loss != mse')
    except:
        pass

    try:
        gauss_noise = float(config['MODEL']['GaussNoise'])
        # print('gauss noise could potential cause the loss != mse')
    except:
        pass

    if (kreg_l1 < 0 or kreg_l2 < 0 or gauss_noise < 0):
        raise ValueError('***ERR***: regularizer or guass noise are < 0! They should be > 0. kreg_l1 = ', kreg_l1, 'kreg_l2 = ', kreg_l2, 'gauss_noise = ', gauss_noise)

    if kreg_l2 > 0 and kreg_l1 == 0:
        model.add(
            layers.Dense(
                NodesList[0],
                activation=Activation[0],
                input_shape=[len(train_dataset.keys())],
                name='input',
                kernel_regularizer=regularizers.l2(kreg_l2),
                kernel_initializer='random_uniform'))
    elif kreg_l1 > 0 and kreg_l2 == 0:
        model.add(
            layers.Dense(
                NodesList[0],
                activation=Activation[0],
                input_shape=[len(train_dataset.keys())],
                name='input',
                kernel_regularizer=regularizers.l1(kreg_l1),
                kernel_initializer='random_uniform'))
    elif kreg_l1 > 0 and kreg_l2 > 0:
        raise ValueError('you can not use both l1 and l2 kernel regularizer: try just use l2')
    else:
        model.add(layers.Dense(NodesList[0], activation=Activation[0], input_shape=[len(train_dataset.keys())], name='input', kernel_initializer='random_uniform'))

    # first hidden layer
    if gauss_noise > 0.0:
        model.add(layers.GaussianNoise(gauss_noise))

    # remaining hidden layer
    for i0 in range(1, len(NodesList)):
        name_str = 'dense-' + str(i0)

        model.add(layers.Dense(NodesList[i0], activation=Activation[i0], name=name_str, kernel_initializer='random_uniform'))

    # output layer
    model.add(layers.Dense(len(train_labels.keys()), name='output'))

    return model


def add_input_layer(config, model, train_dataset, Name, Node, Act, padding='valid'):
    """
    Add different layers to the model
    """
    # [1000, 28, 28, 1] -> [28, 28, 1]
    # print(train_dataset)
    # print(tf.shape(train_dataset.to_numpy()))

    if (tf.__version__[0:1] == '1'):
        # input_shape = train_dataset.get_shape().as_list()[1:]
        input_shape = train_dataset.shape[1:]
    elif (tf.__version__[0:1] == '2'):
        try:
            input_shape = tf.shape(train_dataset).numpy()[1:]
        except:
            input_shape = tf.shape(train_dataset.to_numpy()).numpy()[1:]
    else:
        raise ValueError("Unknown tensorflow version: ", tf.__version__)

    # print('input_shape:', input_shape)

    if (Name.lower().find('conv2d') >= 0):
        # _3_3 -> [3,3]
        kernel = Name.split('_')[1:]
        kernel = [int(x) for x in kernel]
        strides = [1, 1]
        if len(kernel) == 4:
            strides = kernel[2:4]
        model.add(layers.Conv2D(Node, kernel[0:2], strides=strides, activation=Act, input_shape=input_shape, padding=padding, name='input'))
    elif (Name.lower().find('conv3d') >= 0):
        # _3_3 -> [3,3]
        kernel = Name.split('_')[1:]
        kernel = [int(x) for x in kernel]
        strides = [1, 1, 1]
        if len(kernel) == 6:
            strides = kernel[3:6]
        model.add(layers.Conv3D(Node, kernel[0:3], strides=strides, activation=Act, input_shape=input_shape, padding=padding, name='input'))

    elif (Name.lower().find('dense') >= 0):
        model.add(layers.Dense(Node, activation=Act, input_shape=input_shape, name='input'))
    elif (Name.lower().find('lstm') >= 0):
        # print('!!!!input_shape:', input_shape, ' should be [1, 1]!!!!!')
        model.add(layers.LSTM(Node, input_shape=(1, 1), return_sequences=True))
    elif (Name.lower().find('gru') >= 0):
        # print('!!!!input_shape:', input_shape, ' should be [1, 1]!!!!!')
        model.add(layers.GRU(Node, input_shape=(1, 1), return_sequences=True))
    else:
        raise ValueError('The first layer can only be conv2d, your input is: ', Name)


def add_one_layer(config, model, Name, Node, Act, padding='valid', tf_name=''):
    """
    Add different layers to the model
    """
    if (Act == 'None'):
        Act = None
    # print('Name:', tf_name)
    if (Name.lower().find('maxpooling2d') >= 0):
        kernel = Name.split('_')[1:]
        kernel = [int(x) for x in kernel]
        # print('kernel:', kernel, 'padding:', padding)
        model.add(layers.MaxPooling2D(kernel, padding=padding, name=tf_name))
    elif (Name.lower().find('upsampling2d') >= 0):
        kernel = Name.split('_')[1:]
        kernel = [int(x) for x in kernel]
        model.add(layers.UpSampling2D(kernel, name=tf_name))
        # print('kernel:', kernel)
    elif (Name.lower().find('conv2d') >= 0):
        kernel = Name.split('_')[1:]
        kernel = [int(x) for x in kernel]
        strides = [1, 1]
        if len(kernel) == 4:
            strides = kernel[2:4]
        model.add(layers.Conv2D(Node, kernel[0:2], strides=strides, activation=Act, padding=padding, name=tf_name))
        # print('conv2d: ', Node, kernel, strides, Act, padding, tf_name)
    elif (Name.lower().find('conv3d') >= 0):
        kernel = Name.split('_')[1:]
        kernel = [int(x) for x in kernel]
        strides = [1, 1, 1]
        if len(kernel) == 6:
            strides = kernel[3:6]
        model.add(layers.Conv3D(Node, kernel[0:3], strides=strides, activation=Act, padding=padding, name=tf_name))
        # print('kernel:', kernel)
    elif (Name.lower().find('flatten') >= 0):
        model.add(layers.Flatten(name=tf_name))
    elif (Name.lower().find('dense') >= 0):
        model.add(layers.Dense(Node, activation=Act, name=tf_name))
    elif (Name.lower().find('lstm') >= 0):
        # model.add(layers.LSTM(Node,return_sequences = True))
        model.add(layers.LSTM(Node))
    elif (Name.lower().find('gru') >= 0):
        model.add(layers.GRU(Node))
    else:
        raise ValueError('The layer name: ', Name, ' is not programmed!')


def add_output_layer(config, model, train_labels):
    """
    Add an output layer to the model
    """
    # print('****activation:---')
    Act = None
    try:
        Act = config['MODEL']['OutputLayerActivation']
        # print('****activation:---', Act)
        if (Act == ''):
            Act = None
    except:
        pass

    num_output = 1
    try:
        num_output = len(train_labels.keys())
    except:
        try:
            num_output = len(train_labels[0])
        except:
            pass
        pass
    # print('----activation:---', Act)

    output_layer = 'Dense'
    try:
        output_layer = config['MODEL']['OutputLayer']
    except:
        pass
    if (output_layer == 'Dense'):
        model.add(layers.Dense(num_output, activation=Act, name='output'))
    elif (output_layer == 'No'):
        return
    elif (output_layer == 'Conv3D'):
        """ not checked """
        # model.add(layers.Conv3D(Node, kernel[0:3], strides=strides, activation=Act, input_shape=input_shape, padding=padding,  name='input'))



def CNN_supervise(config, train_dataset, train_labels, NodesList, Activation, LayerName, Padding):
    """
    Generate a supervised CNN model based on the inputs
    """
    model = keras.Sequential()

    add_input_layer(config, model, train_dataset, LayerName[0], NodesList[0], Activation[0], Padding[0])
    for i0 in range(1, len(NodesList)):
        add_one_layer(config, model, LayerName[i0], NodesList[i0], Activation[i0], Padding[i0], tf_name=LayerName[i0] + '-' + str(i0))
    add_output_layer(config, model, train_labels)
    # print('!!!supervise!!!')

    return model


def CNN_user_supervise_setup(config, train_dataset, train_labels, NodesList, Activation, LayerName, Padding):
    """
    Generate a supervised CNN model based on the inputs
    """
    # input_shape=tf.shape(train_dataset).numpy()[1:]
    if (tf.__version__[0:1] == '1'):
        # input_shape = train_dataset.get_shape().as_list()[1:]
        input_shape = train_dataset.shape[1:]
    elif (tf.__version__[0:1] == '2'):
        try:
            input_shape = tf.shape(train_dataset).numpy()[1:]
        except:
            input_shape = tf.shape(train_dataset.to_numpy()).numpy()[1:]
    else:
        raise ValueError("Unknown tensorflow version: ", tf.__version__)

    # print('input_shape:', input_shape)

    num_output = 1
    try:
        num_output = len(train_labels.keys())
    except:
        pass

    OutAct = None
    try:
        OutAct = config['MODEL']['OutputLayerActivation']
        if (OutAct == ''):
            OutAct = None
    except:
        pass

    model = CNN_user_supervise(input_shape, num_output, LayerName, NodesList, Activation, Padding, OutAct)
    return model




def build_model(config, train_dataset, train_labels, set_non_trainable=False, train_stats=None):
    """
    Build the NN model based on the input
    """
    ModelArchitect = config['MODEL']['ModelArchitect']

    NodesList = mrnn_utility.getlist_int(config['MODEL']['NodesList'])
    Activation = mrnn_utility.getlist_str(config['MODEL']['Activation'])

    if (len(NodesList) != len(Activation)):
        raise ValueError('In the config file, number of NodesList != Activation list with NodesList = ', NodesList, ' and Activation = ', Activation)

    if (ModelArchitect.lower() == "dnn_kregl1l2_gauss".lower()):
        model = DNN_kregl1l2_gauss(config, train_dataset, train_labels, NodesList, Activation)
    elif (ModelArchitect.lower() == "user_dnn_kregl1l2_gauss_grad".lower()):
        model = user_DNN_kregl1l2_gauss_grad_setup(config, train_dataset, train_labels, NodesList, Activation, train_stats)
    elif (ModelArchitect.lower().find('cnn') >= 0):
        LayerName = mrnn_utility.getlist_str(config['MODEL']['LayerName'])
        Padding = mrnn_utility.getlist_str(config['MODEL']['Padding'])
        if (len(NodesList) != len(LayerName) or len(NodesList) != len(Padding)):
            raise ValueError('In the config file, number of NodesList != LayerName with NodesList = ', NodesList, len(NodesList), ' and LayerName = ', LayerName, len(LayerName),
                             'and Padding = ', Padding, len(Padding))
        elif (ModelArchitect.lower() == "CNN_user_supervise".lower()):
            model = CNN_user_supervise_setup(config, train_dataset, train_labels, NodesList, Activation, LayerName, Padding)
        elif (ModelArchitect.lower() == "CNN_supervise".lower()):
            model = CNN_supervise(config, train_dataset, train_labels, NodesList, Activation, LayerName, Padding)
        else:
            raise ValueError('Model architect = ', ModelArchitect, ' is chosen, but is not implemented!')
    else:
        raise ValueError('Model architect = ', ModelArchitect, ' is chosen, but is not implemented!')

    if set_non_trainable:
        model.trainable = False

    return model


====================================================================================================
mechanoChemML\workflows\mr_learning\mrnn_utility.py
====================================================================================================
import numpy as np
import datetime
import pandas as pd
import argparse, sys, os
import tensorflow as tf
from numpy import array
import matplotlib.pyplot as plt
import tensorflow as tf
import math
from configparser import ConfigParser, ExtendedInterpolation
import vtk
import glob
from natsort import natsorted, ns
import fractions



def parse_sys_args():
    """
    Command line input variables
    """

    # check: https://docs.python.org/3.6/library/argparse.html#nargs
    parser = argparse.ArgumentParser(description='Run ML study for effective properties study', prog="'" + (sys.argv[0]) + "'")
    parser.add_argument('configfile', help="configuration file for the study [*.config]")

    parser.add_argument('-v', '--version', action='version', version='%(prog)s 0.1')
    #
    parser.add_argument('-p', '--platform', choices=['cpu', 'gpu'], type=str, default='gpu', help='choose either use gpu or cpu platform (default: gpu)')

    #
    parser.add_argument('-o', '--output_dir', type=str, help='folder name to store output data')
    parser.add_argument('-r', '--restart_dir', type=str, help='folder name to store restart data')
    parser.add_argument('-t', '--tensorboard_dir', type=str, help='folder name to store tensor board data')
    parser.add_argument('-i', '--inspect', type=int, default=0, choices=[0, 1], help='pre-inspect the data (default: 0)')
    parser.add_argument('-s', '--show', type=int, default=0, choices=[0, 1], help='show the final plot (default: 0)')

    #
    parser.add_argument('-D', '--debug', type=bool, default=False, help="switch on/off the debug flag")
    parser.add_argument('-V', '--verbose', type=int, default=0, choices=[0, 1, 2, 3], help='verbose level of the code (default: 0)')
    parser.add_argument('-P', '--profile', type=bool, default=False, help='switch on/off the profiling output')
    # parser.add_argument('--integers', metavar='N', type=int, nargs='+', help='an integer for the accumulator')
    # parser.add_argument('--sum', dest='accumulate', action='store_const', const=sum, help='sum the integers (default: find the max)') # for future references
    args = parser.parse_args()

    if (not (args.verbose == 3 and args.debug)):
        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'    # suppress info output
        # tf.logging.set_verbosity(tf.logging.ERROR) # suppress deprecation warning
        #0 = all messages are logged (default behavior)
        #1 = INFO messages are not printed
        #2 = INFO and WARNING messages are not printed
        #3 = INFO, WARNING, and ERROR messages are not printed

    if (args.verbose == 3):
        print(parser.print_help())

    if (args.platform == 'cpu'):
        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

    if (args.verbose == 3):
        ml_todos()

    return args


class sys_args:
    """
    System configurations
    """

    def __init__(self):
        self.configfile = ''
        self.platform = 'gpu'
        self.inspect = 0
        self.show = 0
        self.debug = False
        self.verbose = 0


def notebook_args(args):
    """
    Additional configurations
    """

    if (not (args.verbose == 3 and args.debug)):
        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'    # suppress info output
        # tf.logging.set_verbosity(tf.logging.ERROR) # suppress deprecation warning
        #0 = all messages are logged (default behavior)
        #1 = INFO messages are not printed
        #2 = INFO and WARNING messages are not printed
        #3 = INFO, WARNING, and ERROR messages are not printed

    if (args.platform == 'cpu'):
        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

    if (args.verbose == 3):
        ml_todos()



def split_data(datax, datay, split_ratio=['0.6', '0.25', '0.15']):
    """
    Split data based on different ratios
    """
    tr_ratio = float(split_ratio[0])
    cv_ratio = float(split_ratio[1])
    tt_ratio = float(split_ratio[2])

    number_examples = datax.shape[0]
    idx = np.arange(0, number_examples)
    np.random.shuffle(idx)
    datax = [datax[i] for i in idx]    # get list of `num` random samples
    datay = [datay[i] for i in idx]    # get list of `num` random samples

    start = 0
    end_tr = int(tr_ratio * number_examples)
    end_cv = int((tr_ratio + cv_ratio) * number_examples)
    end_tt = number_examples
    tr_datax = np.array(datax[start:end_tr])
    tr_datay = np.array(datay[start:end_tr])
    cv_datax = np.array(datax[end_tr:end_cv])
    cv_datay = np.array(datay[end_tr:end_cv])
    tt_datax = np.array(datax[end_cv:end_tt])
    tt_datay = np.array(datay[end_cv:end_tt])

    return tr_datax, tr_datay, cv_datax, cv_datay, tt_datax, tt_datay


def get_package_version(tf_version):
    """ get the major and minor version of tensor flow """
    versions = tf_version.split('.')[0:2]
    versions = [int(x) for x in versions]
    # print(versions)
    return versions


def getlist_str(option, sep=',', chars=None):
    """
    Return a list from a ConfigParser option. By default, 
     split on a comma and strip whitespaces.
    """
    list0 = [(chunk.strip(chars)) for chunk in option.split(sep)]
    list0 = [x for x in list0 if x]
    return list0


def getlist_int(option, sep=',', chars=None):
    """
    Return a list from a ConfigParser option. By default, 
     split on a comma and strip whitespaces.
    """
    list0 = option.split(sep)
    list0 = [x for x in list0 if x]
    if (len(list0)) > 0:
        return [int(chunk.strip(chars)) for chunk in list0]
    else:
        return []


def getlist_float(option, sep=',', chars=None):
    """
    Return a list from a ConfigParser option. By default, 
     split on a comma and strip whitespaces.
    """
    list0 = option.split(sep)
    list0 = [x for x in list0 if x]
    if (len(list0)) > 0:
        return [float(chunk.strip(chars)) for chunk in list0]
    else:
        return []


def get_now():
    """
    Return the now string: yyyy-mm-dd-hh-mm-ss
    """
    return datetime.datetime.now().strftime("%Y%m%d-%H%M%S")


def exe_cmd(cmd):
    """ execute shell cmd """
    output_info = os.popen(cmd).read()


def get_dummy_data(num):
    """ get dummy_data for num of fields """
    one_I = [1] * (num + 1)
    data2D = [one_I, one_I]
    I = csvDf(data2D)
    return I


def csvDf(dat, **kwargs):
    """
    Generate dataframe based on csv file
    """
    data = array(dat)
    if data is None or len(data) == 0 or len(data[0]) == 0:
        return None
    else:
        return pd.DataFrame(data[1:, 1:], index=data[1:, 0], columns=data[0, 1:], **kwargs)


def read_config_file(configfile, print_keys=False):
    """ 
    read configuration file and modify the related path
    """
    config = ConfigParser(interpolation=ExtendedInterpolation())
    config.read(configfile)
    # print('... read ... configfile = ', configfile)
    # print('old root is:', type(config['TEST']['root']), [config['TEST']['root']])
    modify_root_flag = False
    if config['TEST']['root'] == '':
        config['TEST']['root'] = os.path.dirname(os.path.abspath(configfile)) + '/'
        modify_root_flag = True
    else:
        modify_root_flag = False
        # print('root is: ', config['TEST']['root'])

    # note: check if data files is given with absolute path (start with '/') or relative path, will add the root path to it.
    if (config['TEST']['DataFile'][0] != '/'):
        data_file_list = getlist_str(config['TEST']['DataFile'])
        # print('...modifying.. DataFile from: ', config['TEST']['DataFile'])
        for i0 in range(0, len(data_file_list)):
            data_file_list[i0] = os.path.dirname(os.path.abspath(configfile)) + '/' + data_file_list[i0]
        config['TEST']['DataFile'] = ', '.join(data_file_list)
        # print('...modifying.. DataFile to: ', config['TEST']['DataFile'])

    # if the following values are not given as the absolute path, then, it will be modified to the absolute value
    # In KBNN, use config['TEST']['root'] to provide the relative path.
    # in the training procedure, the following needs to be modified to store data in the scratch folder
    config['RESTART']['CheckPointDir'] = config['TEST']['root'] + '/' + config['RESTART']['CheckPointDir']
    config['OUTPUT']['TensorBoardDir'] = config['TEST']['root'] + '/' + config['OUTPUT']['TensorBoardDir']
    config['OUTPUT']['FinalModelSummary'] = config['TEST']['root'] + '/' + config['OUTPUT']['FinalModelSummary']

    # note: if the given folder name does not end with "/", the following will add "/" to it.
    if (config['RESTART']['CheckPointDir'][-1] != '/'):
        config['RESTART']['CheckPointDir'] = config['RESTART']['CheckPointDir'] + '/'
        # print(' ... add ... / to the CheckPointDir, with a new value of ', config['RESTART']['CheckPointDir'])

    cmd = 'mkdir -p ' + config['RESTART']['CheckPointDir']
    exe_cmd(cmd)
    cmd = 'mkdir -p ' + config['OUTPUT']['TensorBoardDir']
    exe_cmd(cmd)

    # if (print_keys):
        # for sec in config.items():
            # sec_name = sec[0]
            # print("--SECTION NAME--: ", sec_name)
            # for key in config[sec_name]:
                # print('         --key--: {:>25s}:'.format(key), '  ', config[sec_name][key])

    # print('new test root: ', config['TEST']['root'])
    return config


def read_one_vtk(filepath, scalar='', vector=''):
    """
    Read one VTK file
    """
    # print('read_one_vtk')

    reader = vtk.vtkStructuredGridReader()
    reader.SetFileName(filepath)
    reader.ReadAllScalarsOn()
    reader.ReadAllVectorsOn()
    reader.Update()

    griddata = reader.GetOutput()

    #TensorFlows convolutional conv2d operation expects a 4-dimensional tensor with dimensions corresponding to
    # batch, width, height and channel.
    #[batch, in_height, in_width, in_channels]

    if scalar == 'e2':
        e2 = []
        for i in range(griddata.GetPointData().GetScalars('e2').GetNumberOfTuples()):
            a = griddata.GetPointData().GetScalars('e2').GetTuple(i)[0]
            e2.append(a)

        e2 = np.array(e2)
        e2 = e2 + 0.1
        n_x = int(np.sqrt(len(e2)))
        n_y = n_x
        e2 = np.reshape(e2, (n_x, n_y, 1)) / 0.2
        e2[e2 < 0] = 0
        e2[e2 > 1] = 1
        # return e2[:n_x-1, :n_y-1]
        return e2[:n_x, :n_y]


def read_psi_me_from_mechanical_data(file_path):
    """ 
    Read psi_me from mechanical_data: for temporary label of vtk datatype

    This function should be a standalone script to prepare the label and features for vtk files. 
    """

    # delete the leading '=' for the index field
    cmd = "sed -i 's/^.*=//' " + file_path
    exe_cmd(cmd)

    selected_cols = pd.read_csv(file_path, index_col=False, skipinitialspace=True)
    # print(selected_cols)
    label = [None] * (len(selected_cols) + 1)
    print("Att: read_psi_me_from_mechanical_data: 'index' is used to index frames")
    print("ERR could occur if some of the frames have psi_me but not vtk, or have vtk but not psi_me [index out of range error]")
    for i in range(0, len(selected_cols)):
        # print(i, selected_cols['index'][i], selected_cols['Psi_me'][i])
        label[selected_cols['index'][i]] = selected_cols['Psi_me'][i]
        # label[selected_cols['index'][i]] = selected_cols['Psi_me_total'][i]
    # print (selected_cols, label)
    label[len(selected_cols)] = label[len(selected_cols) - 1]
    return label


def load_data_from_npy_for_label_shift_frame(config, dataset_frame, normalization_flag=True, verbose=0):
    """ 
    Load data from npy for label shift frame
    """
    # print('load_data_from_npy_for_label_shift_frame')
    data_file = config['KBNN']['OldShiftFeatures']
    # print("numpy_base_frame_file_name: ", data_file)
    all_data = np.load(data_file)
    # print('load saved numpy base frame for vtk folder')
    # print('load saved numpy base frame for vtk folder', tf.shape(all_data))
    # all_data = all_data.astype(np.float32)
    # print('all data after cast: ', tf.shape(all_data))
    return all_data


def load_data_from_vtk_for_label_shift_frame(config, dataset_frame, normalization_flag=True, verbose=0):
    """ 
    Load data from vtk for label shift frame
    """
    data_file = config['KBNN']['OldShiftFeatures']
    all_data = []
    load_numpy_flag = False

    numpy_base_frame_file_name = "numpy_base_frame_" + config['KBNN']['OldShiftCNNSavedBaseFrameNumpyName'] + ".vtk"
    # print("numpy_base_frame_file_name: ", numpy_base_frame_file_name)

    for file1 in glob.glob(data_file[0:data_file.rfind('/') + 1] + '*'):
        if file1.find(numpy_base_frame_file_name) >= 0:
            all_data = np.load(file1)
            load_numpy_flag = True
            # print('load saved numpy base frame for vtk folder', tf.shape(all_data))

    if (not load_numpy_flag):
        all_the_vtk_files = glob.glob(data_file)
        all_the_vtk_files = natsorted(all_the_vtk_files, alg=ns.IGNORECASE)

        frame_index = [None] * 10000000
        for file1 in all_the_vtk_files:
            # print ('working on:', file1, file1.split('/out'))
            if len(file1.split('/out')) > 1:
                # file1:  out1117.vtk
                framenumber = int(file1.split('/out')[1].split('.vtk')[0])
                frame_index[framenumber] = file1
                # print('framenumber: ', framenumber, " file1: ", file1)

        # print('dataset_frame: ', dataset_frame)
        count = len(dataset_frame)
        for i1 in dataset_frame['frame']:
            count -= 1
            file1 = frame_index[i1]
            # print('i1=: ', i1, file1, ' ', count, ' files left to process!!')
            all_data.append(read_one_vtk(file1, scalar='e2'))

        numpy_file = file1[0:file1.rfind('/')] + '/' + numpy_base_frame_file_name
        all_data = np.array(all_data)
        # print('save data to numpy_file: ', numpy_file)
        np.save(numpy_file, all_data)

    all_data = all_data.astype(np.float32)

    # all_data = tf.cast(all_data, tf.float32)
    # all_data = tf.convert_to_tensor(all_data, dtype=tf.float32)
    # print('all data after cast: ', tf.shape(all_data))
    return all_data


def load_all_data_from_vtk_database(config, normalization_flag=True, verbose=0):
    """
    Load all data from vtk database
    """
    data_file = config['TEST']['DataFile']
    # print(data_file)
    data_file_list = getlist_str(config['TEST']['DataFile'])
    # print(data_file_list)

    all_data = []
    the_label = []
    all_data_one = []
    the_label_one = []

    for data_file in data_file_list:
        load_numpy_flag = False
        # print(data_file)
        # print(data_file[0:data_file.rfind('/') + 1] + '*')
        for file1 in glob.glob(data_file[0:data_file.rfind('/') + 1] + '*'):
            if file1.find('numpy.vtk') >= 0:
                all_data.append(np.load(file1))
                load_numpy_flag = True
                # print('load saved numpy for vtk folder')
                # print('all_data', all_data)

            if file1.find('numpy_label.vtk') >= 0:
                the_label.append(np.load(file1))
                # print('load saved numpy for the label folder')

        if (not load_numpy_flag):
            # if(len(data_file_list) > 1):
            # raise ValueError ('This subroutine is not checked with multiple folders! Check Carefully! Do not mess up the labels!')
            all_the_vtk_files = glob.glob(data_file)
            # print(data_file[0:data_file.rfind('/') + 1] + 'mechanical_data.txt')
            tmp_label = read_psi_me_from_mechanical_data(data_file[0:data_file.rfind('/') + 1] + 'mechanical_data.txt')
            # print(tmp_label)

            all_the_vtk_files = natsorted(all_the_vtk_files, alg=ns.IGNORECASE)
            for file1 in all_the_vtk_files:
                framenumber = int(file1.split('/out')[1].split('.vtk')[0])
                the_label_one.append(tmp_label[framenumber])
                # print (framenumber, tmp_label[framenumber])
                all_data_one.append(read_one_vtk(file1, scalar='e2'))

            numpy_file = file1[0:file1.rfind('/')] + '/numpy.vtk'
            all_data.append(np.array(all_data_one))
            # print('save data to numpy_file: ', numpy_file, np.shape(all_data_one))
            np.save(numpy_file, all_data[-1])

            numpy_file = file1[0:file1.rfind('/')] + '/numpy_label.vtk'
            the_label.append(np.array(the_label_one))
            # print('save data to numpy_file: ', numpy_file, np.shape(the_label_one))
            np.save(numpy_file, the_label[-1])
            # print('all_data: ', np.shape(all_data), len(all_data))
            # print('the_label: ', np.shape(the_label), len(the_label))

    _all_data = all_data[0]
    # print(np.shape(_all_data))
    _the_label = the_label[0]
    for a1 in all_data[1:]:
        # print(np.shape(a1))
        _all_data = np.concatenate((_all_data, a1), axis=0)
        # print(np.shape(_all_data))
    for t1 in the_label[1:]:
        # print(np.shape(_the_label))
        # print(np.shape(t1))
        _the_label = np.concatenate((_the_label, t1), axis=0)
        # print(np.shape(_the_label))
    # print('all data : ', np.shape(_all_data))
    # print('the label: ', np.shape(_the_label))

    all_data = _all_data.astype(np.float32)
    the_label = _the_label.astype(np.float32)

    # print('all data : ', tf.shape(all_data))
    # print('the label: ', tf.shape(the_label))

    label_scale = float(config['TEST']['LabelScale'])
    the_label = the_label * label_scale

    test_derivative = []
    train_stats = []

    if (tf.__version__[0:1] == '1'):
        return all_data, the_label, test_derivative, train_stats
    elif (tf.__version__[0:1] == '2'):
        all_data = tf.convert_to_tensor(all_data, dtype=tf.float32)
        the_label = tf.convert_to_tensor(the_label, dtype=tf.float32)
        return all_data, the_label, test_derivative, train_stats


def load_all_data_from_npy_database(config, normalization_flag=True, verbose=0):
    """
    load all data from npy database
    """
    data_file = config['TEST']['DataFile']
    data_file_list = getlist_str(config['TEST']['DataFile'])

    label_scale = float(config['TEST']['LabelScale'])
    if (label_scale != 1.0):
        raise ValueError('LabelScale != 1.0 for npy database is not supported now!!!')

    all_data = None
    the_label = None
    load_numpy_flag = False
    for data_file in data_file_list:
        # print('data_file: ', data_file)
        for file1 in glob.glob(data_file):
            feature_file = file1
            label_file = feature_file.replace('features', 'labels')

            if all_data is None:
                all_data = np.load(feature_file)
            else:
                tmp_data = np.load(feature_file)
                all_data = np.concatenate((all_data, tmp_data), axis=0)

            if the_label is None:
                the_label = np.load(label_file)
            else:
                tmp_label = np.load(label_file)
                the_label = np.concatenate((the_label, tmp_label), axis=0)

            # print(' feature file: ', feature_file, ' label file: ', label_file)
        # print('all data shape: ', np.shape(all_data), ' all label shape: ', np.shape(the_label))
        # if(np.shape(all_data) != np.shape(the_label)):
        # raise ValueError('features shape does not match the label shape. Check if you really want this to happen. So far, the code is for elasticity BVP full field map!!!')

    all_data = all_data.astype(np.float32)
    the_label = the_label.astype(np.float32)

    # print('all data : ', tf.shape(all_data))
    # print('the label: ', tf.shape(the_label))

    label_scale = float(config['TEST']['LabelScale'])
    the_label = the_label * label_scale

    test_derivative = []
    train_stats = []
    # exit(0)

    if (tf.__version__[0:1] == '1'):
        return all_data, the_label, test_derivative, train_stats
    elif (tf.__version__[0:1] == '2'):
        all_data = tf.convert_to_tensor(all_data, dtype=tf.float32)
        the_label = tf.convert_to_tensor(the_label, dtype=tf.float32)
        return all_data, the_label, test_derivative, train_stats


def load_data_from_vtk_database(config, normalization_flag=True, verbose=0):
    """
    Load data from vtk database with hard coded label  mechanical_data.txt
    """
    data_file = config['TEST']['DataFile']
    # print(data_file)
    data_file_list = getlist_str(config['TEST']['DataFile'])
    # print(data_file_list)

    all_data = []
    the_label = []
    all_data_one = []
    the_label_one = []

    for data_file in data_file_list:
        load_numpy_flag = False
        print(data_file)
        print(data_file[0:data_file.rfind('/') + 1] + '*')
        for file1 in glob.glob(data_file[0:data_file.rfind('/') + 1] + '*'):
            if file1.find('numpy.vtk') >= 0:
                all_data.append(np.load(file1))
                load_numpy_flag = True
                # print('load saved numpy for vtk folder')
                # print('all_data', all_data)

            if file1.find('numpy_label.vtk') >= 0:
                the_label.append(np.load(file1))
                # print('load saved numpy for the label folder')

        if (not load_numpy_flag):
            # if(len(data_file_list) > 1):
            # raise ValueError ('This subroutine is not checked with multiple folders! Check Carefully! Do not mess up the labels!')
            all_the_vtk_files = glob.glob(data_file)
            # print(data_file[0:data_file.rfind('/') + 1] + 'mechanical_data.txt')
            tmp_label = read_psi_me_from_mechanical_data(data_file[0:data_file.rfind('/') + 1] + 'mechanical_data.txt')
            # print(tmp_label)

            all_the_vtk_files = natsorted(all_the_vtk_files, alg=ns.IGNORECASE)
            for file1 in all_the_vtk_files:
                framenumber = int(file1.split('/out')[1].split('.vtk')[0])
                the_label_one.append(tmp_label[framenumber])
                # print (framenumber, tmp_label[framenumber])
                all_data_one.append(read_one_vtk(file1, scalar='e2'))

            numpy_file = file1[0:file1.rfind('/')] + '/numpy.vtk'
            all_data.append(np.array(all_data_one))
            # print('save data to numpy_file: ', numpy_file, np.shape(all_data_one))
            np.save(numpy_file, all_data[-1])

            numpy_file = file1[0:file1.rfind('/')] + '/numpy_label.vtk'
            the_label.append(np.array(the_label_one))
            # print('save data to numpy_file: ', numpy_file, np.shape(the_label_one))
            np.save(numpy_file, the_label[-1])
            # print('all_data: ', np.shape(all_data), len(all_data))
            # print('the_label: ', np.shape(the_label), len(the_label))

    _all_data = all_data[0]
    # print(np.shape(_all_data))
    _the_label = the_label[0]
    for a1 in all_data[1:]:
        # print(np.shape(a1))
        _all_data = np.concatenate((_all_data, a1), axis=0)
        # print(np.shape(_all_data))
    for t1 in the_label[1:]:
        # print(np.shape(_the_label))
        # print(np.shape(t1))
        _the_label = np.concatenate((_the_label, t1), axis=0)
        # print(np.shape(_the_label))
    # print('all data : ', np.shape(_all_data))
    # print('the label: ', np.shape(_the_label))

    all_data = _all_data.astype(np.float32)
    the_label = _the_label.astype(np.float32)

    # print('all data : ', tf.shape(all_data))
    # print('the label: ', tf.shape(the_label))
    # exit(0)

    split_ratio = getlist_float(config['TEST']['SplitRatio'])
    if (len(split_ratio) != 3 or abs(sum(split_ratio) - 1.0) > 1.0e-5):
        raise ValueError('split ratio should be a list containing three float values with sum() == 1.0!!! Your current split_ratio = ', split_ratio, ' with sum = ',
                         sum(split_ratio))


    label_scale = float(config['TEST']['LabelScale'])
    the_label = the_label * label_scale

    train_dataset, train_labels, val_dataset, val_labels, test_dataset, test_labels = split_data(all_data, the_label, split_ratio)

    test_derivative = []
    train_stats = []

    if (tf.__version__[0:1] == '2'):
        train_dataset = tf.convert_to_tensor(train_dataset, dtype=tf.float32)
        train_labels = tf.convert_to_tensor(train_labels, dtype=tf.float32)
        test_dataset = tf.convert_to_tensor(test_dataset, dtype=tf.float32)
        test_labels = tf.convert_to_tensor(test_labels, dtype=tf.float32)
        val_dataset = tf.convert_to_tensor(val_dataset, dtype=tf.float32)
        val_labels = tf.convert_to_tensor(val_labels, dtype=tf.float32)

    ModelArchitect = config['MODEL']['ModelArchitect']
    if (ModelArchitect.lower() == "CNN_autoencoder".lower() or ModelArchitect.lower().find("_unsupervise") >= 0):
        # print('unsupervised learning, features = label')
        return train_dataset, train_dataset, val_dataset, val_dataset, test_dataset, test_dataset, test_derivative, train_stats
    else:
        return train_dataset, train_labels, val_dataset, val_labels, test_dataset, test_labels, test_derivative, train_stats


def load_all_data(config, args):    # for K-fold validation
    """
    load csv, image, url etc data to the main code
    """
    # print('load_all_data')
    verbose = args.verbose

    # load / pre-process data / split data
    if (int(config['TEST']['DataNormalization']) == 0):
        normalization_flag = False
    else:
        normalization_flag = True

    data_file = config['TEST']['DataFile']

    if (data_file.find('csv') > 0):
        dataset, labels, derivative, train_stats = load_all_data_from_csv(config, verbose=args.verbose, normalization_flag=normalization_flag)
    elif (data_file.find('.vtk') > 0):
        print("*****************WARNING**********************:")
        print("if have multiple VTK folder and it's the first time to load vtk and save numpy array. There is a")
        print("potential bug, that after the 1st vtk folder, the following numpy array file is getting bigger ")
        print("and bigger, try to fix this bug next time!!!!!")
        print("***********************************************")
        dataset, labels, derivative, train_stats = load_all_data_from_vtk_database(config, verbose=args.verbose, normalization_flag=normalization_flag)
    elif (data_file.find('.npy') > 0):
        dataset, labels, derivative, train_stats = load_all_data_from_npy_database(config, verbose=args.verbose, normalization_flag=normalization_flag)
    else:
        raise ValueError('unknown options for the DataFile:', data_file)

    # print("...done with data loading")    #,len(dataset), len(labels)) // len(tensor) is not available for tf1.13

    if (args.inspect == 1):
        print('enter pre-inspection')
        print('exit after pre-inspection')
        exit(0)

    return dataset, labels, derivative, train_stats


# the default data file is in csv format with ',' as the delimiter, and the header to describe the field info
def read_csv_fields(file_path, fields, sep=','):
    """
    Read CSV fields information
    """
    # will read the csv file and load the fields according to the new order
    list_of_csv_files = getlist_str(file_path)
    selected_cols = pd.read_csv(list_of_csv_files[0], index_col=False, sep=sep, usecols=fields, skipinitialspace=True)[fields]
    # print('read_csv_fields: ', list_of_csv_files[0], len(selected_cols))

    for f1 in list_of_csv_files[1:]:
        new_selected_cols = pd.read_csv(f1, index_col=False, sep=sep, usecols=fields, skipinitialspace=True)[fields]
        # print('read_csv_fields: ', f1, len(new_selected_cols))
        selected_cols = selected_cols.append(new_selected_cols, ignore_index=True)
    # print('total df datasize: ', len(selected_cols))

    # print (selected_cols)
    # print (type(selected_cols))
    # print (selected_cols.values)
    # print (type(selected_cols.values))
    # return selected_cols.values # return numpy types
    return selected_cols


def dataset_pop_list(data_set, pop_list):
    """
    Pop a list of index from the dataset
    """
    # print ('before pop: ', data_set.keys())
    df2 = pd.concat([data_set.pop(x) for x in pop_list], 1)
    # print ('after pop: ', data_set.keys())
    return df2


def norm(x, train_stats, DataNormOption=0):
    """
    Different data normalization scheme
    """
    if DataNormOption == 0:
        # print('...mean:', train_stats['mean'])
        # print('...std:', train_stats['std'])
        return (x - train_stats['mean']) / train_stats['std']    # ATT > float64
    elif DataNormOption == 1:
        return (x - train_stats['mean']) / train_stats['std']    # ATT > float64
    elif DataNormOption == 2:
        return (x - train_stats['mean']) / train_stats['std'] + 0.5    # ATT > float64
    elif DataNormOption == 3:
        return (x - train_stats['mean']) / train_stats['std']    # ATT > float64


def prepare_data_from_csv_file(config, normalization_flag=True, verbose=0):
    """
    load the desired fields from the csv file, not full list
    split the data based on the label fields
    split the data to three different set [train, validation, test] 
    """
    # print('prepare_data_from_csv_file')

    split_ratio = [0.6, 0.25, 0.15],
    data_file = config['TEST']['DataFile']
    # print('data_file', data_file)

    all_fields = getlist_str(config['TEST']['AllFields'])
    label_fields = getlist_str(config['TEST']['LabelFields'])
    derivative_fields = getlist_str(config['TEST']['DerivativeFields'])

    try:
        KBNN_flag = (config['KBNN']['LabelShiftingModels'] != '')
    except:
        KBNN_flag = False
        pass

    for l1 in label_fields:
        try:
            all_fields.index(l1)
        except:
            raise ValueError("label_fields = ", label_fields, " is not in all_fields = ", all_fields, " Error: all_fields should contain label_fields!!!")

    try:
        split_ratio = getlist_float(config['TEST']['SplitRatio'])
    except:
        pass

    if (verbose == 3):
        print('Data split ratio [train, validation, test] = ', split_ratio)
        print('Data file: ', config['TEST']['datafile'])

    raw_dataset = read_csv_fields(data_file, all_fields)
    dataset = raw_dataset.copy()

    #-----------------following is not a good feature or needed feature, as data normalization normally handle it well------------------------
    #####  if len(feature_shift) > 0:
    #####    print("""   You have enabled feature shift in config file. The number and sequence of shift is in the same order
    #####    of the features you specified in the label_fields. Now you are shifting: """, all_fields[0:len(feature_shift)], ' with ', feature_shift, '.')
    #####    for i0 in range(0, len(feature_shift)):
    #####      key0 = all_fields[i0]
    #####      dataset[key0] = dataset[key0] - feature_shift[i0]
    #####      # print (i0, key0, dataset.keys(), dataset[key0])

    if (KBNN_flag):
        ## index and frames are used to match the CNN training info
        # index was the first try, but it turns out that we should use the base vtu file to predict the base free energy function.
        try:
            raw_dataset_index = read_csv_fields(data_file, ['index'])
            dataset_index = raw_dataset_index.copy()
        except:
            print("***ERR** in loading the index data. Will be neglected!!!")
            dataset_index = None
            pass

        # frame is the final choice. Rerun the collect data script to get new dataset if needed.
        try:
            raw_dataset_frame = read_csv_fields(data_file, ['frame'])
            dataset_frame = raw_dataset_frame.copy()
        except:
            print("***ERR** in loading the frame data. Will be neglected!!!")
            dataset_frame = None
            pass
    #-------------NN label shift------------------
        import mechanoChemML.workflows.mr_learning.mrnn_models as mrnn_models
        mrnn_models.shift_labels(config, dataset, dataset_index, dataset_frame, data_file)
    #------------------following is a little bit non-modulated, easy for bugs ---------------------------

# check data
    if (len(split_ratio) != 3 or abs(sum(split_ratio) - 1.0) > 1.0e-5):
        raise ValueError('split ratio should be a list containing three float values with sum() == 1.0!!! Your current split_ratio = ', split_ratio, ' with sum = ',
                         sum(split_ratio))

    # split data for LSTM and GRU without randomly shuffle
    ModelArchitect = config['MODEL']['ModelArchitect']
    if ModelArchitect.lower().find('lstm') >= 0 or ModelArchitect.lower().find('gru') >= 0:
        print('dataset for LSTM or GRU')

        feature_index = list(range(0, len(dataset) - 1))
        label_index = list(range(1, len(dataset)))

        train_num = int(len(feature_index) * split_ratio[0])
        val_num = int(len(feature_index) * split_ratio[1])
        test_num = len(feature_index) - train_num - val_num

        dataset_stats = dataset.describe()
        dataset_stats = dataset_stats.transpose()
        dataset = (dataset_stats['max'] - dataset) / (dataset_stats['max'] - dataset_stats['min'])

        features = dataset.to_numpy()
        labels = np.squeeze(dataset.to_numpy())

        # print(tf.shape(features))
        features = np.expand_dims(features, axis=-1)
        print(np.shape(features), np.shape(labels), type(features))

        train_dataset = features[feature_index[0:train_num]]
        train_labels = labels[label_index[0:train_num]]
        # print(tf.shape(train_dataset), tf.shape(train_labels))

        val_dataset = features[feature_index[train_num:val_num + train_num]]
        val_labels = labels[label_index[train_num:val_num + train_num]]

        test_dataset = features[feature_index[train_num + val_num:val_num + train_num + test_num]]
        test_labels = labels[label_index[train_num + val_num:val_num + train_num + test_num]]

        train_stats = dataset_stats
        test_derivative = []

        # print(train_dataset, train_labels)
        # print(val_dataset, val_labels)
        # print(test_dataset, test_labels)
        # exit(0)
        return train_dataset, train_labels, val_dataset, val_labels, test_dataset, test_labels, test_derivative, train_stats

    # print('len of total dataset: ', len(dataset))

    # split data
    train_dataset = dataset.sample(frac=split_ratio[0], random_state=0)
    tmp_dataset = dataset.drop(train_dataset.index)
    # print('len of each dataset (train, tmp: ', len(train_dataset), len(tmp_dataset), 'split_ratio: ', split_ratio)

    val_dataset = tmp_dataset.sample(frac=(split_ratio[1] / (split_ratio[1] + split_ratio[2])), random_state=0)
    test_dataset = tmp_dataset.drop(val_dataset.index)
    # print('len of each dataset (train, val, test): ', len(train_dataset), len(val_dataset), len(test_dataset))

    batch_size = int(config['MODEL']['BatchSize'])
    if (batch_size > len(train_dataset) or (batch_size > len(val_dataset) and len(val_dataset) != 0) or batch_size > len(test_dataset)):
        raise ValueError('batch_size is larger than one of your data set, reduce it!', 'batch_size:', batch_size, 'train, validation, test size:', len(train_dataset),
                         len(val_dataset), len(test_dataset), 'Please choose a common factor for your training data!')

    drop_data_flag = 0
    try:
        drop_data_flag = int(config['TEST']['DropData'])
    except:
        pass

    # print('data_set info:', len(train_dataset), len(val_dataset), len(test_dataset), 'default batch size:', batch_size)
    if (drop_data_flag):
        train_data_to_drop = len(train_dataset) % batch_size
        val_data_to_drop = len(val_dataset) % batch_size
        test_data_to_drop = len(test_dataset) % batch_size
        # print('to_drop:', train_data_to_drop, val_data_to_drop, test_data_to_drop)
        if (train_data_to_drop == 0 and val_data_to_drop == 0 and test_data_to_drop == 0):
            print('the pre-set batch-size is good!')
        else:
            tmp_batch_size = fractions.gcd(fractions.gcd(len(train_dataset), len(val_dataset)), len(test_dataset))

            if (tmp_batch_size >= 32 and tmp_batch_size <= 1024):
                print('use updated batch_size: ', tmp_batch_size)
                config['MODEL']['BatchSize'] = str(tmp_batch_size)
                train_data_to_drop = 0
                val_data_to_drop = 0
                test_data_to_drop = 0
            else:
                print('Please re-split data as good as possible! use default batch_size:', batch_size)
                train_data_to_drop = len(train_dataset) % batch_size
                val_data_to_drop = len(val_dataset) % batch_size
                test_data_to_drop = len(test_dataset) % batch_size
    else:
        print("drop_data_flag is False, no data drop is allowed even the size of data is not a multiple of batch size.")

    test_derivative = []
    # print('---derivative fields: ', derivative_fields)
    if (len(derivative_fields) > 0):
        raw_dataset_derivative = read_csv_fields(data_file, derivative_fields)
        dataset_derivative = raw_dataset_derivative.copy()
        dataset_derivative = dataset_derivative.drop(train_dataset.index)
        test_derivative = dataset_derivative.drop(val_dataset.index)
        # print ('check test_derivative before:', test_derivative)
        if (drop_data_flag):
            if test_data_to_drop > 0:
                test_derivative = test_derivative.drop(test_derivative.index[-test_data_to_drop:])
        # print ('check test_derivative after:', test_derivative)
        test_derivative = test_derivative.to_numpy()

        # print ('----- test---   :', test_dataset)
    if (drop_data_flag):
        if train_data_to_drop > 0:
            train_dataset = train_dataset.drop(train_dataset.index[-train_data_to_drop:])
        if val_data_to_drop > 0:
            val_dataset = val_dataset.drop(val_dataset.index[-val_data_to_drop:])
        if test_data_to_drop > 0:
            test_dataset = test_dataset.drop(test_dataset.index[-test_data_to_drop:])

    # # print(len(train_dataset))
    # # print(train_data_to_drop, val_data_to_drop, test_data_to_drop)
    # # print('tmp_batch_size:', tmp_batch_size)
    # print(len(train_dataset), len(val_dataset), len(test_dataset))
    # exit(0)

    # get mean, std, etc
    train_stats = train_dataset.describe()
    dataset_pop_list(train_stats, label_fields)
    train_stats = train_stats.transpose()

    DataNormOption = 0
    try:
        DataNormOption = int(config['TEST']['DataNormOption'])
    except:
        pass

    if DataNormOption == 0:
        """ """
        # do nothing
        # print("---norm---: use 'mean' and 'std ' do the normalization (-1.7, 1.7)")
    elif DataNormOption == 1:
        # print("---norm---: use 0.5*(min+max) and 'max-min' do the normalization (-0.5, 0.5)")
        train_stats['mean'] = 0.5 * (train_stats['min'] + train_stats['max'])
        train_stats['std'] = (train_stats['max'] - train_stats['min'])
    elif DataNormOption == 2:
        # print("---norm---: use 0.5*(min+max) and 'max-min' do the normalization (0, 1)")
        train_stats['mean'] = 0.5 * (train_stats['min'] + train_stats['max'])
        train_stats['std'] = (train_stats['max'] - train_stats['min'])
    elif DataNormOption == 3:
        # print("---norm---: use 0.5*(min+max) and 'max-min' do the normalization (-1, 1)")
        train_stats['mean'] = 0.5 * (train_stats['min'] + train_stats['max'])
        train_stats['std'] = 0.5 * (train_stats['max'] - train_stats['min'])

    # new_std = 0.5 * ( train_stats['std']['F12'] + train_stats['std']['F21'] )
    # train_stats['std']['F12'] = new_std
    # train_stats['std']['F21'] = new_std
    # print('std:', train_stats['std'])
    # print('mean:', train_stats['mean'])

    if (KBNN_flag):
        # print('replace old mean and old std')
        old_features = getlist_str(config['KBNN']['OldEmbedFeatures'])
        if len(old_features) > 0:
            old_mean = getlist_float(config['KBNN']['OldEmbedMean'])
            old_std = getlist_float(config['KBNN']['OldEmbedStd'])
            for i0 in range(0, len(old_features)):
                key0 = old_features[i0]
                if any(key0 in s for s in all_fields):
                    # print('update std, mean of key0=', key0)
                    train_stats['std'][key0] = old_std[i0]
                    train_stats['mean'][key0] = old_mean[i0]
            # print('(after)std:', train_stats['std'])
            # print('(after)mean:', train_stats['mean'])
    # exit(0)

    label_scale = float(config['TEST']['LabelScale'])
    label_shift = float(config['TEST']['LabelShift'])
    # print('Label shift: ', label_shift)

    # get labels
    train_labels = dataset_pop_list(train_dataset, label_fields)
    # print('out_side ', train_dataset.keys(), type(train_dataset))
    val_labels = dataset_pop_list(val_dataset, label_fields)
    test_labels = dataset_pop_list(test_dataset, label_fields)

    train_labels = (train_labels - label_shift) * label_scale
    val_labels = (val_labels - label_shift) * label_scale
    test_labels = (test_labels - label_shift) * label_scale

    # print ('train_labels:', train_labels)

    if (verbose == 3):
        print("Train_dataset tail(5): ")
        print(train_dataset.tail(5))
        print("Train_labels tail(5): ")
        print(train_labels.tail(5))

    # print('  std of train_stats: ', train_stats['std'])
    if (len(test_derivative) > 0):
        test_derivative = test_derivative * label_scale

    normed_test_derivative = []
    if (len(test_derivative) > 0 and normalization_flag):
        std = train_stats['std'].to_numpy()    # pay attention to the types.
        # print('test_derivative * label_scale: ', test_derivative)
        normed_test_derivative = test_derivative * std[0:len(test_derivative[0])]
        if (len(std) > len(test_derivative[0])):
            print("!!!Warning: features number in std is larger than the test derivative field. The first several features std is used to scale test_derivative!!!")
        # print('std:', std, ' label scale: ', label_scale)
        # print('test_derivative: ', test_derivative)
        # print('normed_test_derivative: ', normed_test_derivative)

    # print('train_stats: ', train_stats)
    # print('train_labels: ', train_labels)

    if (normalization_flag):
        # normalize data based on train_means

        # print('dataset_old before', train_dataset)
        normed_train_data = norm(train_dataset, train_stats, DataNormOption)
        # print('dataset_old after', normed_train_data)
        # exit(0)
        normed_val_data = norm(val_dataset, train_stats, DataNormOption)
        normed_test_data = norm(test_dataset, train_stats, DataNormOption)

        # print('---aaa--- ', train_dataset.keys(), type(train_dataset))
        return normed_train_data, train_labels, normed_val_data, val_labels, normed_test_data, test_labels, normed_test_derivative, train_stats
    else:
        return train_dataset, train_labels, val_dataset, val_labels, test_dataset, test_labels, test_derivative, train_stats


def load_all_data_from_csv(config, normalization_flag=True, verbose=0):
    """
    Load all the data from a csv file
    """
    data_file = config['TEST']['DataFile']

    all_fields = getlist_str(config['TEST']['AllFields'])
    label_fields = getlist_str(config['TEST']['LabelFields'])
    derivative_fields = getlist_str(config['TEST']['DerivativeFields'])

    try:
        KBNN_flag = (config['KBNN']['LabelShiftingModels'] != '')
    except:
        KBNN_flag = False
        pass
    # print(data_file)
    # if (KBNN_flag): # is enabled
    # raise ValueError("KBNN is not enabled for K-fold validation")

    # delete this from future version
    # feature_shift = getlist_float(config['TEST']['FeatureShift'])

    for l1 in label_fields:
        try:
            all_fields.index(l1)
        except:
            raise ValueError("label_fields = ", label_fields, " is not in all_fields = ", all_fields, " Error: all_fields should contain label_fields!!!")

    raw_dataset = read_csv_fields(data_file, all_fields)
    dataset = raw_dataset.copy()

    if (KBNN_flag):
        ## index and frames are used to match the CNN training info
        # index was the first try, but it turns out that we should use the base vtu file to predict the base free energy function.
        try:
            raw_dataset_index = read_csv_fields(data_file, ['index'])
            dataset_index = raw_dataset_index.copy()
        except:
            print("***ERR** in loading the index data. Will be neglected!!!")
            dataset_index = None
            pass

        # # frame is the final choice. Rerun the collect data script to get new dataset if needed.
        try:
            raw_dataset_frame = read_csv_fields(data_file, ['frame'])
            dataset_frame = raw_dataset_frame.copy()
        except:
            print("***ERR** in loading the frame data. Will be neglected!!!")
            dataset_frame = None
            pass
    #-------------NN label shift------------------
        import mechanoChemML.workflows.mr_learning.mrnn_models as mrnn_models
        mrnn_models.shift_labels(config, dataset, dataset_index, dataset_frame, data_file)

    # print('---derivative fields: ', derivative_fields)

    test_derivative = []
    if (len(derivative_fields) > 0):
        raw_dataset_derivative = read_csv_fields(data_file, derivative_fields)
        dataset_derivative = raw_dataset_derivative.copy()
        test_derivative = dataset_derivative.to_numpy()

    # get mean, std, etc
    train_stats = dataset.describe()
    dataset_pop_list(train_stats, label_fields)
    train_stats = train_stats.transpose()

    DataNormOption = 0
    try:
        DataNormOption = int(config['TEST']['DataNormOption'])
    except:
        pass

    if DataNormOption == 0:
        """ """
        # do nothing
        # print("---norm---: use 'mean' and 'std ' do the normalization (-1.7, 1.7)")
    elif DataNormOption == 1:
        # print("---norm---: use 0.5*(min+max) and 'max-min' do the normalization (-0.5, 0.5)")
        train_stats['mean'] = 0.5 * (train_stats['min'] + train_stats['max'])
        train_stats['std'] = (train_stats['max'] - train_stats['min'])
    elif DataNormOption == 2:
        # print("---norm---: use 0.5*(min+max) and 'max-min' do the normalization (0, 1)")
        train_stats['mean'] = 0.5 * (train_stats['min'] + train_stats['max'])
        train_stats['std'] = (train_stats['max'] - train_stats['min'])
    elif DataNormOption == 3:
        # print("---norm---: use 0.5*(min+max) and 'max-min' do the normalization (-1, 1)")
        train_stats['mean'] = 0.5 * (train_stats['min'] + train_stats['max'])
        train_stats['std'] = 0.5 * (train_stats['max'] - train_stats['min'])

    # print('std:', train_stats['std'])
    # print('mean:', train_stats['mean'])

    if (KBNN_flag):
        # print('replace old mean and old std')
        old_features = getlist_str(config['KBNN']['OldEmbedFeatures'])
        if len(old_features) > 0:
            old_mean = getlist_float(config['KBNN']['OldEmbedMean'])
            old_std = getlist_float(config['KBNN']['OldEmbedStd'])
            for i0 in range(0, len(old_features)):
                key0 = old_features[i0]
                if any(key0 in s for s in all_fields):
                    # print('update std, mean of key0=', key0)
                    train_stats['std'][key0] = old_std[i0]
                    train_stats['mean'][key0] = old_mean[i0]
            # print('(after)std:', train_stats['std'])
            # print('(after)mean:', train_stats['mean'])

    label_scale = float(config['TEST']['LabelScale'])
    label_shift = float(config['TEST']['LabelShift'])
    # print('Label shift: ', label_shift)

    # get labels
    labels = dataset_pop_list(dataset, label_fields)
    labels = (labels - label_shift) * label_scale

    if (verbose == 3):
        print("Train_dataset tail(5): ")
        print(train_dataset.tail(5))
        print("Train_labels tail(5): ")
        print(train_labels.tail(5))

    # print('  std of train_stats: ', train_stats['std'])
    if (len(test_derivative) > 0):
        test_derivative = test_derivative * label_scale

    normed_test_derivative = []
    if (len(test_derivative) > 0 and normalization_flag):
        std = train_stats['std'].to_numpy()    # pay attention to the types.
        # print('test_derivative * label_scale: ', test_derivative)
        normed_test_derivative = test_derivative * std[0:len(test_derivative[0])]
        if (len(std) > len(test_derivative[0])):
            print("!!!Warning: features number in std is larger than the test derivative field. The first several features std is used to scale test_derivative!!!")
        # print('std:', std, ' label scale: ', label_scale)

    if (normalization_flag):
        normed_dataset = norm(dataset, train_stats, DataNormOption)
        normed_derivative = normed_test_derivative
        return normed_dataset, labels, normed_derivative, train_stats
    else:
        derivative = test_derivative
        return dataset, labels, derivative, train_stats


def inspect_cnn_features(model, config, test_dataset, savefig=False):
    """
    Output intermediate CNN results after each layer
    """
    num_images = int(config['OUTPUT']['NumImages'])
    inspect_layers = getlist_int(config['OUTPUT']['InspectLayers'])

    total_images = 0
    for l0 in inspect_layers:
        out1 = model.check_layer(test_dataset[0:1], l0)
        total_images += tf.shape(out1[0]).numpy()[2]
        # print('total_images:', total_images)

    if (int(np.sqrt(total_images)) * int(np.sqrt(total_images)) >= total_images):
        num_col = int(np.sqrt(total_images))
    else:
        num_col = int(np.sqrt(total_images)) + 1
    num_row = num_col

    for i0 in range(0, num_images):
        plt.figure()
        count = 0
        for l0 in inspect_layers:
            out1 = model.check_layer(test_dataset[i0:i0 + 1], l0)
            img0 = out1[0]
            shape0 = tf.shape(img0).numpy()
            for i in range(1, shape0[2] + 1):    # 2nd index is the feature numbers
                count += 1
                ax = plt.subplot(num_col, num_row, count)
                plt.imshow(out1[0, :, :, i - 1])    # tensor
                plt.gray()
                ax.get_xaxis().set_visible(False)
                ax.get_yaxis().set_visible(False)
        if savefig:
            plt.savefig(str(i0) + '.pdf', bbox_inches='tight', format='pdf')
        plt.show()



def generate_dummy_dataset(old_config):
    """ 
    based on the label list, generate dummy dataset 
    """
    all_fields = getlist_str(old_config['TEST']['AllFields'])
    label_fields = getlist_str(old_config['TEST']['LabelFields'])
    train_dataset = get_dummy_data(len(all_fields) - len(label_fields))
    train_label = get_dummy_data(len(label_fields))
    return train_dataset, train_label

def special_input_case(inputs, input_case=''):
    """ deal with special case for old inputs:
      for example, another DNN is for the frame 800, which has input only F11, F12, F21, F22, 
      whereas, the current model might have additional microstructure features, thus, the input
      needs to be sliced to fit for the old DNN. 
    """
    if input_case != '':

        input_ind = getlist_int(input_case)
        if (len(input_ind) == 1):
            s_ind = 0
            e_ind = input_ind[0]
        elif (len(input_ind) == 2):
            s_ind = input_ind[0]
            e_ind = input_ind[1]
        else:
            s_ind = 0
            e_ind = -1
        return tf.slice(inputs, [0, s_ind], [-1, e_ind])
    else:
        return inputs



====================================================================================================
mechanoChemML\workflows\pde_solver\__init__.py
====================================================================================================


====================================================================================================
mechanoChemML\workflows\pde_solver\geometry.py
====================================================================================================
import os
import pip
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np
import sys
import math, random
import glob
from natsort import natsorted, ns

def force_install(package, versions=None):

    """install one package with versions """

    if versions is not None:
        pip.main(['install', package+'=='+versions])
    else:
        pip.main(['install', package])

def check_and_install():

    try:
        __import__('skgeom')
    except ImportError:
        print('...install... scikit-geometry')
        cmd = 'conda install -c conda-forge scikit-geometry'
        os.system(cmd)

check_and_install()

import skgeom as sg
from skgeom.draw import draw

"""
Generate random polygons with random BCs at random boundary locations for diffusion problem
"""

global TOTAL_COUNT
TOTAL_COUNT = 1

# https://stackoverflow.com/questions/8997099/algorithm-to-generate-random-2d-polygon
def generatePolygon( ctrX, ctrY, aveRadius, irregularity, spikeyness, numVerts ) :
    '''Start with the centre of the polygon at ctrX, ctrY, 
    then creates the polygon by sampling points on a circle around the centre. 
    Randon noise is added by varying the angular spacing between sequential points,
    and by varying the radial distance of each point from the centre.

    Params:
    ctrX, ctrY - coordinates of the "centre" of the polygon
    aveRadius - in px, the average radius of this polygon, this roughly controls how large the polygon is, really only useful for order of magnitude.
    irregularity - [0,1] indicating how much variance there is in the angular spacing of vertices. [0,1] will map to [0, 2pi/numberOfVerts]
    spikeyness - [0,1] indicating how much variance there is in each vertex from the circle of radius aveRadius. [0,1] will map to [0, aveRadius]
    numVerts - self-explanatory

    Returns a list of vertices, in CCW order.
    '''

    irregularity = clip( irregularity, 0,1 ) * 2.0*math.pi / numVerts
    spikeyness = clip( spikeyness, 0,1 ) * aveRadius

    # generate n angle steps
    angleSteps = []
    lower = (2.*math.pi / numVerts) - irregularity
    upper = (2.*math.pi / numVerts) + irregularity
    sum = 0
    for i in range(numVerts) :
        tmp = random.uniform(lower, upper)
        angleSteps.append( tmp )
        sum = sum + tmp

    # normalize the steps so that point 0 and point n+1 are the same
    k = sum / (2.*math.pi)
    for i in range(numVerts) :
        angleSteps[i] = angleSteps[i] / k

    # now generate the points
    points = []
    angle = random.uniform(0, 2*math.pi)
    for i in range(numVerts) :
        r_i = clip( random.gauss(aveRadius, spikeyness), 0, 2*aveRadius )
        x = ctrX + r_i*math.cos(angle)
        y = ctrY + r_i*math.sin(angle)
        points.append( (int(x),int(y)) )

        angle = angle + angleSteps[i]

    return points

def clip(x, min, max) :
     if( min > max ) :  return x    
     elif( x < min ) :  return min
     elif( x > max ) :  return max
     else :             return x

def create_polygon(points, pixel, shape_id=0, use_convex_hull=True):

    sgPoints = []
    for p0 in points:
        sgPoints.append(sg.Point2(p0[0], p0[1]))

    if use_convex_hull :
        # the following make sure the inside has +1, and outside has -1
        # but not work for L-shape, I guess.
        chull_points = sg.convex_hull.graham_andrew(sgPoints)
        poly = sg.Polygon(chull_points)
    else:
        # for L-shape, please define points counter-clock-wise 
        print("Please define the points counter-clock-wise to make sure inner has flag of +1 and outer has -1")
        poly = sg.Polygon(sgPoints)
        draw(sgPoints)

    return poly


def generate_poly_points(num_points, pixel, use_new_random_polygon=False):
    """
    generate points for polygons
    """
    a = sg.random_polygon(num_points * 5, shape='circle', size=pixel/2)
    a2 = sg.simplify(a, 0.2, "ratio", preserve_topology=False)
    points = []
    for v0 in list(a2.vertices):
        x0 = int(float(v0.x())+pixel/2)
        x0 = max(0,x0)
        x0 = min(pixel-1, x0)
        y0 = int(float(v0.y())+pixel/2)
        y0 = max(0,y0)
        y0 = min(pixel-1, y0)
        p0 = [x0, y0]
        points.append(p0)

    if use_new_random_polygon:
        while True:
            verts = generatePolygon( ctrX=pixel/2, ctrY=pixel/2, aveRadius=pixel*0.45, irregularity=0.2, spikeyness=0.2, numVerts=5 )
            verts = [[int(x[0]), int(x[1])] for x in verts]
            list_of_verts = []
            for x in verts:
                list_of_verts.append(x[0])
                list_of_verts.append(x[1])
            # print(max(list_of_verts))
            if max(list_of_verts) < pixel and min(list_of_verts) >= 0:
                break
        return verts
    else:
        return points



def distance(p1,  p2,  p0):
    """
    line defined by p1 and p2
    compute distance from p0 to the line
    """

    d_list = []
    _p1=np.array([float(p1.x()),float(p1.y())])
    _p2=np.array([float(p2.x()),float(p2.y())])
    _p0=np.array([float(p0.x()),float(p0.y())])
    d_center = np.cross(_p2 - _p1, _p0 - _p1)/np.linalg.norm( _p2 - _p1 )
    _d12 = np.linalg.norm( _p2 - _p1 )
    _d10 = np.linalg.norm( _p1 - _p0 )
    _d20 = np.linalg.norm( _p2 - _p0 )

    d_list_max = -1.0
    d_list_min = 1.0
    if abs(d_center) < 0.5*np.sqrt(2):
        _p0=np.array([float(p0.x())-0.5,float(p0.y())-0.5])
        d_1 = np.cross(_p2 - _p1, _p0 - _p1)/np.linalg.norm( _p2 - _p1 )
        _p0=np.array([float(p0.x())+0.5,float(p0.y())-0.5])
        d_2 = np.cross(_p2 - _p1, _p0 - _p1)/np.linalg.norm( _p2 - _p1 )
        _p0=np.array([float(p0.x())+0.5,float(p0.y())+0.5])
        d_3 = np.cross(_p2 - _p1, _p0 - _p1)/np.linalg.norm( _p2 - _p1 )
        _p0=np.array([float(p0.x())-0.5,float(p0.y())+0.5])
        d_4 = np.cross(_p2 - _p1, _p0 - _p1)/np.linalg.norm( _p2 - _p1 )
        d_list = [d_1, d_2, d_3, d_4]
        d_list_max = max(d_list)
        d_list_min = min(d_list)

    # avoid potentially on the extension of the line
    if (_d20 <= _d12 and _d10 <= _d12): 
        if d_list_max < 0: # four nodes are on one side of the line
            return 999
        elif d_list_min > 0: # four nodes are on one side of the line
            return 999
        else :
            return d_center
    else :
        return 999


def fit_polynomial_form(coor, y, coor_at, order=2):
    # https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html
    z = np.polyfit(coor, y, order)
    y_pred = 0.0
    for i in range(0, order+1):
        y_pred += np.power(coor_at, i) * z[order - i]
    return y_pred


def plot_lines(pixel, color='b', linewidth=0.5):
    for x in range(0, pixel):
        plt.axhline(x+0.5, color=color, linestyle='-', linewidth=linewidth)
        plt.axvline(x+0.5, color=color, linestyle='-', linewidth=linewidth)


def add_2edge_bc(bc_definition, count, total_edges):
    for i in range(0, total_edges):
        for j in range(i+2, i+2+total_edges-3):
            # print(i, j%total_edges)
            bc_definition[count] = {i:'c', j%total_edges:'h'} 
            count += 1
            bc_definition[count] = {i:'c', j%total_edges:'c'} 
            count += 1
    return count, bc_definition

# def add_3edge_bc(bc_definition, count, total_edges):
    # for i in range(0, total_edges):
        # for j in range(i+2, i+2+total_edges-5):
            # for k in range(j+2, j+2+total_edges-5):
                # # not loop back, for total_edges = 7+
                # if (k+1)%total_edges != i:
                    # # not loop back, for total_edges = 8+
                    # if i != k%total_edges:
                        # # print(i, j%total_edges, k%total_edges)
                        # bc_definition[count] = {i:'c', j%total_edges:'c', k%total_edges:'c'} 
                        # count += 1
                        # bc_definition[count] = {i:'c', j%total_edges:'c', k%total_edges:'h'} 
                        # count += 1
                        # bc_definition[count] = {i:'c', j%total_edges:'h', k%total_edges:'h'} 
                        # count += 1
                        # bc_definition[count] = {i:'c', j%total_edges:'h', k%total_edges:'c'} 
                        # count += 1
    # return count, bc_definition


def get_bc_definition(total_edges):
    bc_definition = {}
    if total_edges < 6:
        bc_definition = {}
        count = 0
        count, bc_definition = add_2edge_bc(bc_definition, count, total_edges)
    elif total_edges >= 6:
        bc_definition = {}
        count = 0
        count, bc_definition = add_2edge_bc(bc_definition, count, total_edges)
        # too much combinations
        # count, bc_definition = add_3edge_bc(bc_definition, count, total_edges)
        # print(bc_definition)
        # exit(0)
    else:
        print("total_edges = ", total_edges, " is not implemented for get_bc_definition()")
        exit(0)
    print("total_bcs: ", count)

    return bc_definition

def generate_bc_values(bc_size, bc_num, bc_range=[0.5, 1.0], bc_order_type="constant"):
    order = 0
    decimals = 2
    if bc_order_type == "constant":
        order = 0
    elif bc_order_type == "linear":
        order = 1
    elif bc_order_type == "quadratic":
        order = 2
    elif bc_order_type == "sinusoidal":
        order = 1
    else :
        print("bc_order_type = ", bc_order_type, " is not implemented in generate_bc_values()")
        exit(0)

    # np.random.seed(0) # cannot use fixed seed, as we loop over different edges, it could easily mess up things with the same value
    values =  np.random.uniform(bc_range[0], bc_range[1], bc_num * bc_size * (order+1) )
    values =  np.round(values, decimals)
    values = values.reshape(bc_num, bc_size, (order+1))
    list_of_bc_values = values
    return list_of_bc_values

def assign_region_mask (img, poly, pixel):
    for i0 in range(0, pixel):
        for j0 in range(0, pixel):
            d_list = []
            for e0 in poly.edges:
                d = distance(e0.point(0), e0.point(1), sg.Point2(i0, j0))
                d_list.append(d)

            # on one of the edges
            if abs(min(d_list)) < 0.5*np.sqrt(2):
                img[j0, i0, :] = 999
            # not on the edges
            else:
                if poly.oriented_side(sg.Point2(i0, j0)) == sg.Sign.NEGATIVE:
                    img[j0, i0, :] = -1.0
                else:
                    img[j0, i0, :] = 1.0
    # assign value to the inner and outer region of the domain
    img_c = img[:, :, 0:1]
    img_h = img[:, :, 1:3]
    img_c = np.where(img_c != 1, img_c, -2) 
    img_h = np.where(img_h != 1, img_h, 0) 
    img = np.concatenate((img_c, img_h), axis=2)
    return img


def compute_bc_value(one_bc_value, bc_order_type, e0, i0, j0, bc_type):
    normalization_factor = 2.0
    if abs(float(e0.direction().dx())) >= abs(float(e0.direction().dy())):
        is_x_dependent = True
    else:
        is_x_dependent = False

    edge_vector = np.array([float(e0.direction().dx()), float(e0.direction().dy())])
    edge_length = np.linalg.norm(edge_vector)

    point_to_p0_vector = np.array([i0-float(e0.point(0).x()), j0-float(e0.point(0).y())])
    projected_vector = np.dot(point_to_p0_vector, edge_vector)/np.dot(edge_vector, edge_vector) * edge_vector
    point_to_p0_distance = np.linalg.norm(projected_vector)

    norm = np.array([float(e0.direction().dy()), float(e0.direction().dx())])
    norm = norm/np.linalg.norm(norm)
    norm = abs(norm)

    if bc_order_type == "constant":
        new_bc_value = one_bc_value[0]
    elif bc_order_type == "linear":
        new_bc_value = one_bc_value[0] + (one_bc_value[1]-one_bc_value[0]) * point_to_p0_distance / edge_length
    elif bc_order_type == "quadratic":
        # min, next, max
        one_bc_value.sort() 
        y = np.array(one_bc_value)
        coor = np.array([0, edge_length, 0.5*edge_length])
        new_bc_value = fit_polynomial_form(coor, y, point_to_p0_distance, order=2)

    elif bc_order_type == "sinusoidal":
        # https://en.wikipedia.org/wiki/Sine_wave
        # y = A sin(w*t + phi), phi=0, A= max-min, w=2*pi, t = (coor_at - coor_1) / (coor_2 - coor_1)
        # min, max
        one_bc_value.sort() 
        t = point_to_p0_distance / edge_length
        new_bc_value = one_bc_value[0] + (one_bc_value[1] - one_bc_value[0]) * np.sin(2*np.pi*t)
    else :
        # we can certainly add higher order terms
        print("bc_order_type = ", bc_order_type, " is not implemented in compute_bc_value()")
        exit(0)

    if bc_type == 'c':
        return new_bc_value
    elif bc_type == 'h':
        unscaled_bc_value = new_bc_value * normalization_factor - 0.5 * normalization_factor 
        unscaled_bc_value_split = unscaled_bc_value * norm
        bc_value_split = unscaled_bc_value_split / normalization_factor + 0.5
        bc_x = bc_value_split[0]
        bc_y = bc_value_split[1]
        return [bc_x, bc_y]


def apply_bcs_one_edge(img, list_of_edges, pixel, e0, _val, one_bc_value, bc_order_type):
    """
    apply bcs on one edge
    """
    if _val == 'c':
        channel = 0
    elif _val == 'h':
        channel = 1
    else:
        print("bc val type = ", _val, ". Not sure what it is! In apply_bcs_one_edge()")
        exit(0)

    for i0 in range(0, pixel):
        for j0 in range(0, pixel):
            # only distance of pixels on the boundary will be calculated to save computational time
            if img[j0,i0,channel] == 999:
                d = distance(e0.point(0), e0.point(1), sg.Point2(i0, j0))
                if abs(d) < 0.5 * np.sqrt(2):
                # if d < 0.5 and d >= 0.0: # inside the domain
                # if d > -0.5 and d <= 0.0: # outside the domain
                    # print(d)
                    if channel == 0:
                        img[j0, i0, channel] = compute_bc_value(one_bc_value, bc_order_type, e0, i0, j0, bc_type=_val)
                    if channel == 1:
                        # bc_x = compute_bc_value(one_bc_value, bc_order_type, e0, i0, j0, bc_type=_val)
                        [bc_x, bc_y] = compute_bc_value(one_bc_value, bc_order_type, e0, i0, j0, bc_type=_val)
                        img[j0, i0, channel] = bc_x
                        img[j0, i0, channel+1] = bc_y


def remove_extra_bc_pixel_one_channel(img, pixel, channel, channel_value):
    count = 0
    for i0 in range(1, pixel-1):
        for j0 in range(1, pixel-1):
            # only distance of pixels on the boundary will be calculated to save computational time
            if img[j0,i0,channel] > 0:
                is_on_boundary = False
                if img[j0-1,i0,channel] == -1: 
                    is_on_boundary = True
                if img[j0+1,i0,channel] == -1: 
                    is_on_boundary = True
                if img[j0,i0+1,channel] == -1: 
                    is_on_boundary = True
                if img[j0,i0-1,channel] == -1: 
                    is_on_boundary = True

                if img[j0-1,i0-1,channel] == -1: 
                    is_on_boundary = True
                if img[j0+1,i0+1,channel] == -1: 
                    is_on_boundary = True
                if img[j0-1,i0+1,channel] == -1: 
                    is_on_boundary = True
                if img[j0+1,i0-1,channel] == -1: 
                    is_on_boundary = True

                if not is_on_boundary :
                    img[j0, i0, channel] = channel_value
                # print(img[j0,i0,channel], is_on_boundary, j0, i0)
                count += 1
    # print("channel: ", channel, " total pixels > 0.0: ", count)
    return img

def remove_extra_bc_pixel(img, pixel):
    # # dirichlet, inner = -2
    img = remove_extra_bc_pixel_one_channel(img, pixel, channel=0, channel_value=-2)
    # # neumann, inner  = 0
    img = remove_extra_bc_pixel_one_channel(img, pixel, channel=1, channel_value=0)
    img = remove_extra_bc_pixel_one_channel(img, pixel, channel=2, channel_value=0)

    return img

def update_neumann_bc_value(list_of_bc_values, num_ind, neumann_ind, dirichlet_ind, bc_order_type):
    """
    update Neumann BC values to make sure that the final results are not exceeding 1.0
    """
    # print(list_of_bc_values[num_ind, :, :])
    new_max_value = 1.0
    for i0 in dirichlet_ind:
        # print(list_of_bc_values[num_ind, i0, :])
        new_max_value = min(new_max_value, 1.0-np.max(list_of_bc_values[num_ind, i0, :]) + 0.5)

    # generate new bc values for Neumann BCs
    new_range = [0.5, new_max_value]
    new_bc_values = generate_bc_values(bc_size=len(dirichlet_ind), bc_num=1, bc_range=new_range, bc_order_type=bc_order_type)

    _count = 0
    for i0 in neumann_ind:
        # print(i0, _count, new_bc_values)
        list_of_bc_values[num_ind, i0, :] = new_bc_values[0, _count, :]
        _count += 1
    # print('new_bc_values:', new_bc_values, new_range, list_of_bc_values[num_ind, :, :])

def apply_bcs(poly, pixel, bc_num=5, bc_range=[0.5, 1.0], bc_order_type="constant", shape_id=0, selected_bcs=5):
    """
    Apply constant, linear, quadratic, sinusoidal for the diffusion problem
    """
    global TOTAL_COUNT
    img_ref = np.zeros([pixel, pixel, 3])
    img_ref = assign_region_mask(img_ref, poly, pixel)

    #draw(poly, alpha=0.4)
    total_edges = 0
    list_of_edges = []
    for e0 in poly.edges:
        list_of_edges.append(e0)
        # print(total_edges, e0)
        total_edges += 1

    bc_definition = get_bc_definition(total_edges)
    selected_bc_num = min(selected_bcs, len(bc_definition))
    print('total edges: ', total_edges, 'total bc: ', len(bc_definition), 'selected_bc_num: ', selected_bc_num)
    # exit(0)
    all_keys = np.arange(len(bc_definition))
    np.random.shuffle(all_keys)
    # np.random.shuffle(all_keys)
    # print(all_keys)
    data_count = 0
    # for key, val in bc_definition.items():
    for key in all_keys[0: selected_bc_num]:
        val = bc_definition[key]

        bc_size = len(val.items())
        print ("working on BCs: ", key, val, bc_size)
        list_of_bc_values = generate_bc_values(bc_size=bc_size, bc_num=bc_num, bc_range=bc_range, bc_order_type=bc_order_type)
        # continue

        # print(list_of_bc_values, np.shape(list_of_bc_values))
        for num_ind in range(0, bc_num):
            filename_prefix = "e"+str(total_edges) + "-s"+str(shape_id) + "-" + bc_order_type + "-" + str(data_count)
            img = np.copy(img_ref)
            print('****working on ****:  total edge: ', total_edges, 'shape_id: ', shape_id, 'bc_order_type: ', bc_order_type, 'bc_value_num_id: ', num_ind, 'TOTAL remaining:', TOTAL_COUNT)
            TOTAL_COUNT -= 1

            with_neumann = False
            # need to use list for displacement fields
            neumann_ind = []
            dirichlet_ind = []
            it0 = 0
            for _key, _val in val.items():
                if _val == 'h':
                    with_neumann = True
                    neumann_ind.append(it0)
                else:
                    dirichlet_ind.append(it0)
                it0 += 1
            if with_neumann: update_neumann_bc_value(list_of_bc_values, num_ind, neumann_ind, dirichlet_ind, bc_order_type)

            it0 = 0
            for _key, _val in val.items():
                e0 = list_of_edges[_key]
                one_bc_value = list_of_bc_values[num_ind, it0, :]
                print('one_bc key, value: ', _key, _val, one_bc_value)
                apply_bcs_one_edge(img, list_of_edges, pixel, e0, _val, one_bc_value, bc_order_type)
                it0 += 1
            # if with_neumann: print(list_of_bc_values[num_ind, :, :])
            # create_cubit_mesh_dealii_bc(one_bc=val, one_bc_value=list_of_bc_values[num_ind, :, :], bc_order_type=bc_order_type, filename_prefix=filename_prefix)

            # reset the free BC pixel to the correct mask value
            img_c = img[:, :, 0:1]
            img_h = img[:, :, 1:3]
            img_c = np.where(img_c != 999, img_c, -2) 
            img_h = np.where(img_h != 999, img_h, 0) 
            img = np.concatenate((img_c, img_h), axis=2)

            img = remove_extra_bc_pixel(img, pixel)

            # not showing the pngs
            if False :
                plt.clf()
                plt.subplot(131)
                plt.imshow(img[:,:,0])
                draw(poly, alpha=0.5)
                plot_lines(pixel)

                plt.subplot(132)
                plt.imshow(img[:,:,1])
                plot_lines(pixel)
                draw(poly, alpha=0.5)

                plt.subplot(133)
                plt.imshow(img[:,:,2])
                plot_lines(pixel)
                draw(poly, alpha=0.5)

                plt.savefig(filename_prefix+'.png')
                plt.show()
                # plt.close()
                # exit(0)


            feature = np.expand_dims(img, axis=0)
            label = feature[:,:,:,0:1]
            label = np.where(label != -2, label, 0.5)
            feature_filename = "np-features" + "-" + filename_prefix + ".npy"
            np.save(feature_filename, feature)
            label_filename = feature_filename.replace("features", "labels")
            np.save(label_filename, label)
            data_count += 1


def vary_domain(all_num_points, pixel, bc_num=5, total_shapes=10, selected_bcs=5):
    """
    randomly generate polygons and apply different BCs to them.
    """
    # counter_clock_wise, inside +1, outside -1

    for num_points in all_num_points:
        print("num_points", num_points)
        # how many shapes per group of shapes
        for shape_id in range(0, total_shapes): 
            points = generate_poly_points(num_points, pixel)
            poly = create_polygon(points, pixel, shape_id=shape_id)
            # create_cubit_mesh(poly, pixel, shape_id=shape_id)
            # draw(poly)
            # plt.show()
            apply_bcs(poly, pixel, bc_order_type="constant", bc_num=bc_num, shape_id=shape_id, selected_bcs=selected_bcs)
            apply_bcs(poly, pixel, bc_order_type="linear", bc_num=bc_num, shape_id=shape_id, selected_bcs=selected_bcs)
            apply_bcs(poly, pixel, bc_order_type="quadratic", bc_num=bc_num, shape_id=shape_id, selected_bcs=selected_bcs)
            apply_bcs(poly, pixel, bc_order_type="sinusoidal", bc_num=bc_num, shape_id=shape_id, selected_bcs=selected_bcs)

def merge_one_folder(data_folder):
    """
    Merge all the generated data into one file and delete individual files
    """
    file_list = glob.glob(data_folder + '/np-features*.npy')
    file_list = natsorted(file_list, alg=ns.IGNORECASE)
    # print (file_list)
    features = None
    labels = None

    count = 0
    for f1 in file_list:
        print('file: ', count, f1)
        count += 1
        one_feature = np.load(f1)
        label_path = f1.replace('features', 'labels')
        one_label = np.load(label_path)
        print('file:', f1, 'label:', np.shape(one_label), 'feature:', np.shape(one_feature))
        if (features is None):
            features = np.copy(one_feature)
            labels = np.copy(one_label)
        else:
            features = np.concatenate((features, one_feature), axis=0)
            labels = np.concatenate((labels, one_label), axis=0)
    feature_name = "np-features-all.npy"
    label_name = "np-labels-all.npy"
    np.save(feature_name, features)
    np.save(label_name, labels)
    print('feature shape: ', np.shape(features))
    print('label shape: ', np.shape(labels))
    cmd = 'rm np-*constant*.npy np-*linear*.npy np-*quadratic*.npy np-*sinusoidal*.npy -rf'
    os.system(cmd)


if __name__ ==  "__main__":

    # example run
    Edge_List = [4]
    Pixel = 32
    BC_num = 5
    Total_shape = 5
    Selected_BCs = 4
    Types_BCs = 4 # 4 types of BCs: constant, linear, quadratic, sinusoidal
    TOTAL_COUNT = len(Edge_List) * Types_BCs * Total_shape * Selected_BCs * BC_num
    print('total_count', TOTAL_COUNT)
    vary_domain(all_num_points=Edge_List, pixel=Pixel, bc_num=BC_num, total_shapes=Total_shape, selected_bcs=Selected_BCs) 
    merge_one_folder('./')


====================================================================================================
mechanoChemML\workflows\pde_solver\pde_system_diffusion_steady_state.py
====================================================================================================
import numpy as np

import tensorflow as tf

import mechanoChemML.src.pde_layers as pde_layers
from mechanoChemML.workflows.pde_solver.pde_workflow_steady_state import PDEWorkflowSteadyState

class LayerDiffusionSteadyStateBulkResidual(pde_layers.LayerBulkResidual):
    """
    Steady state bulk residual
    """
    # data: [batch, in_height, in_width, in_channels]
    # filter: [filter_height, filter_width, in_channels, out_channels]
    # dh is needed.

    def __init__(self, dh, normalization_factor=2.0, D0=1.0, name='R_bulk_diffusion'):
        super(LayerDiffusionSteadyStateBulkResidual, self).__init__(name=name)

        self.dh = dh
        self.dof = 1
        self.normalization_factor = normalization_factor
        self.D0 = D0

        self.initialize_arrays()

    def call(self, input):
        """ 
        apply the int (B^T H) dV for element wise c value with 4 nodal value
        - input data: [batch, in_height, in_width, 4] (2x2 nodal values for u)
        - output: [batch, in_height, in_width, 4] (nodal value residual)
        """

        data = self.GetElementInfo(input)
        data = data * self.normalization_factor - 0.5 * self.normalization_factor

        shape=data.get_shape()[0:].as_list()    
        domain_shape = shape[1:3]
        gradu1, gradu2, gradu3, gradu4 = self.ComputeGraduAtGPs(data)
        H1, H2, H3, H4 = self.ConstitutiveRelation(gradu1, gradu2, gradu3, gradu4)
        R = self.ComputeIntTranBxP(H1, H2, H3, H4, domain_shape)
        return R

    def ConstitutiveRelation(self, gradu1, gradu2, gradu3, gradu4):
        # ----------- testing stochastic D0 -------------------
        # random_D0 = tf.random.uniform(tf.shape(gradu1), minval=self.D0-0.5, maxval=self.D0+0.5, dtype=tf.float32)
        # random_D0 = tf.random.normal(tf.shape(gradu1), self.D0, self.D0*0.4, tf.float32, seed=1024)
        # H1 = tf.multiply(gradu1, random_D0) 
        #-----------------------------------------------------

        H1 = self.D0 * gradu1
        H2 = self.D0 * gradu2
        H3 = self.D0 * gradu3
        H4 = self.D0 * gradu4
        return H1, H2, H3, H4


class WeakPDESteadyStateDiffusion(PDEWorkflowSteadyState):
    """

    """

    def __init__(self):
        super().__init__()
        self.dof = 1
        self.dof_name = ['c']
        self.problem_name = 'diffusion'
        self.D0 = 1.0
        self.UseTwoNeumannChannel = True

    def _bulk_residual(self, y_pred):
        """
        bulk residual for steady state diffusion
        """
        elem_bulk_residual=LayerDiffusionSteadyStateBulkResidual(dh=self.dh, D0=self.D0)(y_pred)
        return elem_bulk_residual


if __name__ == '__main__':
    """ Weak PDE constrained NN for steady-state diffusion """
    problem = WeakPDESteadyStateDiffusion()
    problem.run()
    # problem.test(test_folder='DNS')
    # problem.test(test_folder='Test_inter')
    # problem.test(test_folder='Test_extra')
    # problem.debug_problem(use_label=False)
    # problem.debug_problem(use_label=True)
    # problem.test_residual_gaussian(noise_std=1e-4, sample_num=1000)


====================================================================================================
mechanoChemML\workflows\pde_solver\pde_system_elasticity_linear.py
====================================================================================================
import numpy as np

import tensorflow as tf

import mechanoChemML.src.pde_layers as pde_layers
from mechanoChemML.workflows.pde_solver.pde_workflow_steady_state import PDEWorkflowSteadyState

class LayerLinearElasticityBulkResidual(pde_layers.LayerBulkResidual):
    """
    Linear elasticity bulk residual
    """
    # data: [batch, in_height, in_width, in_channels]
    # filter: [filter_height, filter_width, in_channels, out_channels]
    # dh is needed.

    def __init__(self, dh, E0=2.5, nu0=0.3, normalization_factor=2.0, name='R_bulk_elasticity'):
        super(LayerLinearElasticityBulkResidual, self).__init__(name=name)

        self.dh = dh
        self.dof = 2
        self.normalization_factor = normalization_factor
        self.lambda0, self.mu0 = self.E_nu_to_lambda_mu(E=E0, nu=nu0)
        self.initialize_arrays()

    def call(self, input):
        """ 
        apply the int (B^T P) dV for element wise u value with 8 nodal value
        - input data: [batch, in_height, in_width, 8] (2x2x2 nodal values for u)
        - output: [batch, in_height, in_width, 8] (nodal value residual)
        """
        # scaled_data = non_scaled/normalization_factor + 0.5 for range  [-x, +x]
        # non_scaled = scaled_data * normalization_factor - 0.5 * normalization_factor

        data = self.GetElementInfo(input)
        data = data * self.normalization_factor - 0.5 * self.normalization_factor

        shape = data.get_shape()[0:].as_list()    
        domain_shape = shape[1:3]
        gradu1, gradu2, gradu3, gradu4 = self.ComputeGraduAtGPs(data)

        I4, I2x2 = self.Get2ndOrderIdentityTensor(gradu1, domain_shape)
        epsilon = self.GetEpsilon(gradu1, gradu2, gradu3, gradu4, domain_shape)

        sigma1, sigma2, sigma3, sigma4 = self.ConstitutiveRelation(epsilon, I2x2)
        R = self.ComputeIntTranBxP(sigma1, sigma2, sigma3, sigma4, domain_shape)
        return R

    def ConstitutiveRelation(self, epsilon, I2x2):
        """
        Linear elasticity constitutive relationship
        """
        epsilon_trace = tf.linalg.trace(epsilon)
        epsilon_trace = tf.expand_dims(epsilon_trace, 4)
        epsilon_trace = tf.expand_dims(epsilon_trace, 5)

        # get sigma for linear elasticity
        sigma = self.lambda0 * tf.math.multiply(epsilon_trace, I2x2) + 2.0 * self.mu0 * epsilon

        sigma1 = sigma[:,:,:,0,:,:]
        sigma2 = sigma[:,:,:,1,:,:]
        sigma3 = sigma[:,:,:,2,:,:]
        sigma4 = sigma[:,:,:,3,:,:]

        sigma1 = tf.reshape(sigma1, [-1, 4])
        sigma2 = tf.reshape(sigma2, [-1, 4])
        sigma3 = tf.reshape(sigma3, [-1, 4])
        sigma4 = tf.reshape(sigma4, [-1, 4])
        return sigma1, sigma2, sigma3, sigma4


class WeakPDELinearElasticity(PDEWorkflowSteadyState):
    def __init__(self):
        super().__init__()
        self.dof = 2
        self.dof_name = ['Ux', 'Uy']
        self.problem_name = 'linear-elasticity'
        self.E0 = 25
        self.nu0 = 0.3
        self.UseTwoNeumannChannel = False

    def _bulk_residual(self, y_pred):
        """
        bulk residual for linear elasticity
        """
        elem_bulk_residual=LayerLinearElasticityBulkResidual(dh=self.dh, E0=self.E0, nu0=self.nu0)(y_pred)
        return elem_bulk_residual


if __name__ == '__main__':
    """ Weak PDE constrained NN for linear elasticity """
    problem = WeakPDELinearElasticity()
    problem.run()
    # problem.test(test_folder='DNS')
    # problem.test(test_folder='Test_inter')
    # problem.test(test_folder='Test_extra')
    # problem.debug_problem(use_label=False)
    # problem.debug_problem(use_label=True)
    # problem.test_residual_gaussian(noise_std=1e-4, sample_num=1000)


====================================================================================================
mechanoChemML\workflows\pde_solver\pde_system_elasticity_nonlinear.py
====================================================================================================
import numpy as np

import tensorflow as tf

import mechanoChemML.src.pde_layers as pde_layers
from mechanoChemML.workflows.pde_solver.pde_workflow_steady_state import PDEWorkflowSteadyState

class LayerNonLinearElasticityBulkResidual(pde_layers.LayerBulkResidual):
    """
    Non-linear elasticity bulk residual
    """
    # data: [batch, in_height, in_width, in_channels]
    # filter: [filter_height, filter_width, in_channels, out_channels]
    # dh is needed.

    def __init__(self, dh, E0=2.5, nu0=0.3, normalization_factor=2.0, name='R_bulk_elasticity'):
        super(LayerNonLinearElasticityBulkResidual, self).__init__(name=name)

        self.dh = dh
        self.dof = 2
        self.normalization_factor = normalization_factor
        self.lambda0, self.mu0 = self.E_nu_to_lambda_mu(E=E0, nu=nu0)
        self.initialize_arrays()

    def call(self, input):
        """ 
        apply the int (B^T P) dV for element wise u value with 8 nodal value
        - input data: [batch, in_height, in_width, 4*dof] (2x2x2 nodal values for u)
        - output: [batch, in_height, in_width, 4*dof] (nodal value residual)
        """

        data = self.GetElementInfo(input)
        data = data * self.normalization_factor - 0.5 * self.normalization_factor
        shape = data.get_shape()[0:].as_list()    
        domain_shape = shape[1:3]
        gradu1, gradu2, gradu3, gradu4 = self.ComputeGraduAtGPs(data)

        I4, I2x2 = self.Get2ndOrderIdentityTensor(gradu1, domain_shape)
        F2x2 = self.GetF(gradu1, gradu2, gradu3, gradu4, I4, domain_shape)
        P1, P2, P3, P4 = self.ConstitutiveRelation(F2x2, I2x2)
        R = self.ComputeIntTranBxP(P1, P2, P3, P4, domain_shape)
        return R

    def ConstitutiveRelation(self, F2x2, I2x2):
        """
        Non-linear elasticity constitutive relationship
        """
        detF = tf.expand_dims(tf.linalg.det(F2x2), 4)
        detF = tf.expand_dims(detF, 5)

        #---------------------------------------------------------------------------------
        # get detF mask to make sure inv(F2x2) works.
        # It is very possible that F2x2 is not invertible. The following will set
        # the region where detF < 0.5 or detF > 3.0 to identity tensor to make sure
        # F2x2 is invertible, as a numerical solution.

        detF_mask_finite = tf.where(tf.math.is_finite(detF), tf.fill(tf.shape(detF), 1.0), tf.fill(tf.shape(detF), 0.0))
        detF_mask_negative = tf.where(detF < 0.1, tf.fill(tf.shape(detF), 0.0), tf.fill(tf.shape(detF), 1.0))
        detF_mask_large = tf.where(detF > 5.0, tf.fill(tf.shape(detF), 0.0), tf.fill(tf.shape(detF), 1.0))

        detF_mask = tf.multiply(detF_mask_negative, detF_mask_large)
        detF_mask = tf.multiply(detF_mask, detF_mask_finite)
        detF_mask_reverse = tf.where( detF_mask == 0, tf.fill(tf.shape(detF_mask), 1.0), tf.fill(tf.shape(detF_mask), 0.0))

        F2x2_modified = tf.multiply(F2x2, detF_mask) + tf.multiply(I2x2, detF_mask_reverse)

        detF = tf.expand_dims(tf.linalg.det(F2x2_modified), 4)
        detF = tf.expand_dims(detF, 5)
        #---------------------------------------------------------------------------------

        # get other values
        try:
            InvF = tf.linalg.inv(F2x2_modified)
        except tensorflow.python.framework.errors_impl.InvalidArgumentError:
            raise ValueError ('F2x2 not invertable', F2x2_modified)

        TransInvF = tf.transpose(InvF, perm=[0,1,2,3,5,4])

        # get P
        P = self.lambda0 * (tf.math.multiply(detF,detF) - detF) * TransInvF + self.mu0 * ( F2x2_modified - TransInvF)
        P = tf.multiply(P, detF_mask)

        P1 = P[:,:,:,0,:,:]
        P2 = P[:,:,:,1,:,:]
        P3 = P[:,:,:,2,:,:]
        P4 = P[:,:,:,3,:,:]

        P1 = tf.reshape(P1, [-1, 4])
        P2 = tf.reshape(P2, [-1, 4])
        P3 = tf.reshape(P3, [-1, 4])
        P4 = tf.reshape(P4, [-1, 4])
        return P1, P2, P3, P4


class WeakPDENonLinearElasticity(PDEWorkflowSteadyState):
    def __init__(self):
        super().__init__()
        self.dof = 2
        self.dof_name = ['Ux', 'Uy']
        self.problem_name = 'nonlinear-elasticity'
        self.E0 = 25
        self.nu0 = 0.3
        self.UseTwoNeumannChannel = False

    def _bulk_residual(self, y_pred):
        """
        bulk residual for nonlinear elasticity
        """
        elem_bulk_residual=LayerNonLinearElasticityBulkResidual(dh=self.dh, E0=self.E0, nu0=self.nu0)(y_pred)
        return elem_bulk_residual


if __name__ == '__main__':
    """ Weak PDE constrained NN for nonlinear elasticity """
    problem = WeakPDENonLinearElasticity()
    problem.run()
    # problem.test(test_folder='DNS')
    # problem.test(test_folder='Test_inter')
    # problem.test(test_folder='Test_extra')
    # problem.debug_problem(use_label=False)
    # problem.debug_problem(use_label=True)
    # problem.test_residual_gaussian(noise_std=1e-4, sample_num=1000)


====================================================================================================
mechanoChemML\workflows\pde_solver\pde_utility.py
====================================================================================================
import math
import matplotlib.pyplot as plt
import matplotlib as mpl
import tensorflow as tf
import numpy as np
import datetime
import pickle

import os
import numpy as np
import tensorflow as tf

import mechanoChemML.src.pde_layers as pde_layers

def plot_tex(tex=False):
    mpl.style.reload_library()
    plt.style.use('zxx')
    print('find zxx: ', os.path.isfile('zxx.mplstyle'))
    if (os.path.isfile('zxx.mplstyle')):
        plt.style.use('zxx.mplstyle')
    if (tex) :
        plt.style.use('tex')
    print(plt.style.available)
    print(mpl.get_configdir())

def get_cm():
    from matplotlib.colors import LinearSegmentedColormap
    colors = [(0, 0, 1), (0,1,1), (0, 1, 0), (1,1,0), (1, 0, 0)] 
    cmap_name = 'hot'
    # Create the colormap
    cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=256)
    cm.set_bad(color='white')
    return cm

def plot_one_loss(pickle_file, png_filename, show_line=True):
    saved_config = pickle.load(open(pickle_file, "rb"))
    # for key, item in saved_config.items():
        # print(key)
    losses = saved_config['losses']
    # for key, item in losses.items():
        # print(key)
    plt.clf()
    plot_tex(True)

    if np.min(losses['loss']) < 0:
        plt.plot(losses['loss'], 'b')
        plt.plot(losses['val_loss'], 'k')
        plt.yscale('symlog')
        if show_line:
            plt.axvline(100, color ='k', linestyle ="--") 
            plt.axvline(500, color ='k', linestyle ="--") 
            plt.axvline(1000, color ='k', linestyle ="--") 
            plt.axvline(2000, color ='k', linestyle ="--") 
            plt.axvline(4000, color ='k', linestyle ="--") 
    else:
        plt.semilogy(losses['loss'], 'b')
        plt.semilogy(losses['val_loss'], 'k')
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.legend(['Training', 'Validation'])
    # plt.xlim([0,5000])
    plt.tight_layout()
    plt.savefig(png_filename)
    # plt.show()
    print('save to:', png_filename)
    # exit(0)

def plot_sigma2(pickle_file, png_filename, show_line=True, sigma1=''):
    saved_config = pickle.load(open(pickle_file, "rb"))
    sigma2 = saved_config['var_sigma2']
    plt.clf()
    plot_tex(True)

    plt.semilogy(sigma2, 'k')

    if show_line: 
        plt.axvline(100, color ='k', linestyle ="--") 
        plt.axvline(500, color ='k', linestyle ="--") 
        plt.axvline(1000, color ='k', linestyle ="--") 
        plt.axvline(2000, color ='k', linestyle ="--") 
        plt.axvline(4000, color ='k', linestyle ="--") 
    plt.xlabel('epoch')
    plt.ylabel(r'$\Sigma_2$')
    # plt.xlim([0,5000])
    # plt.legend([sigma1])
    plt.title(sigma1)
    plt.tight_layout()
    plt.savefig(png_filename)
    # plt.show()
    print('save to:', png_filename)
    # exit(0)

def plot_PDE_solutions_new(img_input, img_label, img_pre_mean, img_pre_var, img_pre_std, dof=1, dof_name=['c'], tot_img=6, filename='', fig_size=3.3):
    """
    plot the results of PDEs

    Args:
        img_input (numpy array): size of [1, :, :, dof*3]
        img_label (numpy array): size of [1, :, :, dof]
        img_pre_mean (numpy array): size of [1, :, :, dof]
        img_pre_var (numpy array): size of [1, :, :, dof]
        img_pre_std (numpy array): size of [1, :, :, dof] 
        dof (int): default (=1)
        dof_name (list): list of string (default ['c'])
        tot_img (int): without plotting std (tot_img=6, default), with std (tot_img=7)
        filename (str): default ('')
    """
    dof_name = ['','']
    hot=get_cm()
    bc_mask_dirichlet = pde_layers.ComputeBoundaryMaskNodalData(img_input, dof=dof, opt=1)
    the_bc_mask_dirichlet = tf.squeeze(bc_mask_dirichlet, [0])

    the_img_input = tf.squeeze(img_input, [0])
    the_img_label = tf.squeeze(img_label, [0])
    the_img_pre_mean = tf.squeeze(img_pre_mean, [0])
    the_img_pre_var = tf.squeeze(img_pre_var, [0])
    the_img_pre_std = tf.squeeze(img_pre_std, [0])
    # print('Dirichlet BC shape:', tf.shape(bc_mask_dirichlet))
    # print('Dirichlet BC:', bc_mask_dirichlet)

    # magic number from: https://stackoverflow.com/questions/18195758/set-matplotlib-colorbar-size-to-match-graph
    fraction=0.046
    pad=0.04

    # make the margin as NaN ( margin = -1)
    # the problem domain internal part of the input is not filled with random numbers (thus it is = -2)
    tmp_img_label = np.concatenate((the_img_label, the_img_label), axis=2)
    # the_img_input = np.ma.masked_where(tmp_img_label < -0.9, the_img_input)
    the_img_input = np.ma.masked_where(the_img_input <= 0.0, the_img_input)

    the_img_pre_mean = np.ma.masked_where(the_img_label < -0.9, the_img_pre_mean)
    the_img_pre_var = np.ma.masked_where(the_img_label < -0.9, the_img_pre_var)
    the_img_pre_std = np.ma.masked_where(the_img_label < -0.9, the_img_pre_std)

    the_img_label = np.ma.masked_where(the_img_label < -0.9, the_img_label)

    the_img_mark = 1.0e-10 * np.ones(np.shape(the_img_label))

    # remove the Dirichlet BCs region for mean, var, std by setting the value to NaN
    # the_img_pre_mean = np.ma.masked_where(the_bc_mask_dirichlet == 0.0, the_img_pre_mean)
    the_img_pre_var = np.ma.masked_where(the_bc_mask_dirichlet == 0.0, the_img_pre_var)
    the_img_pre_std = np.ma.masked_where(the_bc_mask_dirichlet == 0.0, the_img_pre_std)
    # the_img_label = np.ma.masked_where(the_bc_mask_dirichlet == 0.0, the_img_label)

    figsize_list_x = [x*fig_size*1.1 for x in range(1, 20)]
    figsize_list_y = [x*fig_size for x in range(1, 20)]

    # for debugging purpose only to show intermediate results for NN predictions
    tot_img = tot_img +1 

    fig = plt.figure(figsize=(figsize_list_x[tot_img-1], figsize_list_y[dof-1] ))

    for i0 in range(0, dof):
        # display Dirichlet BCs
        ax = plt.subplot(dof, tot_img, 1 + tot_img * i0)
        c_img = plt.imshow(the_img_input[:, :, i0], cmap=hot)  # tensor
        plt.colorbar(c_img, fraction=fraction, pad=pad)
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        plt.title('Dirichlet BC ' + dof_name[i0])

        # display Neumann BCs
        ax = plt.subplot(dof, tot_img, 2 + tot_img * i0)
        c_img = plt.imshow(the_img_input[:, :, dof+i0], cmap=hot)  # tensor
        plt.colorbar(c_img, fraction=fraction, pad=pad)
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        plt.title('Neumann BC (x) ' + dof_name[i0])

        # display Neumann BCs
        ax = plt.subplot(dof, tot_img, 3 + tot_img * i0)
        c_img = plt.imshow(the_img_input[:, :, dof+i0+1], cmap=hot)  # tensor
        plt.colorbar(c_img, fraction=fraction, pad=pad)
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        plt.title('Neumann BC (y) ' + dof_name[i0])

        # display label
        ax = plt.subplot(dof, tot_img, 4 + tot_img * i0)
        c_img = plt.imshow(the_img_label[:, :, i0], cmap=hot)  # tensor
        plt.colorbar(c_img, fraction=fraction, pad=pad)
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        label_min = np.amin(the_img_label[:, :, i0])
        label_max = np.amax(the_img_label[:, :, i0])
        plt.title('DNS ' + dof_name[i0])

        # display reconstruction: mean
        ax = plt.subplot(dof, tot_img, 5 + tot_img * i0)
        # use the same range for better visual comparison
        print('Pred. Mean. is using label_min and label_max as colorbar range. Thus, the plot might not look so right.')
        c_img = plt.imshow(the_img_pre_mean[:, :, i0], cmap=hot, vmin=label_min, vmax=label_max)  # tensor
        # c_img = plt.imshow(the_img_pre_mean[:, :, i0], cmap=hot)  # tensor
        plt.colorbar(c_img, fraction=fraction, pad=pad)
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        if tot_img >= 8:
            plt.title('Pred. Mean ' + dof_name[i0])
        else:
            plt.title('Pred. ' + dof_name[i0])

        # display error
        ax = plt.subplot(dof, tot_img, 6 + tot_img * i0)
        # the denominator - 0.5 is needed. Otherwise, the relative error is too small and not correct, as the scaled zero is 0.5.
        # c_img = plt.imshow((the_img_label[:, :, i0] - the_img_pre_mean[:, :, i0]) / ((the_img_label[:, :, i0] + the_img_mark[:, :, i0]) - 0.5), cmap=hot)  # tensor
        c_img = plt.imshow(the_img_pre_mean[:, :, i0] - the_img_label[:, :, i0], cmap=hot)  # tensor
        plt.colorbar(c_img, fraction=fraction, pad=pad)
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        # plt.title('Rel. Error ' + dof_name[i0])
        plt.title('Pointwise Error ' + dof_name[i0])

        if tot_img >= 7:
            # # display reconstruction: var
            # ax = plt.subplot(dof, tot_img, 7 + tot_img * i0)
            # c_img = plt.imshow(the_img_pre_var[:, :, i0], cmap=hot)  # tensor
            # plt.colorbar(c_img, fraction=fraction, pad=pad)
            # ax.get_xaxis().set_visible(False)
            # ax.get_yaxis().set_visible(False)
            # plt.title('Pred. Var. ' + dof_name[i0])


            # display reconstruction: mean
            ax = plt.subplot(dof, tot_img, 7 + tot_img * i0)
            # use the same range for better visual comparison
            print('Pred. Mean. is using label_min and label_max as colorbar range. Thus, the plot might not look so right.')
            c_img = plt.imshow(the_img_pre_mean[:, :, i0], cmap=hot)  # tensor
            # c_img = plt.imshow(the_img_pre_mean[:, :, i0], cmap=hot)  # tensor
            plt.colorbar(c_img, fraction=fraction, pad=pad)
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)
            plt.title('Pred. (actual range) ' + dof_name[i0])

            if tot_img == 8 :
                # display reconstruction: std
                ax = plt.subplot(dof, tot_img, 8 + tot_img * i0)
                c_img = plt.imshow(2.0 * the_img_pre_std[:, :, i0], cmap=hot)  # tensor
                plt.colorbar(c_img, fraction=fraction, pad=pad)
                ax.get_xaxis().set_visible(False)
                ax.get_yaxis().set_visible(False)
                plt.title('Pred. (2xStd.) ' + dof_name[i0])

    plt.tight_layout()
    if filename:
        plt.savefig(filename)
    else:
        now_str = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
        plt.savefig("prediction" + now_str + ".png")
    # plt.show()
    # exit(0)

def plot_PDE_solutions(img_input, img_label, img_pre_mean, img_pre_var, img_pre_std, dof=1, dof_name=['c'], tot_img=6, filename='', fig_size=2.2):
    """
    plot the results of PDEs

    Args:
        img_input (numpy array): size of [1, :, :, dof*2]
        img_label (numpy array): size of [1, :, :, dof]
        img_pre_mean (numpy array): size of [1, :, :, dof]
        img_pre_var (numpy array): size of [1, :, :, dof]
        img_pre_std (numpy array): size of [1, :, :, dof] 
        dof (int): default (=1)
        dof_name (list): list of string (default ['c'])
        tot_img (int): without plotting std (tot_img=6, default), with std (tot_img=7)
        filename (str): default ('')
    """
    hot=get_cm()
    bc_mask_dirichlet = pde_layers.ComputeBoundaryMaskNodalData(img_input, dof=dof, opt=1)
    the_bc_mask_dirichlet = tf.squeeze(bc_mask_dirichlet, [0])

    the_img_input = tf.squeeze(img_input, [0])
    the_img_label = tf.squeeze(img_label, [0])
    the_img_pre_mean = tf.squeeze(img_pre_mean, [0])
    the_img_pre_var = tf.squeeze(img_pre_var, [0])
    the_img_pre_std = tf.squeeze(img_pre_std, [0])
    # print('Dirichlet BC shape:', tf.shape(bc_mask_dirichlet))
    # print('Dirichlet BC:', bc_mask_dirichlet)

    # magic number from: https://stackoverflow.com/questions/18195758/set-matplotlib-colorbar-size-to-match-graph
    fraction=0.046
    pad=0.04

    # make the margin as NaN ( margin = -1)
    # the problem domain internal part of the input is not filled with random numbers (thus it is = -2)
    tmp_img_label = np.concatenate((the_img_label, the_img_label), axis=2)
    # the_img_input = np.ma.masked_where(tmp_img_label < -0.9, the_img_input)
    the_img_input = np.ma.masked_where(the_img_input <= 0.0, the_img_input)

    the_img_pre_mean = np.ma.masked_where(the_img_label < -0.9, the_img_pre_mean)
    the_img_pre_var = np.ma.masked_where(the_img_label < -0.9, the_img_pre_var)
    the_img_pre_std = np.ma.masked_where(the_img_label < -0.9, the_img_pre_std)

    the_img_label = np.ma.masked_where(the_img_label < -0.9, the_img_label)

    the_img_mark = 1.0e-10 * np.ones(np.shape(the_img_label))

    # remove the Dirichlet BCs region for mean, var, std by setting the value to NaN
    the_img_pre_mean = np.ma.masked_where(the_bc_mask_dirichlet == 0.0, the_img_pre_mean)
    the_img_pre_var = np.ma.masked_where(the_bc_mask_dirichlet == 0.0, the_img_pre_var)
    the_img_pre_std = np.ma.masked_where(the_bc_mask_dirichlet == 0.0, the_img_pre_std)
    the_img_label = np.ma.masked_where(the_bc_mask_dirichlet == 0.0, the_img_label)

    figsize_list_x = [x*fig_size*1.1 for x in range(1, 20)]
    figsize_list_y = [x*fig_size for x in range(1, 20)]

    fig = plt.figure(figsize=(figsize_list_x[tot_img-1], figsize_list_y[dof-1] ))

    for i0 in range(0, dof):
        # display Dirichlet BCs
        ax = plt.subplot(dof, tot_img, 1 + tot_img * i0)
        c_img = plt.imshow(the_img_input[:, :, i0], cmap=hot)  # tensor
        plt.colorbar(c_img, fraction=fraction, pad=pad)
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        plt.title('Dirichlet BC ' + dof_name[i0])

        # display Neumann BCs
        ax = plt.subplot(dof, tot_img, 2 + tot_img * i0)
        c_img = plt.imshow(the_img_input[:, :, dof+i0], cmap=hot)  # tensor
        plt.colorbar(c_img, fraction=fraction, pad=pad)
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        plt.title('Neumann BC ' + dof_name[i0])

        # display label
        ax = plt.subplot(dof, tot_img, 3 + tot_img * i0)
        c_img = plt.imshow(the_img_label[:, :, i0], cmap=hot)  # tensor
        plt.colorbar(c_img, fraction=fraction, pad=pad)
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        label_min = np.amin(the_img_label[:, :, i0])
        label_max = np.amax(the_img_label[:, :, i0])
        plt.title('DNS ' + dof_name[i0])

        # display reconstruction: mean
        ax = plt.subplot(dof, tot_img, 4 + tot_img * i0)
        # use the same range for better visual comparison
        print('Pred. Mean. is using label_min and label_max as colorbar range. Thus, the plot might not look so right.')
        # c_img = plt.imshow(the_img_pre_mean[:, :, i0], cmap=hot, vmin=label_min, vmax=label_max)  # tensor
        c_img = plt.imshow(the_img_pre_mean[:, :, i0], cmap=hot)  # tensor
        plt.colorbar(c_img, fraction=fraction, pad=pad)
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        plt.title('Pred. Mean ' + dof_name[i0])

        # display error
        ax = plt.subplot(dof, tot_img, 5 + tot_img * i0)
        # the denominator - 0.5 is needed. Otherwise, the relative error is too small and not correct, as the scaled zero is 0.5.
        # c_img = plt.imshow((the_img_label[:, :, i0] - the_img_pre_mean[:, :, i0]) / ((the_img_label[:, :, i0] + the_img_mark[:, :, i0]) - 0.5), cmap=hot)  # tensor
        c_img = plt.imshow(the_img_pre_mean[:, :, i0] - the_img_label[:, :, i0], cmap=hot)  # tensor
        plt.colorbar(c_img, fraction=fraction, pad=pad)
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        # plt.title('Rel. Error ' + dof_name[i0])
        plt.title('Pointwise Error ' + dof_name[i0])

        if tot_img >= 6:
            # display reconstruction: var
            ax = plt.subplot(dof, tot_img, 6 + tot_img * i0)
            c_img = plt.imshow(the_img_pre_var[:, :, i0], cmap=hot)  # tensor
            plt.colorbar(c_img, fraction=fraction, pad=pad)
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)
            plt.title('Pred. Var. ' + dof_name[i0])

            if tot_img == 7 :
                # display reconstruction: std
                ax = plt.subplot(dof, tot_img, 7 + tot_img * i0)
                c_img = plt.imshow(2.0 * the_img_pre_std[:, :, i0], cmap=hot)  # tensor
                plt.colorbar(c_img, fraction=fraction, pad=pad)
                ax.get_xaxis().set_visible(False)
                ax.get_yaxis().set_visible(False)
                plt.title('Pred. (2xStd.) ' + dof_name[i0])

    plt.tight_layout()
    if filename:
        plt.savefig(filename)
    else:
        now_str = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
        plt.savefig("prediction" + now_str + ".png")
    # plt.show()


def plot_fields(list_of_field, list_of_field_name, dof, dof_name,  filename='', print_data=False, vmin=None, vmax=None, Tex=False, fig_size=2.2, mask=False):
    """
    plot the fields

    Args:
        list_of_field (list): list of numpy array [1, :, :, dof]
        list_of_field_name (list): list of strings
        dof (int): dof per node
        dof_name (list): list of string 
        filename (str): default ('')
    """
    if Tex:
        plot_tex(Tex)

    hot=get_cm()

    # magic number from: https://stackoverflow.com/questions/18195758/set-matplotlib-colorbar-size-to-match-graph
    fraction=0.046
    pad=0.04

    tot_img = len(list_of_field_name)
    # print('tot_img:', tot_img)

    figsize_list_x = [x*fig_size*1.1 for x in range(1, 20)]
    figsize_list_y = [x*fig_size for x in range(1, 20)]

    fig = plt.figure(figsize=(figsize_list_x[tot_img-1], figsize_list_y[dof-1] ))
    for i0 in range(0, dof):
        for j0 in range(0, tot_img):
            _field_name = list_of_field_name[j0] + ' ' + dof_name[i0]
            ax = plt.subplot(dof, tot_img, j0 + 1 + tot_img * i0)
            one_field = list_of_field[j0][0, :, :, i0]
            if mask:
                one_field = np.ma.masked_where(one_field < -0.9, one_field)
            c_img = plt.imshow(one_field, cmap=hot, vmin=vmin, vmax=vmax)  # tensor
            if print_data:
                print(_field_name, list_of_field[j0][0, :, :, i0])
            plt.colorbar(c_img, fraction=fraction, pad=pad)
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)
            plt.title(_field_name)

    plt.tight_layout()
    if filename:
        plt.savefig(filename)
    else:
        now_str = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
        plt.savefig("prediction" + now_str + ".png")
    # plt.show()


def split_data(datax, datay, batch_size, split_ratio=['0.8', '0.1', '0.1']):
    """ split data according to a specific ratio """

    split_ratio = [float(x) for x in split_ratio]
    if (len(split_ratio) != 3 or abs(sum(split_ratio) - 1.0) > 1.0e-5):
        raise ValueError(
            'split ratio should be a list containing three float values with sum() == 1.0!!! Your current split_ratio = ',
            split_ratio, ' with sum = ', sum(split_ratio))
    tr_ratio = float(split_ratio[0])
    cv_ratio = float(split_ratio[1])
    tt_ratio = float(split_ratio[2])

    number_examples = datax.shape[0]
    idx = np.arange(0, number_examples)
    np.random.shuffle(idx)
    datax = [datax[i] for i in idx]    # get list of `num` random samples
    datay = [datay[i] for i in idx]    # get list of `num` random samples

    start = 0
    end_tr = int(tr_ratio * number_examples / batch_size) * batch_size 
    end_cv = int((tr_ratio + cv_ratio) * number_examples)
    end_tt = number_examples
    tr_datax = np.array(datax[start:end_tr])
    tr_datay = np.array(datay[start:end_tr])
    cv_datax = np.array(datax[end_tr:end_cv])
    cv_datay = np.array(datay[end_tr:end_cv])
    tt_datax = np.array(datax[end_cv:end_tt])
    tt_datay = np.array(datay[end_cv:end_tt])

    return tr_datax, tr_datay, cv_datax, cv_datay, tt_datax, tt_datay

def split_data_heter(datax, datay, dataz, batch_size, split_ratio=['0.8', '0.1', '0.1']):
    """ split data according to a specific ratio """

    split_ratio = [float(x) for x in split_ratio]
    if (len(split_ratio) != 3 or abs(sum(split_ratio) - 1.0) > 1.0e-5):
        raise ValueError(
            'split ratio should be a list containing three float values with sum() == 1.0!!! Your current split_ratio = ',
            split_ratio, ' with sum = ', sum(split_ratio))
    tr_ratio = float(split_ratio[0])
    cv_ratio = float(split_ratio[1])
    tt_ratio = float(split_ratio[2])

    number_examples = datax.shape[0]
    idx = np.arange(0, number_examples)
    np.random.shuffle(idx)
    datax = [datax[i] for i in idx]    # get list of `num` random samples
    datay = [datay[i] for i in idx]    # get list of `num` random samples
    dataz = [dataz[i] for i in idx]    # get list of `num` random samples

    start = 0
    end_tr = int(tr_ratio * number_examples / batch_size) * batch_size 
    end_cv = int((tr_ratio + cv_ratio) * number_examples)
    end_tt = number_examples
    tr_datax = np.array(datax[start:end_tr])
    tr_datay = np.array(datay[start:end_tr])
    tr_dataz = np.array(dataz[start:end_tr])
    cv_datax = np.array(datax[end_tr:end_cv])
    cv_datay = np.array(datay[end_tr:end_cv])
    cv_dataz = np.array(dataz[end_tr:end_cv])
    tt_datax = np.array(datax[end_cv:end_tt])
    tt_datay = np.array(datay[end_cv:end_tt])
    tt_dataz = np.array(dataz[end_cv:end_tt])

    return tr_datax, tr_datay, tr_dataz, cv_datax, cv_datay, cv_dataz, tt_datax, tt_datay, tt_dataz


def expand_dataset(features, labels, times):
    """ expand the features and labels to 2^(n+1) with n=times """
    for i in range(0, times):
        features = np.concatenate((features, features), axis=0)
        labels = np.concatenate((labels, labels), axis=0)
        # print('2^(' + '{}'.format(i + 1) + '): feature shape = ', np.shape(features), ' label shape = ', np.shape(labels))
    return features, labels

def ExpandDatasetHeter(features, mats, labels, times):
    """ expand the features and labels to 2^(n+1) with n=times """
    for i in range(0, times):
        features = np.concatenate((features, features), axis=0)
        mats = np.concatenate((mats, mats), axis=0)
        labels = np.concatenate((labels, labels), axis=0)
        # print('2^(' + '{}'.format(i + 1) + '): feature shape = ', np.shape(features), ' label shape = ', np.shape(labels))
    return features, mats, labels


class BatchDataHeter(tf.keras.utils.Sequence):
    """Produces a sequence of the data with labels."""
    """Borrowed from: class MNISTSequence(tf.keras.utils.Sequence) """

    def __init__(self, data, batch_size=128):
        """Initializes the sequence.

    Args:
      data: Tuple of numpy `array` instances, the first representing images and
            the second labels.
      batch_size: Integer, number of elements in each training batch.
    """
        self.features, self.mats, self.labels = data
        # self.features, self.labels = BatchData.__preprocessing(images, labels)
        self.batch_size = batch_size

    # @staticmethod
    # def __preprocessing(images, labels):
    # """Preprocesses image and labels data.

    # Args:
    # images: Numpy `array` representing the image data.
    # labels: Numpy `array` representing the labels data (range 0-9).

    # Returns:
    # images: Numpy `array` representing the image data, normalized
    # and expanded for convolutional network input.
    # labels: Numpy `array` representing the labels data (range 0-9),
    # as one-hot (categorical) values.
    # """
    # # images = 2 * (images / 255.) - 1. # normalization
    # images = images[..., tf.newaxis]

    # labels = tf.keras.utils.to_categorical(labels)
    # return images, labels

    def __len__(self):
        return int(tf.math.ceil(len(self.features) / self.batch_size))  # contains batches less than the size of batch_size
        # return int(len(self.features) / self.batch_size) # all batches are equal-sized.

    def __getitem__(self, idx):
        batch_x = self.features[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_m = self.mats[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]
        return [batch_x, batch_m], batch_y


class BatchData(tf.keras.utils.Sequence):
    """Produces a sequence of the data with labels."""
    """Borrowed from: class MNISTSequence(tf.keras.utils.Sequence) """

    def __init__(self, data, batch_size=128):
        """Initializes the sequence.

    Args:
      data: Tuple of numpy `array` instances, the first representing images and
            the second labels.
      batch_size: Integer, number of elements in each training batch.
    """
        self.features, self.labels = data
        # self.features, self.labels = BatchData.__preprocessing(images, labels)
        self.batch_size = batch_size

    # @staticmethod
    # def __preprocessing(images, labels):
    # """Preprocesses image and labels data.

    # Args:
    # images: Numpy `array` representing the image data.
    # labels: Numpy `array` representing the labels data (range 0-9).

    # Returns:
    # images: Numpy `array` representing the image data, normalized
    # and expanded for convolutional network input.
    # labels: Numpy `array` representing the labels data (range 0-9),
    # as one-hot (categorical) values.
    # """
    # # images = 2 * (images / 255.) - 1. # normalization
    # images = images[..., tf.newaxis]

    # labels = tf.keras.utils.to_categorical(labels)
    # return images, labels

    def __len__(self):
        return int(tf.math.ceil(len(self.features) / self.batch_size))  # contains batches less than the size of batch_size
        # return int(len(self.features) / self.batch_size) # all batches are equal-sized.

    def __getitem__(self, idx):
        batch_x = self.features[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]
        return batch_x, batch_y


class BatchDataTime(tf.keras.utils.Sequence):
    """Produces a sequence of the data with labels."""
    """Borrowed from: class MNISTSequence(tf.keras.utils.Sequence) """

    def __init__(self, data, batch_size=128):
        """Initializes the sequence.

    Args:
      data: Tuple of numpy `array` instances, the first representing images and
            the second labels.
      batch_size: Integer, number of elements in each training batch.
    """
        self.features, self.time, self.labels,  = data
        self.batch_size = batch_size

    def __len__(self):
        # return int(tf.math.ceil(len(self.features) / self.batch_size))  # contains batches less than the size of batch_size
        return int(len(self.features) / self.batch_size) # all batches are equal-sized.

    def __getitem__(self, idx):
        batch_x    = self.features[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_x_time = self.time[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_y    = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]
        return batch_x, batch_x_time, batch_y


def exe_cmd(cmd, output=False):
    import subprocess, os
    if output:
        output_info = os.popen(cmd).read()
        return output_info
    else:
        os.system(cmd)



def plot_one_field_stat(data, dpi=150, name='stat.png'):
    """
    Plot the statistics of a data

    Args:
        data (numpy array): data[:, :, :]
        dpi (int): dpi of png (=150)
        name (str): name of png output (='stat.png') 
    """
    sample_num = tf.shape(data).numpy()[0]
    # print('data', tf.shape(data))
    mean_data = tf.reduce_mean(data, axis=0)
    # print('mean-data', tf.shape(mean_data))
    std_data = tf.math.reduce_std(data, axis=0)
    # print('std-data', tf.shape(std_data))
    expand_mean_data = tf.tile(tf.expand_dims(mean_data, axis=0), [sample_num, 1, 1] )
    # print('exp mean data', tf.shape(expand_mean_data))
    var_data = tf.reduce_mean( tf.math.pow(data - expand_mean_data, 2), axis=0)
    # print('var data', tf.shape(var_data))

    # data = [:,:,:]
    hot=get_cm()
    fig, axes = plt.subplots(nrows=2, ncols=2)
    ax = axes[0][0]
    c_img=ax.imshow(data[0,:,:], cmap=hot)
    fig.colorbar(c_img, ax=ax)
    ax.set_title('sample')
    ax = axes[0][1]
    c_img=ax.imshow(mean_data, cmap=hot)
    fig.colorbar(c_img, ax=ax)
    ax.set_title('mean')
    ax = axes[1][0]
    c_img=ax.imshow(std_data, cmap=hot)
    fig.colorbar(c_img, ax=ax)
    ax.set_title('std')
    ax = axes[1][1]
    c_img=ax.imshow(var_data, cmap=hot)
    fig.colorbar(c_img, ax=ax)
    ax.set_title('var')

    plt.savefig(name, dpi=dpi)
    plt.clf()

def plot_one_field(data, x_dim, y_dim, dpi=150, name='solution.png'):
    """
    Plot the histogram of a data

    Args:
        data (numpy array): data[:, :, :]
        x_dim (int): subplots in the x_dim to plot
        y_dim (int): subplots in the y_dim to plot
        dpi (int): dpi of png (=150)
        name (str): name of png output (='solution.png') 
    """

    hot=get_cm()
    fig, axes = plt.subplots(nrows=x_dim, ncols=y_dim)
    for i in range(0, x_dim):
        for j in range(0, y_dim):
            ax = axes[i][j]
            c_img=ax.imshow(data[i+y_dim*j,:,:], cmap=hot)
            fig.colorbar(c_img, ax=ax)
            ax.set_xticks([])
            ax.set_yticks([])
    plt.savefig(name, dpi=dpi)
    plt.clf()

def plot_one_field_hist(data, x_dim, y_dim, dpi=150, name='hist.png'):
    """
    Plot the histogram of a data

    Args:
        data (numpy array): data[:, :, :]
        x_dim (int): subplots in the x_dim to plot
        y_dim (int): subplots in the y_dim to plot
        dpi (int): dpi of png (=150)
        name (str): name of png output (='hist.png') 
    """
    # data = [:,:,:]
    fig, axes = plt.subplots(nrows=x_dim, ncols=y_dim)
    for i in range(0, x_dim):
        for j in range(0, y_dim):
            ax = axes[i][j]
            ax.hist(data[:,i,j], bins='auto')
            ax.set_xticks([])
            ax.set_yticks([])
    plt.savefig(name, dpi=dpi)
    plt.clf()


if __name__ == '__main__':
    tot_img = 7


    # dof = 1
    # mat1 = np.random.rand(2,16,16,dof)
    # mat2 = np.random.rand(2,16,16,dof*2)
    # plot_PDE_solutions(
            # img_input = mat2[0:1, :, :, 0:2*dof], 
            # img_label = mat1[0:1, :, :, 0:1*dof], 
            # img_pre_mean = mat1[0:1, :, :, 0:1*dof], 
            # img_pre_var = mat1[0:1, :, :, 0:1*dof], 
            # img_pre_std = mat1[0:1, :, :, 0:1*dof], 
            # dof=dof, 
            # dof_name=['c'], 
            # tot_img = tot_img,
            # filename='test1.png')

    # dof = 2
    # mat1 = np.random.rand(2,16,16,dof)
    # mat2 = np.random.rand(2,16,16,dof*2)
    # plot_PDE_solutions(
            # img_input = mat2[0:1, :, :, 0:2*dof], 
            # img_label = mat1[0:1, :, :, 0:1*dof], 
            # img_pre_mean = mat1[0:1, :, :, 0:1*dof], 
            # img_pre_var = mat1[0:1, :, :, 0:1*dof], 
            # img_pre_std = mat1[0:1, :, :, 0:1*dof], 
            # dof=dof, 
            # dof_name=['ux', 'uy'], 
            # tot_img = tot_img,
            # filename='test2.png')

    # dof = 3
    # mat1 = np.random.rand(2,16,16,dof)
    # mat2 = np.random.rand(2,16,16,dof*2)
    # plot_PDE_solutions(
            # img_input = mat2[0:1, :, :, 0:2*dof], 
            # img_label = mat1[0:1, :, :, 0:1*dof], 
            # img_pre_mean = mat1[0:1, :, :, 0:1*dof], 
            # img_pre_var = mat1[0:1, :, :, 0:1*dof], 
            # img_pre_std = mat1[0:1, :, :, 0:1*dof], 
            # dof=dof, 
            # dof_name=['ux', 'uy', 'c'], 
            # tot_img = tot_img,
            # filename='test3.png')

    tot_img = 6
    for dof in range(1, 3):
        mat1 = np.random.rand(2,16,16,dof)
        mat2 = np.random.rand(2,16,16,dof*2)
        plot_PDE_solutions(
                img_input = mat2[0:1, :, :, 0:2*dof], 
                img_label = mat1[0:1, :, :, 0:1*dof], 
                img_pre_mean = mat1[0:1, :, :, 0:1*dof], 
                img_pre_var = mat1[0:1, :, :, 0:1*dof], 
                img_pre_std = mat1[0:1, :, :, 0:1*dof], 
                dof=dof, 
                dof_name=['a']*dof, 
                tot_img = tot_img,
                filename='test-'+str(tot_img) + '-' + str(dof) + '.png')


    # for dof in range(1, 3):
        # for tot_img in range(1, 8):
            # mat1 = np.random.rand(2,16,16,dof)
            # names = []
            # fields = []
            # dof_name = ['test'] * dof
            # for i in range(0, tot_img):
                # fields.append(mat1)
                # names.append(str(i))
            # plot_fields(fields, names, dof, dof_name, filename='test-'+str(tot_img) + '-' + str(dof) + '-2.45.png')


====================================================================================================
mechanoChemML\workflows\pde_solver\pde_workflow_steady_state.py
====================================================================================================
from __future__ import absolute_import, division, print_function, unicode_literals

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
#os.environ["CUDA_VISIBLE_DEVICES"]="0"
#os.environ["CUDA_VISIBLE_DEVICES"]="3"



# Helper libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import sys
import datetime
import glob
import time
import pickle
import logging
from natsort import natsorted, ns
import socket
from configparser import ConfigParser, ExtendedInterpolation
import argparse

# # TensorFlow and tf.keras
import tensorflow as tf
tf.keras.backend.set_floatx('float32')
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.python.training import py_checkpoint_reader
import tensorflow_probability as tfp
# tf.logging.set_verbosity(tf.logging.ERROR)

from mechanoChemML.src.nn_models import BNN_user_weak_pde_general
import mechanoChemML.src.pde_layers as pde_layers
from mechanoChemML.workflows.pde_solver.pde_utility import plot_PDE_solutions, plot_fields, split_data, expand_dataset, exe_cmd, BatchData, plot_one_field_hist, plot_one_field_stat, plot_one_field,plot_PDE_solutions_new


class PDEWorkflowSteadyState:
    """
    General Weak PDE constrained workflow.

    Workflow for any specific physical system should inherit from this general workflow.

    """

    def __init__(self):
        self.restart_dir_to_load = ''
        self.now_str = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
        self.today_str = datetime.datetime.now().strftime("%Y-%m-%dT%H-%M")

        self._parse_sys_args()
        if self.args.restartfrom:
            print("Note: old config.ini content is loaded. The specified config.ini will be totally neglected.")
            self._load_saved_states()
            # retract old now string from .pickle file
            self.now_str_old = self.args.restartfrom.split('-')[-1].split('.')[0]
        else:
            # 
            self._read_config_file()

        self._debugger()

        # prepare the folder
        cmd = "mkdir -p results restart "
        exe_cmd(cmd)

        self.features = None #: training inputs with Dirichlet and Neumann BCs: features
        self.labels = None #: training labels (not used during training, for comparison purpose only.): labels 

        if self.config['NN']['NNArchitecture'].find('Flipout') >= 0:
            self.isBNN = True
        else:
            self.isBNN = False

        if self.args.restartfrom:
            # only these two parameters in the old configuration file will be altered during restart.
            self.epochs = self.args.continuerun
            self.InitialEpoch = 0
        else:
            self.epochs = int(self.config['NN']['Epochs'])
            self.InitialEpoch = int(self.config['NN']['InitialEpoch'])

        if self.isBNN:
            self.filename_base = self.today_str + '-' + socket.gethostname() + '-BNN-' + self.args.configfile[:-4]
            self.monte_carlo_num = int(self.config['NN']['MonteCarloNum'])
            self.Sigma1 = float(self.config['NN']['Sigma1'])
            self.Sigma2 = float(self.config['NN']['Sigma2'])
            self.tot_img = 8
        else:
            self.filename_base = self.today_str + '-' + socket.gethostname() + '-NN-' + self.args.configfile[:-4]
            self.monte_carlo_num = 1
            self.Sigma1 = 0.0
            self.Sigma2 = 0.0
            self.tot_img = 6

        self.expand_times = int(self.config['NN']['DataAugTimes'])
        self.batch_size = int(self.config['NN']['BatchSize'])
        self.data_path = self.config['NN']['DataPath']
        self.NNOptimizer = self.config['NN']['Optimizer']
        self.LR0 = float(self.config['NN']['LearningRate'])

        try:
            self.NeumannFirst = int(self.config['NN']['NeumannFirst'])
        except:
            self.NeumannFirst = 0

        try:
            self.FixLoc = int(self.config['NN']['FixLoc'])
        except:
            self.FixLoc = 0

        self.model = None

        try:
            self.data_folder = [x.strip() for x in self.config['NN']['DataFolder'].split()]
        except:
            self.data_folder = ['DNS/']
            print('Default DNS/ is used for DataFolder in NN in config.ini')

        if self.args.restartfrom:
            # make sure that new filename starts the same as old filename. 
            self.filename_base += (
            '-x' + str(self.expand_times) 
            + '-B' + str(self.batch_size) 
            + '-E' + self.config['NN']['Epochs'] 
            + '-I' + self.config['NN']['InitialEpoch'] 
            + '-mc' + str(self.monte_carlo_num) 
            + '-1S' + "{:.1e}".format(self.Sigma1) 
            + '-2S' + "{:.1e}".format(self.Sigma2) 
            + '-' + self.NNOptimizer 
            + '-' + "{:.1e}".format(self.LR0) 
            + '-' + self.data_path.replace('/', '')
            + '-'  + self.now_str_old 
            + '-e' + str(self.args.continuerun)
            )
        else:
            self.filename_base += (
            '-x' + str(self.expand_times) 
            + '-B' + str(self.batch_size) 
            + '-E' + str(self.epochs)            
            + '-I' + str(self.InitialEpoch)            
            + '-mc' + str(self.monte_carlo_num) 
            + '-1S' + "{:.1e}".format(self.Sigma1) 
            + '-2S' + "{:.1e}".format(self.Sigma2) 
            + '-' + self.NNOptimizer 
            + '-' + "{:.1e}".format(self.LR0) 
            + '-' + self.data_path.replace('/', '')
            + '-' + self.now_str 
            )

        self.restart_dir = 'restart/' +  self.filename_base
        self.filename = 'results/' +  self.filename_base

    def _load_saved_states(self):
        """ load saved information from the pickle file during restart """
        saved_config = pickle.load(open(self.args.restartfrom, "rb"))
        self.config = saved_config ['configdata']
        self.restart_dir_to_load = saved_config['savedckpdir']
        print('Content of old config.ini to be used: ', self.config)

    def _read_config_file(self):
        """ read configurations from the config.ini file """
        config = ConfigParser(interpolation=ExtendedInterpolation())
        config.read(self.args.configfile)
        self.config = config

    def _parse_sys_args(self):
        """ parse system args """
        parser = argparse.ArgumentParser(description='Run BNN', prog="'" + (sys.argv[0]) + "'")
        parser.add_argument('configfile', type=str, help='simulation configuration file')
        parser.add_argument('-rf', '--restartfrom', type=str, default='', help='restart from saved .pickle files (with previous predictions)')
        parser.add_argument('-ra', '--restartat', type=int, default=0, help='restart at which epoch')
        parser.add_argument('-init', '--initfrom', type=str, default='', help='initialize from saved .pickle files (overwrite the info given in config.ini)')
        parser.add_argument('-c', '--continuerun', type=int, default=-1,  help='continue run how many epoches from the restart file)')
        try:
            args = parser.parse_args()
            self.args = args
        except:
            parser.print_help()
            exit(0)

    def _debugger(self):
        """ setup the debugger """
        logger = logging.getLogger('root')
        FORMAT = "[%(filename)s:%(lineno)s - %(funcName)20s() ] %(message)s"
        logging.basicConfig(format=FORMAT)
        logger.setLevel(logging.DEBUG)
        self.logger = logger

    def _output_bc_stats(self):

        self.bc_data_seq = BatchData(data=(self.features, self.labels), batch_size=20)
        bc_data_stats = tf.zeros_like(self.features[0:1,:,:,:]).numpy()
        bc_val_min = None
        bc_val_max = None
        for step, (batch_x, batch_y) in enumerate(self.bc_data_seq):
            bc_counts = tf.where(batch_x > 0, tf.ones_like(batch_x, dtype=tf.float32), tf.zeros_like(batch_y, dtype=tf.float32))
            bc_counts = tf.reduce_sum(bc_counts, [0], keepdims=True).numpy()
            bc_data_stats = bc_data_stats + bc_counts
            bc_values = batch_x
            bc_values = np.ma.masked_where(bc_values <= 0, bc_values)
            bc_min = np.amin(bc_values, axis=(1,2))
            bc_max = np.amax(bc_values, axis=(1,2))
            if bc_val_min is None:
                bc_val_min = bc_min
                bc_val_max = bc_max
            else:
                bc_val_min = np.concatenate((bc_val_min, bc_min), axis=0)
                bc_val_max = np.concatenate((bc_val_max, bc_max), axis=0)
            print(step, np.shape(bc_data_stats), 'bc_min=', np.shape(bc_val_min), 'bc_max=', np.shape(bc_val_max))

        bc_data_info = {
                'count':bc_data_stats,
                'min': bc_val_min,
                'max': bc_val_max,
                }
        pickle_out = open('bc_data_' + str(np.shape(bc_val_min)[0]) + '.pickle', "wb")
        pickle.dump(bc_data_info, pickle_out)
        pickle_out.close()

        exit(0)

    def _output_bc_stats_good_bad(self):

        self.bc_data_seq = BatchData(data=(self.features, self.labels), batch_size=1)
        bc_data_stats = tf.zeros_like(self.features[0:1,:,:,:]).numpy()
        bc_val_min = None
        bc_val_max = None
        for step, (batch_x, batch_y) in enumerate(self.bc_data_seq):

# top 20 Neumann:  [248, 95, 187, 273, 19, 242, 101, 89, 166, 93, 110, 6, 310, 82, 302, 224, 260, 190, 207, 209]
# bad 20 Neumann:  [138, 38, 61, 55, 186, 119, 47, 26, 281, 90, 249, 42, 54, 279, 56, 276, 319, 30, 259, 257]
# top 20 no Neumann:  [105, 244, 191, 261, 35, 8, 99, 115, 136, 255, 18, 22, 243, 171, 32, 12, 0, 270, 23, 193]
# bad 20 no Neumann:  [152, 295, 307, 206, 308, 60, 298, 204, 218, 256, 219, 141, 240, 226, 174, 250, 289, 262, 285, 316]

            # if step in [248, 95, 187, 273, 19, 242, 101, 89, 166, 93, 110, 6, 310, 82, 302, 224, 260, 190, 207, 209]:
            # if step in [138, 38, 61, 55, 186, 119, 47, 26, 281, 90, 249, 42, 54, 279, 56, 276, 319, 30, 259, 257]:
            # if step in [105, 244, 191, 261, 35, 8, 99, 115, 136, 255, 18, 22, 243, 171, 32, 12, 0, 270, 23, 193]:
            if step in [152, 295, 307, 206, 308, 60, 298, 204, 218, 256, 219, 141, 240, 226, 174, 250, 289, 262, 285, 316]:
                bc_counts = tf.where(batch_x > 0, tf.ones_like(batch_x, dtype=tf.float32), tf.zeros_like(batch_y, dtype=tf.float32))
                bc_counts = tf.reduce_sum(bc_counts, [0], keepdims=True).numpy()
                bc_data_stats = bc_data_stats + bc_counts
                bc_values = batch_x
                bc_values = np.ma.masked_where(bc_values <= 0, bc_values)
                bc_min = np.amin(bc_values, axis=(1,2))
                bc_max = np.amax(bc_values, axis=(1,2))
                if bc_val_min is None:
                    bc_val_min = bc_min
                    bc_val_max = bc_max
                else:
                    bc_val_min = np.concatenate((bc_val_min, bc_min), axis=0)
                    bc_val_max = np.concatenate((bc_val_max, bc_max), axis=0)
                print(step, np.shape(bc_data_stats), 'bc_min=', np.shape(bc_val_min), 'bc_max=', np.shape(bc_val_max))

        bc_data_info = {
                'count':bc_data_stats,
                'min': bc_val_min,
                'max': bc_val_max,
                }
        pickle_out = open('bc_data_' + str(np.shape(bc_val_min)[0]) + '.pickle', "wb")
        pickle.dump(bc_data_info, pickle_out)
        pickle_out.close()

        exit(0)



    def _load_data(self, only_neumann_data=False, test_folder=''):
        """ 
        load data 

        Args:
            only_neumann_data (bool): only load the BVP setup with Neumann BCs. Use this flag when train the NN with Neumann BCs first.
            test_folder (str): the default location of data is in 'DNS'. If test_folder is specified, the data in this folder will be loaded for testing purpose.
        """

        # waiting for gpu resources without killing the program
        # while (True):
            # try:
                # tf.math.ceil(0.1)
                # cmd = "echo 'gpu resource is allowed' > " + self.filename+'-gpu.log'
                # exe_cmd(cmd)
                # break
            # except:
                # cmd = "echo 'gpu resource is not available, waiting...' > " + self.filename+'-gpu.log'
                # exe_cmd(cmd)
                # time.sleep(1)
                # pass

        self.features = None
        self.labels = None

        if test_folder == '':
            only_testing = False
        else:
            only_testing = True

        if only_testing:
            data_folder_list = [test_folder]
            # data_folder = self.data_path + '/' + test_folder
        else:
            data_folder_list = self.data_folder
            # data_folder = self.data_path + '/' + self.data_folder + '/'

        for one_folder in data_folder_list:
            data_folder = self.data_path + '/' + one_folder + '/'

            file_list = glob.glob(data_folder + '/np-features*.npy')
            file_list = natsorted(file_list, alg=ns.IGNORECASE)
            # print (file_list)

            count = 0
            for f1 in file_list:
                print('file: ', count, f1)
                count += 1
                one_feature = np.load(f1)
                label_path = f1.replace('features', 'labels')
                one_label = np.load(label_path)
                print('file:', f1, 'label:', np.shape(one_label), 'feature:', np.shape(one_feature))
                if (self.features is None):
                    self.features = np.copy(one_feature)
                    self.labels = np.copy(one_label)
                else:
                    self.features = np.concatenate((self.features, one_feature), axis=0)
                    self.labels = np.concatenate((self.labels, one_label), axis=0)

            if (not only_testing) and only_neumann_data:
                raise ValueError("only neumann data option is disabled")
                # selected_index = []
                # for i0 in range(0, np.shape(self.features)[0]):
                    # # print('i0=', i0, np.any(np.greater(self.features[i0,:,:,2:4], 0)))
                    # # the following should still be compatible with the extra Neumann channel
                    # if np.any(np.greater(self.features[i0,:,:,self.dof:2*self.dof], 0)): # with Neumann BCs 
                        # selected_index.append(i0)
                # selected_index = np.array(selected_index)
                # # print(np.shape(selected_index))
                # # print(np.shape(self.features[selected_index]))
                # self.features = self.features[selected_index]
                # self.labels = self.labels[selected_index]
        print('len of self.features: ', np.shape(self.features))
        self.dh = 1.0 / (np.shape(self.features)[2] - 1.0)

        # the_feature = pde_layers.LayerFillZeros()(self.features)
        # the_feature = pde_layers.LayerFillRandomNumber()(self.features)
        # for i in range(0, 1000, 50):
            # plot_fields(
                    # list_of_field = [
                        # the_feature[i:i+1, :, :, 0:1], 
                        # the_feature[i:i+1, :, :, 1:2], 
                        # the_feature[i:i+1, :, :, 2:3], 
                        # ],
                    # list_of_field_name = [
                        # 'Dirichlet', 
                        # 'Neumann x', 
                        # 'Neumann y', 
                        # ], 
                    # dof = 1, 
                    # dof_name = ['c'],
                    # filename = 'results/' + self.problem_name + '-' + str(i) + '-Sol.png')

        # R_red, y_pred, y_true_dummy, _, _, _ = self._compute_residual(the_feature, self.labels)
        # print(np.shape(R_red), np.shape(y_pred))
        # for i in range(0, 1000, 50):
            # print(i, R_red[i,20:30,20:30,0])
            # # print(i, self.labels[i,20:30,20:30,0])

        # BC perturbation
        # the_feature = pde_layers.LayerFillRandomToBCs(stddev=0.05)(the_feature)
        # the_feature = the_feature.numpy()

        # self._output_bc_stats()
        # self._output_bc_stats_good_bad()
        self.features = self.features.astype(np.single)    
        self.labels = self.labels.astype(np.single)    
        print(self.features.dtype)

        if only_testing:
            self.test_dataset = self.features
            self.test_label = self.labels
            # for scaling test
            # the_feature, the_label = expand_dataset(self.features, self.labels, times=12)
            # self.test_seq = BatchData(data=(the_feature, the_label), batch_size=4096)

            self.test_seq = BatchData(data=(self.test_dataset, self.test_label), batch_size=1)
            self.train_seq = self.test_seq
        else:
            the_feature, the_label = expand_dataset(self.features, self.labels, times=self.expand_times)
            self.train_dataset, self.train_label, self.val_dataset, self.val_label, self.test_dataset, self.test_label = split_data(the_feature, the_label, self.batch_size, split_ratio=['0.8', '0.1', '0.1'])
            # self.train_dataset, self.train_label, self.val_dataset, self.val_label, self.test_dataset, self.test_label = split_data(the_feature, the_label, split_ratio=['0.1', '0.1', '0.8'])
            self.train_seq = BatchData(data=(self.train_dataset, self.train_label), batch_size=self.batch_size)
            self.val_seq   = BatchData(data=(self.val_dataset,   self.val_label),   batch_size=self.batch_size)
            self.test_seq  = BatchData(data=(self.test_dataset,  self.test_label),  batch_size=self.batch_size)
            print('len of features: ', np.shape(the_feature), 
                  'len of training data: ', np.shape(self.train_dataset), 
                  'len of test data: ', np.shape(self.test_dataset), 
                  'batch size: ', self.batch_size, 
                  'total train batches: ', len(self.train_seq),
                  'total val batches: ', len(self.val_seq),
                  'total test batches: ', len(self.test_seq),
                  )

    def _bulk_residual(self):
        """
        Dummy _bulk_residual function. The actual residual should be implemented in each physical problem. 

        todo:
            - Please implement the residual in each related physical systems (PDEs).

        error:
            - Not implemented.
        """
        raise ValueError('Residual is not implemented! Please implement it in the specific problem!')


    def _compute_residual(self, features, y_pred, only_y_pred=False):
        """
        Compute different residuals, and apply the Dirichlet BCs to the NN predicted solutions.

        args:
            features (tensor): size of [None, :, :, 2*dof]
            y_pred (tensor): size of [None, :, :, dof]

        return:
            - different residuals and the y_pred with applied Dirichlet BCs.
        """

        # mask contains the region not on the Dirichlet boundary
        bc_mask_dirichlet = pde_layers.ComputeBoundaryMaskNodalData(features, dof=self.dof, opt=1)
        reverse_bc_mask_dirichlet = tf.where( bc_mask_dirichlet == 0, tf.fill(tf.shape(bc_mask_dirichlet), 1.0), tf.fill(tf.shape(bc_mask_dirichlet), 0.0))

        # apply the Dirichlet BCs to y_pred
        input_dirichlet = features[:,:,:,0:self.dof]
        dirichlet_bc = tf.multiply(input_dirichlet, reverse_bc_mask_dirichlet)
        # print(y_pred.dtype, bc_mask_dirichlet.dtype, input_dirichlet.dtype)
        y_pred = tf.multiply(y_pred, bc_mask_dirichlet)
        y_pred = y_pred + dirichlet_bc

        if only_y_pred:
            return y_pred

        bc_mask_neumann = pde_layers.ComputeBoundaryMaskNodalData(features, dof=self.dof, opt=2)
        reverse_bc_mask_neumann = tf.where( bc_mask_neumann == 0, tf.fill(tf.shape(bc_mask_neumann), 1.0), tf.fill(tf.shape(bc_mask_neumann), 0.0))

        if self.UseTwoNeumannChannel :
            neumann_residual = pde_layers.ComputeNeumannBoundaryResidualNodalDataNew(features, dh=self.dh, dof=self.dof)
        else:
            neumann_residual = pde_layers.ComputeNeumannBoundaryResidualNodalData(features, dh=self.dh, dof=self.dof)

        y_true_dummy = pde_layers.LayerFillRandomNumber()(input_dirichlet)
        elem_bulk_residual=self._bulk_residual(y_pred)
        elem_residual_mask = pde_layers.GetElementResidualMask(y_true_dummy)
        R = pde_layers.GetNodalInfoFromElementInfo(elem_bulk_residual, elem_residual_mask, dof=self.dof)
        R_fix = tf.where(dirichlet_bc==0.5, R, 0.0)

        # get only neumann part
        R_neumann = tf.multiply(R, reverse_bc_mask_neumann) 
        # neumann BCs
        dR_neumann = R_neumann - neumann_residual 

        # remove boundary part
        R_no_dirichlet = tf.multiply(R, bc_mask_dirichlet) 
        R_no_dirichlet_no_neumann = tf.multiply(R_no_dirichlet, bc_mask_neumann) 
        R_body = R_no_dirichlet_no_neumann

        # actual residual without the essential BCs
        R_red = R_no_dirichlet - neumann_residual
        return R_red, y_pred, y_true_dummy, R_body, dR_neumann, R_fix

    def _loss_probabilistic(self):
        """
        General probabilistic loss functions. The _compute_residual() has to be specified in each problem.
        """
        def loss(y_true, y_pred):

            if self.UseTwoNeumannChannel :
                inputs = y_pred[:,:,:,self.dof:4*self.dof] # new Neumann Channel
            else:
                inputs = y_pred[:,:,:,self.dof:3*self.dof] # old Neumann Channel
            y_pred = y_pred[:,:,:,0:self.dof]
            dist = tfp.distributions.Normal(loc=tf.zeros_like(y_pred), scale=self.Sigma1)
            y_noise = tf.squeeze(dist.sample(1), [0]) # only sample 1, thus, lead dimension can be squeezed. 
            y_pred = y_pred + y_noise
            R_red, y_pred, y_true_dummy, _, _, _ = self._compute_residual(inputs, y_pred)
            dist = tfp.distributions.Normal(loc=tf.zeros_like(y_true_dummy), scale=self.model.Sigma2)
            return self.BetaMSELoss * tf.reduce_mean(tf.square(tf.where(y_true_dummy > -0.9, tf.random.normal(tf.shape(y_true_dummy), 0.5, 0.05, tf.float32, seed=1), tf.zeros_like(y_true_dummy)) - tf.where(y_true_dummy > -0.9, y_pred, tf.zeros_like(y_pred)))) + self.BetaPDELoss * tf.keras.backend.sum(tf.reduce_mean(-dist.log_prob(R_red), 0))
        return loss

    def _loss_deterministic(self):
        """
        General deterministic loss functions. The _compute_residual() has to be specified in each problem.
        """
        def loss(y_true, y_pred):
            if self.UseTwoNeumannChannel :
                inputs = y_pred[:,:,:,self.dof:4*self.dof] # new Neumann Channel
            else:
                inputs = y_pred[:,:,:,self.dof:3*self.dof] # old Neumann Channel
            y_pred = y_pred[:,:,:,0:self.dof]
            R_red, y_pred, y_true_dummy, _, _, _ = self._compute_residual(inputs, y_pred)
            return self.BetaMSELoss * tf.reduce_mean(tf.square(tf.where(y_true_dummy > -0.9, tf.random.normal(tf.shape(y_true_dummy), 0.5, 0.05, tf.float32, seed=1), tf.zeros_like(y_true_dummy)) - tf.where(y_true_dummy > -0.9, y_pred, tf.zeros_like(y_pred)))) + self.BetaPDELoss * tf.reduce_mean(tf.reduce_sum(tf.square(R_red), axis=[1,2,3]))
        return loss

    def _build_loss(self):
        """ 
        Build the loss for weak-PDE constrained NN
        """
        self.BetaMSELoss = tf.Variable(1.0, trainable=False, dtype=tf.float32)
        self.BetaPDELoss = tf.Variable(1.0, trainable=False, dtype=tf.float32)
        if self.isBNN:
            loss = self._loss_probabilistic()
        else: 
            loss = self._loss_deterministic(),
        return loss
    
    def _build_optimizer(self):
        """ 
        Build the optimizer for weak-PDE constrained NN
        """
        # not using the decay learning rate function.
        # global_step = tf.Variable(0, name='global_step', trainable=False)
        # LearningRate = tf.compat.v1.train.exponential_decay(
            # learning_rate=self.LR0,
            # global_step=global_step,
            # decay_steps=100,
            # decay_rate=0.8,
            # staircase=True)

        if self.NNOptimizer.lower() == 'RMSprop'.lower() :
            optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.LR0)
        elif self.NNOptimizer.lower() == 'Adadelta'.lower() :
            optimizer = tf.keras.optimizers.Adadelta(learning_rate=self.LR0)
        elif self.NNOptimizer.lower() == 'Adam'.lower() :
            optimizer = tf.keras.optimizers.Adam(learning_rate=self.LR0)
        elif self.NNOptimizer.lower() == 'Nadam'.lower() :
            optimizer = tf.keras.optimizers.Nadam(learning_rate=self.LR0)
        elif self.NNOptimizer.lower() == 'SGD'.lower() : # not very well
            optimizer = tf.keras.optimizers.SGD(learning_rate=self.LR0, momentum=0.9) 
        else:
            raise ValueError('Unknown optimizer option:', self.NNOptimizer)
        return optimizer


    def _check_weights(self):
        """
        Print the weights of layers. For debug purpose, not using anywhere.
        """
        for one_layer in self.model.layers:
            print ('layer = ',one_layer)
            print ('weights =', one_layer.get_weights())
            print ('weights shape = ', np.shape(one_layer.get_weights()))

    def _load_saved_cnn_model(self):
        """
        Use saved optimized CNN parameters to initialize the mean of BNN parameters.
        """
        try:
            self.BestCNNWeight = self.config['NN']['SaveCNNModel']
        except:
            self.BestCNNWeight = ''

        if self.args.initfrom:
            self.BestCNNWeight = self.args.initfrom
        if not self.BestCNNWeight:
            return 0 

        print('----- Loading saved NN optimized model parameter to initialize BNN----------')
        if self.BestCNNWeight.find('.pickle') >= 0:
            # It is encouraged to use pickle file to auto find the check point info and load best CNN weight. 
            saved_config = pickle.load(open(self.BestCNNWeight, "rb"))
            print('best cnn weight (pickle): ', saved_config['savedckpdir'])
            best_cnn_weights = tf.train.latest_checkpoint(saved_config['savedckpdir'])
            print('best_cnn_weights: ', best_cnn_weights)
        else:
            # Manually provide the best CNN weight checkpoint info. 
            # It is discouraged to use this approach. 
            print('best cnn weight (others): ', self.BestCNNWeight)
            print('auto restart dir to load: ', self.restart_dir)
            print('check: ', '/'.join(self.restart_dir.split('/')[:-1]) + '/' + self.BestCNNWeight)
            best_cnn_weights = tf.train.latest_checkpoint('/'.join(self.restart_dir.split('/')[:-1]) + '/' + self.BestCNNWeight)
            print('best_cnn_weights: ', best_cnn_weights)

        reader = py_checkpoint_reader.NewCheckpointReader(best_cnn_weights)
        var_to_shape_map = reader.get_variable_to_shape_map()
        var_to_dtype_map = reader.get_variable_to_dtype_map()
        #print(var_to_shape_map)
        #print(var_to_dtype_map)

        saved_kernel = []
        saved_bias = []
        for key, value in natsorted(var_to_shape_map.items()):
            # print(key, value)
            if key.find('all_layers') >= 0 and key.find('OPTIMIZER') < 0 :
                # print(key, value)
                val0 = None
                if key.find('kernel') >= 0:
                    val0 = reader.get_tensor(key)
                    saved_kernel.append(val0)
                if key.find('bias') >= 0:
                    val0 = reader.get_tensor(key)
                    saved_bias.append(val0)

        # print('all trainable variable:',self.model.trainable_variables )
        # print('total(all trainable variable):',len(self.model.trainable_variables))
        kernel_ind = 0 
        bias_ind = 0
        # to_untrainable = []
        v_ind = 0
        for v0 in self.model.trainable_variables :
            if v0.name.find('kernel_posterior_loc') >= 0 :
                v0_value = v0.value().numpy()
                # print('v0= ', type(v0), v0.name, np.shape(v0_value))
                v0.assign(saved_kernel[kernel_ind])
                kernel_ind += 1
                # to_untrainable.append(v_ind)
                #v0.assign(v0_value)
            if v0.name.find('bias_posterior_loc') >= 0 :
                v0_value = v0.value().numpy()
                # print('v0= ', type(v0), v0.name, np.shape(v0_value))
                v0.assign(saved_bias[bias_ind])
                bias_ind += 1
                #v0.assign(v0_value)
            if v0.name.find('untransformed_scale') >= 0 :
                # v0.assign(v0.value().numpy()*0.001)
                v0.assign(v0.value().numpy()*2.0)
                # v0.assign(v0.value().numpy()) # still bad
                # v0.assign(v0.value().numpy()*0.5) # very bad
                # v0.assign(v0.value().numpy()*0.25) # very bad, the distribution is wide, and loss are huge
                # print('v0= ', type(v0), v0.name, np.shape(v0_value))
            v_ind += 1
        if len(saved_kernel) != kernel_ind:
            print('saved kernel: ', saved_kernel, 'kernel_ind: ', kernel_ind)
            raise ValueError("WARNING: loaded cnn saved kernel numbers != bnn kernel numbers, might load wrong model")
        if len(saved_bias) != bias_ind:
            print('saved bias: ', saved_bias, 'bias_ind: ', bias_ind)
            raise ValueError("WARNING: loaded cnn saved bias numbers != bnn bias numbers, might load wrong model")

        # #gets a reference to the list containing the trainable variables
        # print('trainable variable: ', len(self.model.trainable_variables))
        # # -----following not working-----
        # trainable_collection = tf.compat.v1.get_collection_ref(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES)
        # print(trainable_collection)
        # variables_to_remove = list()
        # for vari in trainable_collection:
            # #uses the attribute 'name' of the variable
            # if vari.name=="batch_normalization/gamma:0" or vari.name=="batch_normalization/beta:0":
                # variables_to_remove.append(vari)
        # for rem in variables_to_remove:
            # trainable_collection.remove(rem)
        # -----following not working-----
        
        # #It is very difficult to make mean to untrainable, and we should not do so.
        # print('type of trainable_variables: ', type(self.model.trainable_variables))
        # to_untrainable = [x for x in to_untrainable[::-1]]
        # print('to_untrainable: ', to_untrainable)
        # for v0 in to_untrainable:
            # del self.model.trainable_variables[v0]

            # # print('new v0: ', v0)
        # # print('all trainable weights:',self.model.trainable_weights )
        # exit(0)
        # print('get layers: ', self.model.get_layer(index=1))
        # print('layers kernel: ', self.model.get_layer(index=1).trainable_weights)
        # # print('layers bias: ', self.model.get_layer(index=1).bias)
        # print('layer trainable_variables: ', self.model.get_layer(index=1).trainable_variables)
        # print('all trainable variable:',self.model.trainable_variables )
        # print('all trainable weights:',self.model.trainable_weights )
        # print("Successfully load weight: ", latest)
        # print('layers: ', self.model.layers)
        # print('get layers: ', self.model.get_layer(index=0))
        # self.model.summary()
        return 1

    def _build_model(self):
        """ 
        Build the weak-PDE constrained NN model.
        """

        # if tf.config.list_physical_devices('GPU'):
          # # physical_devices = tf.config.list_physical_devices('GPU')
          # # print(physical_devices)
          # # strategy = tf.distribute.MirroredStrategy(devices=["/gpu:0", "/gpu:1"])
          # self.mirrored_strategy = tf.distribute.MirroredStrategy()
        # else:  # use default strategy
          # self.mirrored_strategy = tf.distribute.get_strategy() 

        # with self.mirrored_strategy.scope():
            # print(tf.Variable(1.))
            # self.model = BNN_user_weak_pde_general(
                    # layers_str=self.config['NN']['NNArchitecture'],
                    # NUM_TRAIN_EXAMPLES=len(self.train_seq), # total batch numbers
                    # Sigma2=self.Sigma2)
            # print(self.model)
            # self.optimizer = self._build_optimizer()

        self.model = BNN_user_weak_pde_general(
                layers_str=self.config['NN']['NNArchitecture'],
                NUM_TRAIN_EXAMPLES=len(self.train_seq), # total batch numbers
                Sigma2=self.Sigma2)
        self.optimizer = self._build_optimizer()

        self.model.compile(
            loss = self._build_loss(),
            optimizer = self.optimizer,
            experimental_run_tf_function = False,    # allow the kl-call in the layer structure
        )

    # all the data need to be converted to dataset, requires significant change of the code structure. 
    # @tf.function
    # def distributed_train_step(dist_inputs):
      # per_replica_losses = self.mirrored_strategy.run(train_step, args=(dist_inputs,))
      # return self.mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,axis=None)


    def _train(self):
        """
        Use batch-optimization to train the model.
        """

        if self.epochs > 0:
            cmd = "mkdir -p " + self.restart_dir
            exe_cmd(cmd)

        self.model_train_loss = []
        self.model_val_loss = []
        model_loss = 1000

        checkpoint_path = self.restart_dir + "/ckpt"    # it's difficult to include time info, as we need to restart simulation

        restart_at = 0
        if self.args.restartfrom:
            print('checkpoint_dir to load restart: ', self.restart_dir_to_load)
            latest = tf.train.latest_checkpoint(self.restart_dir_to_load)

            # load specific ckpt.
            if self.args.restartat > 0:
                latest = self.restart_dir_to_load + '/ckpt' + str(self.args.restartat).zfill(4)
            print("latest checkpoint: ", latest)
            
            # check if ckpt exist
            if (latest != None):
                self.filename += '-ra-' + latest.split('/')[-1]
                self.model.load_weights(latest)
                print("Successfully load weight: ", latest)
                # return 1
            else:
                print("No saved weights, start to train the model from the beginning!")
                pass

        self.losses = {'loss': [], 'val_loss': [], 'mse_loss':[], 'res_body':[], 'res_neu':[]}
        self.var_sigma2 = []

        # print model information
        input_shape=(None, np.shape(self.features)[1], np.shape(self.features)[2], np.shape(self.features)[3])
        self.model.build(input_shape) # `input_shape` is the shape of the input data
        self.model.summary()

        # load optimized CNN parameters to BNN if specified.
        print('self.args.restartfrom:', self.args.restartfrom)
        if self.args.restartfrom == '' :
            if (self._load_saved_cnn_model()):
                self.InitialEpoch = 0

        if self.FixLoc == 1:
            customized_trainer = True
        else:
            customized_trainer = False
        print("use customized_trainer:", customized_trainer)
        if customized_trainer:
            tvars = self.model.trainable_variables
            none_loc_vars = [var for var in tvars if '_loc' not in var.name]
            none_scale_vars = [var for var in tvars if '_scale' not in var.name]
            print(len(tvars), len(none_loc_vars), len(none_scale_vars))

        def my_loss(y_true, y_pred):

            if self.UseTwoNeumannChannel :
                inputs = y_pred[:,:,:,self.dof:4*self.dof] # new Neumann Channel
            else:
                inputs = y_pred[:,:,:,self.dof:3*self.dof] # old Neumann Channel
            y_pred = y_pred[:,:,:,0:self.dof]
            dist = tfp.distributions.Normal(loc=tf.zeros_like(y_pred), scale=self.Sigma1)
            y_noise = tf.squeeze(dist.sample(1), [0]) # only sample 1, thus, lead dimension can be squeezed. 
            y_pred = y_pred + y_noise
            R_red, y_pred, y_true_dummy, _, _, _ = self._compute_residual(inputs, y_pred)
            dist = tfp.distributions.Normal(loc=tf.zeros_like(y_true_dummy), scale=self.model.Sigma2)
            return self.BetaMSELoss * tf.reduce_mean(tf.square(tf.where(y_true_dummy > -0.9, tf.random.normal(tf.shape(y_true_dummy), 0.5, 0.05, tf.float32, seed=1), tf.zeros_like(y_true_dummy)) - tf.where(y_true_dummy > -0.9, y_pred, tf.zeros_like(y_pred)))) + self.BetaPDELoss * tf.keras.backend.sum(tf.reduce_mean(-dist.log_prob(R_red), 0))

        time_elapsed_list = []
        for epoch in range(self.epochs):
            start_time = time.time()
            # The first half of training only train data with Neumann BCs.
            # The second half train all the data.
            if self.NeumannFirst:
                if epoch == 0:
                    self._load_data(only_neumann_data=True)
                elif epoch == int(self.epochs * 0.5):
                    self._load_data(only_neumann_data=False)

            # Coefficients to enable the epoch initialization.
            if epoch < self.InitialEpoch:
                self.BetaMSELoss.assign(float(1.0))
                self.BetaPDELoss.assign(float(0.0))
            else:
                self.BetaMSELoss.assign(float(0.0))
                self.BetaPDELoss.assign(float(1.0))

            # if (epoch+1)%50 == 0:
            # print('epoch:', epoch)
            epoch_loss = []
            for step, (batch_x, batch_y) in enumerate(self.train_seq):
                if customized_trainer:
                    with tf.GradientTape() as t:
                        current_loss = my_loss(batch_y, self.model(batch_x))
                    # grads = t.gradient(current_loss, tvars)
                    # self.optimizer.apply_gradients(zip(grads,tvars))
                    grads = t.gradient(current_loss, none_loc_vars)
                    self.optimizer.apply_gradients(zip(grads,none_loc_vars))
                    # print(current_loss)
                    # print("...training...", current_loss)
                    # exit(0)
                    epoch_loss.append(current_loss)
                else:
                    batch_loss = self.model.train_on_batch(batch_x, batch_y)
                    epoch_loss.append(batch_loss)
                    # print("...training...", batch_loss)


            epoch_val_loss = []
            epoch_mse_loss = []
            epoch_res_body = []
            epoch_res_neu = []
            if self.val_seq is not None:
                for step, (batch_x, batch_y) in enumerate(self.val_seq):
                    val_loss = self.model.test_on_batch(batch_x, batch_y)
                    epoch_val_loss.append(val_loss)

                    #-------------------------- DEBUG Loss--------------------------
                    y_pred = self.model.predict_on_batch(batch_x) 
                    y_true = batch_y

                    if self.UseTwoNeumannChannel :
                        inputs = y_pred[:,:,:,self.dof:4*self.dof] # New Neumann Channel
                    else:
                        inputs = y_pred[:,:,:,self.dof:3*self.dof] # Old Neumann Channel
                    y_pred = y_pred[:,:,:,0:self.dof]
                    R_red, y_pred, y_true_dummy, R_body, dR_neumann, _ = self._compute_residual(inputs, y_pred)

                    mse_loss = 0.0
                    res_body = 0.0
                    res_neu = 0.0

                    mse_loss = self.BetaMSELoss * (tf.reduce_mean(tf.square(tf.where(y_true_dummy > -0.9, 0.5 * tf.ones_like(y_true_dummy, dtype=tf.float32), tf.zeros_like(y_true_dummy, dtype=tf.float32)) - tf.where(y_true_dummy > -0.9, y_pred, tf.zeros_like(y_pred, dtype=tf.float32)))) )
                    res_body = self.BetaPDELoss * tf.reduce_mean(tf.reduce_sum(tf.square(R_body), axis=[1,2,3]))
                    res_neu = self.BetaPDELoss * tf.reduce_mean(tf.reduce_sum(tf.square(dR_neumann), axis=[1,2,3]))

                    epoch_mse_loss.append(mse_loss)
                    epoch_res_body.append(res_body)
                    epoch_res_neu.append(res_neu)
                    #------------------------------------------------- end of loss debug-----------------

            time_elapsed = time.time() - start_time
            time_elapsed_list.append(time_elapsed)
            print("time_elapsed = %s " % time_elapsed)

            self.losses['loss'].append(tf.reduce_mean(epoch_loss))
            self.losses['val_loss'].append(tf.reduce_mean(epoch_val_loss))

            #----------------------- DEBUG -----------------------
            epoch_mse_loss = tf.reduce_mean(epoch_mse_loss)
            epoch_res_body = tf.reduce_mean(epoch_res_body)
            epoch_res_neu = tf.reduce_mean(epoch_res_neu)
            self.losses['mse_loss'].append(epoch_mse_loss.numpy())
            self.losses['res_body'].append(epoch_res_body.numpy())
            self.losses['res_neu'].append(epoch_res_neu.numpy())
            #----------------------- DEBUG -----------------------

            # if epoch % 50 == 0:
            if epoch % 1 == 0:
                # print('Epoch: {}, loss: {:.4e}, val_loss: {:.4e}'.format(epoch, self.losses['loss'][epoch], self.losses['val_loss'][epoch]))
                print('Epoch: {}, loss: {:.4e}, val_loss: {:.4e}'.format(epoch, self.losses['loss'][epoch], self.losses['val_loss'][epoch]),
                        'mse: {:.4e}'.format(epoch_mse_loss.numpy()), 
                        'res_body: {:.4e}'.format(epoch_res_body.numpy()), 
                        'res_neu: {:.4e}'.format(epoch_res_neu.numpy()), 
                        'var(Sigma2): {:.4e}'.format(tf.math.pow(self.model.Sigma2.numpy(),2)),
                        'std(Sigma2): {:.4e}'.format(self.model.Sigma2.numpy()),
                        )
            self.var_sigma2.append(tf.math.pow(self.model.Sigma2.numpy(),2))
            # exit(0)

            # save check points every 10 epoches
            if epoch % 100 == 0:
                self.model.save_weights(checkpoint_path + str(epoch).zfill(4), save_format='tf')


            # save as pickle every 100 epoches
            if epoch % 100 == 0 or epoch == 1:
                self.simulation_results = {
                        'configdata':self.config,
                        'restartedfrom': self.restart_dir_to_load,
                        'savedckpdir':self.restart_dir,
                        'losses':self.losses,
                        'var_sigma2': self.var_sigma2,
                        }
                pickle_out = open(self.filename + '.pickle', "wb")
                pickle.dump(self.simulation_results, pickle_out)
                pickle_out.close()
                print('save to: ', self.filename + '.pickle')
        print("BatchSize: {}, Averaged time per epoch: {:.8f} s".format(self.batch_size, np.mean(np.array(time_elapsed_list[1:]))))

        # save the last epoch
        if self.epochs > 0:
            self.model.save_weights(checkpoint_path + str(epoch).zfill(4), save_format='tf')

            plt.semilogy(self.losses['loss'], 'b')
            plt.semilogy(self.losses['val_loss'], 'r')
            plt.legend(['loss', 'val_loss'])
            plt.xlabel('epoch')
            plt.savefig(self.filename+'-loss.png')
            print('save to:', self.filename+'-loss.png')

            if self.isBNN:
                plt.clf()
                plt.semilogy(self.var_sigma2, 'b')
                plt.legend(['var(sigma2)'])
                plt.xlabel('epoch')
                plt.savefig(self.filename+'-sigma2.png')
                print('save to:', self.filename+'-sigma2.png')

    def debug_problem(self, use_label=False):
        """ for debugging purpose """
        self._load_data()

        # show the plots how many times:
        features = self.features
        labels = self.labels

        # features = tf.convert_to_tensor(features, dtype=tf.float32)
        # labels = tf.convert_to_tensor(labels, dtype=tf.float32)
        # print( pde_layers.LayerFillRandomToBCs(stddev=0.005)(features) )

        try: 
            prediction = self.model.predict(self.features)
            # features = y_pred[:,:,:,self.dof:3*self.dof]
            y_pred = prediction[:,:,:,0:self.dof] 
        except:
            prediction = pde_layers.LayerFillRandomNumber()(features)
            y_pred = prediction[:,:,:,0:self.dof] 

        bc_mask_dirichlet = pde_layers.ComputeBoundaryMaskNodalData(features, dof=self.dof, opt=1)
        bc_mask_neumann = pde_layers.ComputeBoundaryMaskNodalData(features, dof=self.dof, opt=2)
        if self.UseTwoNeumannChannel :
            neumann_residual = pde_layers.ComputeNeumannBoundaryResidualNodalDataNew(features, dh=self.dh, dof=self.dof)
        else:
            neumann_residual = pde_layers.ComputeNeumannBoundaryResidualNodalData(features, dh=self.dh, dof=self.dof)

        if use_label:
            y_pred = labels

        print('features',tf.shape(features))
        print('labels',tf.shape(labels))
        print('bc_mask_dirichlet:', tf.shape(bc_mask_dirichlet))

        print('bc_mask_neumann:', tf.shape(bc_mask_neumann))

        reverse_bc_mask_dirichlet = tf.where( bc_mask_dirichlet == 0, tf.fill(tf.shape(bc_mask_dirichlet), 1.0), tf.fill(tf.shape(bc_mask_dirichlet), 0.0))
        print('reverse_bc_mask_dirichlet :', tf.shape(reverse_bc_mask_dirichlet ))

        reverse_bc_mask_neumann = tf.where( bc_mask_neumann == 0, tf.fill(tf.shape(bc_mask_neumann), 1.0), tf.fill(tf.shape(bc_mask_neumann), 0.0))
        print('reverse_bc_mask_neumann :', tf.shape(reverse_bc_mask_neumann ))

        mask_labels = tf.multiply(labels, reverse_bc_mask_dirichlet) 
        print('mask_labels :', tf.shape(mask_labels ))
        print('neumann_residual :', tf.shape(neumann_residual ), tf.reduce_sum(tf.square( neumann_residual )))
        
        print('y_pred',tf.shape(y_pred))

        input_dirichlet = labels
        dirichlet_bc = tf.multiply(input_dirichlet, reverse_bc_mask_dirichlet)
        y_pred = tf.multiply(y_pred, bc_mask_dirichlet)
        y_pred = y_pred + dirichlet_bc
        print('dirichlet_bc', tf.shape(dirichlet_bc))

        elem_bulk_residual=self._bulk_residual(y_pred)

        print('elem_bulk_residual',tf.shape(elem_bulk_residual))
        elem_residual_mask = pde_layers.GetElementResidualMask(labels)
        print('elem_residual_mask',tf.shape(elem_residual_mask))
        R = pde_layers.GetNodalInfoFromElementInfo(elem_bulk_residual, elem_residual_mask, dof=self.dof)
        R_fix = tf.where(dirichlet_bc==0.5, R, 0.0)
        F_mean = tf.reduce_sum(R_fix, axis=[1,2])
        print('F_mean', F_mean)
        print('R',tf.shape(R))
        R_norm = tf.norm( R, axis=[1,2])
        R_reduce_mean = tf.reduce_mean(tf.square( R ))
        R_reduce_sum = tf.reduce_sum(tf.square( R ))
        R_reduce_mean_norm = tf.reduce_mean(R_norm)
        # c = tf.constant([[1.0, 2.0], [3.0, 4.0]])
        # R_norm = tf.norm( c, axis=1, keepdims=True)
        # print(R_norm)
        # print(R_reduce_mean)
        # print(R_reduce_mean_norm)
        # print(R_reduce_sum)
        # R_new = R - neumann_residual

        R_neumann = tf.multiply(R, reverse_bc_mask_neumann)
        print('R_neumann :', tf.shape(R_neumann ))
        delta_R_neumann = R_neumann - neumann_residual
        print('delta_R_neumann :', tf.shape(delta_R_neumann ))

        R = R - neumann_residual

        dist = tfp.distributions.Normal(loc=tf.zeros_like(R), scale=0.00001)
        print( tf.keras.backend.sum(-dist.log_prob(R)) )

        R_bc_mask_dirichlet = tf.multiply(R, bc_mask_dirichlet) 
        print('R_bc_mask_dirichlet:', tf.shape(R_bc_mask_dirichlet))
        R_bc_mask_dirichlet_neumann = tf.multiply(R_bc_mask_dirichlet, bc_mask_neumann) 
        print('R_bc_mask_dirichlet_neumann:', tf.shape(R_bc_mask_dirichlet_neumann))

        print(tf.reduce_mean(tf.square( R_bc_mask_dirichlet )))
        print(tf.reduce_mean(tf.reduce_sum(tf.square(R_bc_mask_dirichlet_neumann), axis=[1,2,3]))) 
        print(tf.reduce_mean(tf.reduce_sum(tf.square(delta_R_neumann), axis=[1,2,3])))

        plot_fields(
                list_of_field = [
                    features[0:1, :, :, 0:self.dof], 
                    features[0:1, :, :, self.dof:2*self.dof], 
                    labels[0:1, :, :, 0:self.dof],  
                    y_pred[0:1, :, :, 0:self.dof]],
                list_of_field_name = [
                    'Dirichlet', 
                    'Neumann', 
                    'Label', 
                    'Pred. Sol.'], 
                dof = self.dof, 
                dof_name = self.dof_name,
                filename = 'results/' + self.problem_name + '-Sol.png')

        plot_fields(
                list_of_field = [
                    bc_mask_dirichlet[0:1, :, :, 0:self.dof], 
                    reverse_bc_mask_dirichlet[0:1, :, :, 0:self.dof], 
                    mask_labels[0:1, :, :, 0:self.dof], 
                    bc_mask_neumann[0:1, :, :, 0:self.dof],  
                    reverse_bc_mask_neumann[0:1, :, :, 0:self.dof]],
                list_of_field_name = [
                    'BC_Mask_Dirichlet', 
                    'Rev. BC_Mask_Dir.', 
                    'Rev. BC_Mask_Dir. * Labels', 
                    'BC_Mask_Neumann', 
                    'Rev. BC_Mask_Neu'], 
                dof = self.dof, 
                dof_name = self.dof_name,
                filename = 'results/' + self.problem_name + '-BCs.png')

        plot_fields(
                list_of_field = [
                    R[0:1, :, :, 0:self.dof], 
                    tf.tile(elem_residual_mask[0:1, :, :, 0:1], [1,1,1,self.dof]), 
                    neumann_residual[0:1, :, :, 0:self.dof], 
                    delta_R_neumann[0:1, :, :, 0:self.dof],  
                    R_bc_mask_dirichlet_neumann[0:1, :, :, 0:self.dof]],
                list_of_field_name = [
                    'Nodal R', 
                    'Elem Residual Mask', 
                    'Neumann R', 
                    'dR Neumann', 
                    'R * BC_M_Dir. * BC_M_Neu.'], 
                dof = self.dof, 
                dof_name = self.dof_name,
                filename = 'results/' + self.problem_name + '-R.png')


    def test(self, test_folder='', plot_png=True, output_reaction_force=False):
        """
        Make prediction with the surrogate model

        Args:
            test_folder (str): folder name under relative to the DataPath in the config.ini file.

        Note:
            - If test_folder is not specified, this subroutine will make prediction based on test_seq that is split from the training dataset. 
            - If test_folder is specified, this subroutine will load all the data from the folder to test_seq to make prediction.
            - For deterministic model, the MonteCarloNum from config.ini is over written to 1.

        """
        if test_folder == '':
            only_testing = False
            # return
        else:
            only_testing = True
            test_folder.replace('/', '')
            self._load_data(test_folder=test_folder + '/')

        if self.args.restartfrom:
            if self.model is None:
                self._build_model()
                self._train()

        print(' ... Running monte carlo inference')
        # Compute log prob of heldout set by averaging draws from the model:
        # p(heldout | train) = int_model p(heldout|model) p(model|train)
        #                   ~= 1/n * sum_{i=1}^n p(heldout | model_i)
        # where model_i is a draw from the posterior p(model|train).
        predictions = []
        reaction_force = []
        print(len(self.test_seq))
        # if not test_folder:
            # self.monte_carlo_num = 200
        for _ in range(self.monte_carlo_num):
            y_pred = self.model.predict(self.test_seq, verbose=1) 

            # # for scaling test
            # start_time = time.time()
            # y_pred = self.model.predict(self.test_seq, verbose=1) 
            # print("--- %s m seconds ---" % ((time.time() - start_time) * 1000/4096))
            # exit(0)

            if self.UseTwoNeumannChannel :
                inputs = y_pred[:,:,:,self.dof:4*self.dof] # New Neumann Channel
            else:
                inputs = y_pred[:,:,:,self.dof:3*self.dof] # Old Neumann Channel
            y_pred = y_pred[:,:,:,0:self.dof]
            if output_reaction_force:
                _, y_pred, _, _, _, R_fix = self._compute_residual(inputs, y_pred)
                reaction_force.append(R_fix)
            else:
                y_pred= self._compute_residual(inputs, y_pred, only_y_pred=True)
            predictions.append(y_pred)

        probs = tf.stack(predictions, axis=0)
        # print(" ... probs ...", np.shape(probs))
        mean_probs = tf.reduce_mean(probs, axis=0)
        # print(" ... mean_probs ...", np.shape(mean_probs))
        std_probs = tf.math.reduce_std(probs, axis=0)
        # print(" ... std_probs ...", np.shape(std_probs))
        expand_mean_probs = tf.tile(tf.expand_dims(mean_probs, axis=0), [self.monte_carlo_num, 1, 1, 1, 1] )
        # print(" ... expand mean_probs ...", np.shape(expand_mean_probs))
        var_probs = tf.reduce_mean( tf.math.pow(probs - expand_mean_probs, 2), axis=0)
        # print(" ... var_probs ...", np.shape(var_probs))

        if output_reaction_force:
            # output reaction forces at the location where x=0, and y=0.
            # the loadings are the DNS actually value without scaling
            # ux, uy, tx, ty, fx_mean, fy_mean, fx_std, fy_std 
            R_f = tf.stack(reaction_force, axis=0)
            mean_R_f = tf.reduce_mean(R_f, axis=0) 
            std_R_f = tf.math.reduce_std(R_f, axis=0) 
            # print(np.shape(mean_R_f), np.shape(std_R_f))
            F_mean = tf.reduce_sum(mean_R_f, axis=[1,2])
            F_std = tf.reduce_sum(std_R_f, axis=[1,2])
            print('reaction force(mean): ', F_mean)
            print('reaction force(std): ', F_std)
            Loadings = 2.0 * tf.reduce_max(inputs, axis=[1,2])-1
            _filename = self.filename + '-' + test_folder + 'F' + '.npy'
            all_force_info = tf.concat([Loadings, F_mean, F_std], axis=1)
            np.save(_filename, all_force_info)

            # print('F_mean ', F_mean)
            # print('F_std ', F_std)
            # print('BCs max (scaled)', tf.reduce_max(inputs, axis=[1,2]))
            # print('BCs max (original)', 2.0 * tf.reduce_max(inputs, axis=[1,2])-1)
        
        # heldout_log_prob = tf.reduce_mean(tf.math.log(mean_probs))
        # print(' ... Held-out nats: {:.3f}'.format(heldout_log_prob))

        if only_testing:
            if plot_png:
                for i in range(0, tf.shape(self.test_label)[0]):
                    if self.UseTwoNeumannChannel :
                        plot_PDE_solutions_new(
                                img_input = self.test_dataset[i:i+1, :, :, 0:3*self.dof], 
                                img_label = self.test_label[i:i+1, :, :, 0:self.dof], 
                                img_pre_mean = mean_probs[i:i+1, :, :, 0:self.dof], 
                                img_pre_var = var_probs[i:i+1, :, :, 0:self.dof], 
                                img_pre_std = std_probs[i:i+1, :, :, 0:self.dof], 
                                dof=self.dof, 
                                dof_name=self.dof_name, 
                                tot_img=self.tot_img, 
                                filename = self.filename + '-' + test_folder.replace('/','_') + '-' + str(i) + '.png'
                                )
                    else:
                        plot_PDE_solutions(
                                img_input = self.test_dataset[i:i+1, :, :, 0:2*self.dof], 
                                img_label = self.test_label[i:i+1, :, :, 0:self.dof], 
                                img_pre_mean = mean_probs[i:i+1, :, :, 0:self.dof], 
                                img_pre_var = var_probs[i:i+1, :, :, 0:self.dof], 
                                img_pre_std = std_probs[i:i+1, :, :, 0:self.dof], 
                                dof=self.dof, 
                                dof_name=self.dof_name, 
                                tot_img=self.tot_img, 
                                filename = self.filename + '-' + test_folder.replace('/','_') + '-' + str(i) + '.png'
                                )
                    print('save to: ', self.filename + '-' + test_folder.replace('/','_') + '-' + str(i) + '.png')

            return self.test_dataset[:, :, :, 0:3*self.dof], self.test_label[:, :, :, 0:self.dof], mean_probs[:, :, :, 0:self.dof], var_probs[:, :, :, 0:self.dof], std_probs[:, :, :, 0:self.dof]
        else:
            # just plot one data point for visualization.

            if self.UseTwoNeumannChannel :
                plot_PDE_solutions_new(
                        img_input = self.test_dataset[0:1, :, :, 0:3*self.dof], 
                        img_label = self.test_label[0:1, :, :, 0:self.dof], 
                        img_pre_mean = mean_probs[0:1, :, :, 0:self.dof], 
                        img_pre_var = var_probs[0:1, :, :, 0:self.dof], 
                        img_pre_std = std_probs[0:1, :, :, 0:self.dof], 
                        dof=self.dof, 
                        dof_name=self.dof_name, 
                        tot_img=self.tot_img, 
                        filename = self.filename + '.png'
                        )
            else:
                plot_PDE_solutions(
                        img_input = self.test_dataset[0:1, :, :, 0:2*self.dof], 
                        img_label = self.test_label[0:1, :, :, 0:self.dof], 
                        img_pre_mean = mean_probs[0:1, :, :, 0:self.dof], 
                        img_pre_var = var_probs[0:1, :, :, 0:self.dof], 
                        img_pre_std = std_probs[0:1, :, :, 0:self.dof], 
                        dof=self.dof, 
                        dof_name=self.dof_name, 
                        tot_img=self.tot_img, 
                        filename = self.filename + '.png'
                        )
            print('save to: ', self.filename + '.png')


            # only simulations with new restarted files are saved 
            if self.epochs >= 100:
                # additional simulation results are saved to pickle for future post processing purpose.
                self.simulation_results['features'] = self.test_dataset[0:10, :, :, 0:2*self.dof]
                self.simulation_results['labels'] = self.test_label[0:10, :, :, 0:self.dof]
                self.simulation_results['mean'] = mean_probs[0:10, :, :, 0:self.dof]
                self.simulation_results['var'] = var_probs[0:10, :, :, 0:self.dof]
                self.simulation_results['std'] = std_probs[0:10, :, :, 0:self.dof]

                pickle_out = open(self.filename + '.pickle', "wb")
                pickle.dump(self.simulation_results, pickle_out)
                pickle_out.close()
                print('save to: ', self.filename + '.pickle')


    def test_residual_gaussian(self, noise_std=1.e-3, sample_num=10000):
        """
        Test the residual noise distribution based on a Gaussian perturbation to inputs.

        Args:
            noise_std (float): default (1.0e-3)
            sample_num (int): default (10000)

        Note:
        It is preferred to use the DNS label data to make the test as the actually residual (mean) from such data is very small. 
        By default, it will load data from DataPath/DNS. Only the first data point will be used.
        """

        self._load_data()

        x_dim = tf.shape(self.labels).numpy()[1]
        y_dim = tf.shape(self.labels).numpy()[2]

        filename = 'results/' + self.problem_name + '-num-' + str(sample_num) + '-std-' + "{:.1e}".format(noise_std)

        dummy_batch = 500 # if too big, we get gpu memory issue
        dpi = 150

        if tf.shape(self.labels)[0] != 1:
            self.labels = self.labels[0:1,:,:,:]
            self.features = self.features[0:1,:,:,:]
            # raise ValueError("the residual Gaussian test only work for self.labels with first dim = 1, tf.shape(self.labels)=", tf.shape(self.labels))

        self.features = tf.convert_to_tensor(self.features, dtype=tf.float32)
        self.labels = tf.convert_to_tensor(self.labels, dtype=tf.float32)
        dist = tfp.distributions.Normal(loc=tf.zeros_like(self.labels), scale=noise_std)

        features = tf.tile(self.features, [sample_num,1,1,1])
        labels   = tf.tile(self.labels,   [sample_num,1,1,1])
        y_pred = labels
        # print(tf.shape(y_pred))
        y_noise = tf.squeeze(dist.sample(sample_num), [1]) # after sampling, the 2nd dim is 1.
        y_pred = y_pred + y_noise
        y_pred = tf.concat([self.labels, y_pred[1:,:,:,:]], axis=0) # replace 0 with data point without perturbation
        # print(tf.shape(y_pred))

        dummy_seq = np.array_split(np.arange(sample_num), int(sample_num/dummy_batch)+1)
        # print(dummy_seq)
        R_all = []
        R_list = []
        for s0 in dummy_seq:
            start = s0[0]
            end = s0[-1] + 1 # because start:end will not count the end, +1 will make sure end index will be counted
            R, _, _, _, _, _ = self._compute_residual(features[start:end,:,:,:], y_pred[start:end,:,:,:])
            # print(tf.shape(R))
            R_list.append(R)
        R = np.vstack(R_list)
        # print(tf.shape(R))

        for i0 in range(0, self.dof):
            plot_one_field(data=R[:,:,:,i0], x_dim=2, y_dim=2, dpi=dpi, name=filename + '-' + self.dof_name[i0] + '-R.png')
            plot_one_field_stat(data=R[:,:,:,i0], dpi=dpi, name=filename + '-' + self.dof_name[i0] + '-R-stat.png')
            plot_one_field_hist(data=R[:,:,:,i0], x_dim=x_dim, y_dim=y_dim, dpi=dpi, name=filename + '-' + self.dof_name[i0] + '-R-hist.png')
            plot_one_field_hist(data=y_pred[:,:,:,i0], x_dim=x_dim, y_dim=y_dim, dpi=dpi, name=filename + '-' + self.dof_name[i0] + '-solution-hist.png')


    def run(self):
        """ 
        Run the model by performing:

        - load data
        - build model
        - train
        - test
        """
        self._load_data()
        self._build_model()
        self._train()
        self.test()
            
if (__name__ == '__main__'):
    model = PDEWorkflowSteadyState()
    model.run()


====================================================================================================
mechanoChemML\workflows\systemID\__init__.py
====================================================================================================


====================================================================================================
mechanoChemML\workflows\systemID\forward_model.py
====================================================================================================
from ufl import *
from dolfin import *
import numpy as np
import os

def Schnakenberg_model(results_dir='results'):
  point0=Point(0,0)
  point1=Point(10,10)
  mesh = RectangleMesh(MPI.comm_world,point0,point1,50, 50)
  set_log_active(False)
  P1 = FiniteElement('P', triangle, 1)
  element = MixedElement([P1, P1])
  V = FunctionSpace(mesh, element)
  #
  # Define functions
  C_all = Function(V)
  C_all_n = Function(V)
  C1, C2 = split(C_all)
  C1_n, C2_n = split(C_all_n)

  ############
  #residual
  ############

  bcs=[]
  w1, w2 = TestFunctions(V)
  grad_w1=grad(w1)
  grad_w2=grad(w2)
  grad_C1=grad(C1)
  grad_C2=grad(C2)


  theta=np.zeros(12)
  #active parameters
  theta[0]=0.05
  theta[2]=0.1
  theta[3]=-1
  theta[5]=1

  theta[7]=2
  theta[8]=0.9
  theta[11]=-1
  
  dt=Constant(0.25)
  R_q=(C1-C1_n)/dt*w1
  R_q+= theta[0]*inner(grad_w1,grad_C1)
  R_q+= theta[1]*inner(grad_w1,grad_C2)
  R_q-= theta[2]*w1
  R_q-= theta[3]*C1*w1
  R_q-= theta[4]*C2*w1
  R_q-= theta[5]*C1*C1*C2*w1
  
  R_q+=(C2-C2_n)/dt*w2
  R_q+= theta[6]*inner(grad_w2,grad_C1)
  R_q+= theta[7]*inner(grad_w2,grad_C2)
  R_q-= theta[8]*w2
  R_q-= theta[9]*C1*w2
  R_q-= theta[10]*C2*w2
  R_q-= theta[11]*C1*C1*C2*w2
  
  R=R_q*dx
  J=derivative(R, C_all)

  #initial condition
  C_all_n.vector()[:]=0.5+np.random.uniform(-0.01,0.01,C_all_n.vector()[:].size)
  C_all.assign(C_all_n)
  
  #output setting
  if(not os.path.exists(results_dir)):
    os.mkdir(results_dir)
  
  file_C1 = XDMFFile(MPI.comm_world,results_dir+'/C1.xdmf');
  file_C2 = XDMFFile(MPI.comm_world,results_dir+'/C2.xdmf');
  file_data = HDF5File(MPI.comm_world, results_dir+'/data.h5', 'w')

  
  file_C1.write(C_all.sub(0),0);
  file_C2.write(C_all.sub(1),0);

  file_data.write(mesh,'/mesh')
  file_data.write(C_all,'/C_all',0)
  
  total_num_steps=160
  t=0
  step=0
  while(step<total_num_steps):
    problem = NonlinearVariationalProblem(R, C_all,bcs,J)
    solver = NonlinearVariationalSolver(problem)
    print('step=',step,'; dt=',float(dt),'; total time=',t)
    prm = solver.parameters
    prm["newton_solver"]["absolute_tolerance"] = 1E-8
    prm["newton_solver"]["relative_tolerance"] = 1E-9
    prm["newton_solver"]["maximum_iterations"] = 50
    prm["newton_solver"]["error_on_nonconvergence"] = False
    a, converge_flag=solver.solve()
    if(converge_flag):
      step+=1
      C_all_n.assign(C_all)
      t+=float(dt)
  
      file_C1.write(C_all.sub(0),step);
      file_C2.write(C_all.sub(1),step);
      file_data.write(C_all,'/C_all',step)
    else:
      print(' ')
      #print('Not converge, halve the time step...')
      # dt.assign(float(dt)/2)



def threeField_neo_Hookean_model(results_dir='results'):
  print('generating data... ')
  set_log_active(False)
  zeros = Constant((0.0, 0.0, 0.0))
  point0=Point(0,0,0)
  point1=Point(10,2,2)
  mesh = BoxMesh(MPI.comm_world,point0,point1,25,5,5)
  V = VectorFunctionSpace(mesh, "Lagrange", 1)
  u = Function(V,name='u') 
  #rectangular
  x_0=0
  x_1= 10

  y_0=0
  y_1=2

  BC1 =  CompiledSubDomain("near(x[0], side) && on_boundary", side = x_0 )
  BC2 =  CompiledSubDomain("near(x[0], side) && on_boundary", side = x_1 )

  BC3 =  CompiledSubDomain("near(x[1], side) && on_boundary", side = y_0 )
  BC4 =  CompiledSubDomain("near(x[1], side) && on_boundary", side = y_1 )


  boundary_subdomains = MeshFunction("size_t", mesh, mesh.topology().dim() - 1)
  boundary_subdomains.set_all(0)
  BC2.mark(boundary_subdomains,1)
  BC3.mark(boundary_subdomains,2)
  BC4.mark(boundary_subdomains,3)

  dss = ds(subdomain_data=boundary_subdomains)

  v = TestFunction(V) 
  # Kinematics
  d=len(u)
  I = Identity(d)             # Identity tensor
  Fe = I + grad(u)          # Deformation gradient    
  
  J=det(Fe)             
  C = Fe.T*Fe                # Right Cauchy-Green tensor
  invC=inv(C)              
  I1 = tr(C)
  I2=0.5*(I1*I1-tr(C*C) )
  I3  = det(C)

  barI1=J**(-2/3)*I1

  omega1=Constant(0)
  omega2=Constant(0)

  P=2*Fe*(omega1*(J**(-2/3)*I-1./3.*barI1*invC))+omega2*2*(J-1)*J*inv(Fe.T)


  x = SpatialCoordinate(mesh)
  #force=40,0.5,3
  #force
  def loss_eval(m):
    error=0
    omega1.assign(m[0])
    omega2.assign(m[1])
    shape_list=['extension','extension_2','bending','torsion']
    bcl_1 = DirichletBC(V, zeros, BC1)
    bcl_1_y = DirichletBC(V, zeros, BC3)
  
    for shape in shape_list:
      u.vector()[:]=0
      print('running deformation modes: ', shape)
      load_scale_list=[0.01,0.1,0.4,0.8,1]
      hdf5=HDF5File(MPI.comm_world, 'results/'+shape+'.h5','w')
      hdf5.write(mesh,'/mesh')
      for load_scale in load_scale_list:
        bcs=[bcl_1]  
        surface=1
        if shape=='extension_2':
          force=80
          T = Constant((0, force*load_scale, 0))
          surface=3
          bcs=[bcl_1_y]
        if shape=='extension':
          force=40 
          T = Constant((force*load_scale, 0, 0))
        elif shape=='bending':
          force=0.5
          T = Constant((0, 0, force*load_scale) )
        elif shape=='torsion':
          force=5
          T = Expression(("0"," f*sqrt( (x[1]-1)*(x[1]-1)+(x[2]-1)*(x[2]-1) )*sin(atan2(x[2]-1,x[1]-1))","-f*sqrt((x[1]-1)*(x[1]-1)+(x[2]-1)*(x[2]-1) )*cos(atan2(x[2]-1,x[1]-1)) "),degree =1,f=force*load_scale)
      
        R=inner(P,grad(v))*dx-dot(T,v)*dss(surface)
        Jobian=derivative(R, u)
        problem = NonlinearVariationalProblem(R, u, bcs, Jobian)
        solver = NonlinearVariationalSolver(problem)

        prm = solver.parameters      
        solver.solve()
    
      hdf5.write(u,'u')
      file = XDMFFile(MPI.comm_world,'results/visualization/'+shape+'.xdmf')
      file.write(u)


  m=[40.0,400.0]
  loss_eval(m)



====================================================================================================
mechanoChemML\workflows\systemID\generate_basis.py
====================================================================================================
from ufl import *
from dolfin import *
import numpy as np
import h5py as h5
import os


def Schnakenberg_basis(data_list,results_dir='results'):
  mesh=Mesh()
  hdf5=HDF5File(MPI.comm_world, results_dir+'/data.h5','r')
  hdf5.read(mesh,'/mesh',False)
  P1 = FiniteElement('P', triangle, 1)
  element = MixedElement([P1, P1])
  V = FunctionSpace(mesh, element)
  #
  # Define functions
  C_all = Function(V)
  C_all_n = Function(V)
  C1, C2 = split(C_all)
  C1_n, C2_n = split(C_all_n)

  ############
  #residual
  ############

  bcs=[]
  w1, w2 = TestFunctions(V)
  grad_w1=grad(w1)
  grad_w2=grad(w2)
  grad_C1=grad(C1)
  grad_C2=grad(C2)
  dt=Constant(0.25)
  def assemble_R(basis_id):
    if basis_id==0:
      R = -inner(grad_w1,grad_C1)*dx
    elif basis_id==1:
      R = -inner(grad_w1,grad_C2)*dx
    elif basis_id==2:
      R = 1*w1*dx
    elif basis_id==3:
      R = C1*w1*dx
    elif basis_id==4:  
      R = C2*w1*dx
    elif basis_id==5:
      R = C1*C1*C2*w1*dx
    elif basis_id==6:
      R = -inner(grad_w2,grad_C1)*dx
    elif basis_id==7:
      R = -inner(grad_w2,grad_C2)*dx
    elif basis_id==8:
      R = 1*w2*dx
    elif basis_id==9:
      R = C1*w2*dx
    elif basis_id==10:  
      R = C2*w2*dx
    elif basis_id==11:
      R = C1*C1*C2*w2*dx
    elif basis_id==12:
      R= (C1-C1_n)/dt*w1*dx
    elif basis_id==13:
      R= (C2-C2_n)/dt*w2*dx
    
      
    R_=assemble(R)
    for bc in bcs:
      bc.apply(R_)
    R_value=R_.get_local()

    return R_value

  sigma=0
  basis_path='basis'
  if not os.path.isdir(basis_path):
    os.mkdir(basis_path)
  print('save operators to '+basis_path)
  for step in data_list:
    if step==data_list[-1]:
      break
    hdf5.read(C_all,'C_all/vector_'+str(step+1))
    hdf5.read(C_all_n,'C_all/vector_'+str(step))


    #add noise
    C_all.vector()[:]=C_all.vector()[:]+np.random.normal(0,sigma,C_all.vector()[:].size)
    C_all_n.vector()[:]=C_all_n.vector()[:]+np.random.normal(0,sigma,C_all_n.vector()[:].size)
    basis=np.column_stack([assemble_R(basis_id) for basis_id in range(14)])
    #
    
    print('saving operators at time step ',step+1)
    np.savetxt('basis/basis_sigma_'+str(sigma)+'_step_'+str(step+1)+'.dat',basis)


def threeField_neo_Hookean_basis(data_list,results_dir='results/'):
  print('generating basis...')
  q_degree = 10
  zeros = Constant((0.0, 0.0, 0.0))
  mesh=Mesh()
  hdf5=HDF5File(MPI.comm_world, results_dir+data_list[0]+'.h5','r')
  hdf5.read(mesh,'/mesh',False)

  V = VectorFunctionSpace(mesh, "Lagrange", 1)
  u = Function(V,name='u') 
  #rectangular
  x_0=0
  x_1= 10
  y_0=0
  y_1=2

  BC1 =  CompiledSubDomain("near(x[0], side) && on_boundary", side = x_0 )
  BC2 =  CompiledSubDomain("near(x[0], side) && on_boundary", side = x_1 )
  BC3 =  CompiledSubDomain("near(x[1], side) && on_boundary", side = y_0 )
  BC4 =  CompiledSubDomain("near(x[1], side) && on_boundary", side = y_1 )

  boundary_subdomains = MeshFunction("size_t", mesh, mesh.topology().dim() - 1)
  boundary_subdomains.set_all(0)
  BC2.mark(boundary_subdomains,1)
  BC3.mark(boundary_subdomains,2)
  BC4.mark(boundary_subdomains,3)

  dss = ds(subdomain_data=boundary_subdomains)

  v = TestFunction(V) 
  # Kinematics
  d=len(u)
  I = Identity(d)             # Identity tensor
  Fe = I + grad(u)          # Deformation gradient    
  
  J=det(Fe)             
  C = Fe.T*Fe                # Right Cauchy-Green tensor
  invC=inv(C)              
  I1 = tr(C)
  I2=0.5*(I1*I1-tr(C*C) )
  I3  = det(C)

  # fiber direction
  a=Constant((1.0,0.0,0.0))
  A=outer(a, a)
  I4=dot(a,C*a)
  I5=dot(a,C*C*a)
  
  barI1=J**(-2/3)*I1
  barI2=J**(-4/3)*I2
  barI4=J**(-2/3)*I4
  barI5=J**(-4/3)*I5

  bcs=[]
  def assemble_R(basis_id):
    if basis_id==0:
      R = -dot(T,v)*dss(surface)
    elif basis_id==1:
      S=(J**(-2./3.)*I-1./3.*barI1*invC)
      P=Fe*2*S
      R=inner(P,grad(v))*dx(metadata={'quadrature_degree': q_degree})
    elif basis_id==2:
      S=2*(barI1-3)*(J**(-2./3.)*I-1./3.*barI1*invC)
      P=Fe*2*S
      R=inner(P,grad(v))*dx(metadata={'quadrature_degree': q_degree})
    elif basis_id==3:
      S=(J**(-2./3.)*barI1*I- J**(-4./3.)*C-2./3.*barI2*invC )
      P=Fe*2*S
      R=inner(P,grad(v))*dx(metadata={'quadrature_degree': q_degree})
    elif basis_id==4:
      S=2*(barI2-3)*(J**(-2./3.)*barI1*I- J**(-4./3.)*C-2./3.*barI2*invC )
      P=Fe*2*S
      R=inner(P,grad(v))*dx(metadata={'quadrature_degree': q_degree})
    elif basis_id==5:
      S=(J**(-2./3.)*A-1./3.*barI4*invC)
      P=Fe*2*S
      R=inner(P,grad(v))*dx(metadata={'quadrature_degree': q_degree})
    elif basis_id==6:
      S=2*(barI4-1)*(J**(-2./3.)*A-1./3.*barI4*invC)
      P=Fe*2*S
      R=inner(P,grad(v))*dx(metadata={'quadrature_degree': q_degree})
    elif basis_id==7:
      S=(J-1)*J*invC
      P=Fe*2*S
      R=inner(P,grad(v))*dx(metadata={'quadrature_degree': q_degree})
    elif basis_id==8:
      S=2*(J-1)*(J-1)*(J-1)*J*invC
      P=Fe*2*S
      R=inner(P,grad(v))*dx(metadata={'quadrature_degree': q_degree})
   
    R_=assemble(R)
    for bc in bcs:
      bc.apply(R_)
    R_value=R_.get_local()
    return R_value
  
  #shape_list=['extension','extension_2','bending','torsion']
  bcl_1 = DirichletBC(V, zeros, BC1)
  bcl_1_y = DirichletBC(V, zeros, BC3)
  for shape in data_list:
    bcs=[bcl_1]
    surface=1
    if shape=='extension_2':
      force=80
      T = Constant((0, force, 0))
      surface=3
      bcs=[bcl_1_y]
    if shape=='extension':
      force=40 
      T = Constant((force, 0, 0))
    elif shape=='bending':
      force=0.5
      T = Constant((0, 0, force) )
    elif shape=='torsion':
      force=5
      T = Expression(("0"," f*sqrt( (x[1]-1)*(x[1]-1)+(x[2]-1)*(x[2]-1) )*sin(atan2(x[2]-1,x[1]-1))","-f*sqrt((x[1]-1)*(x[1]-1)+(x[2]-1)*(x[2]-1) )*cos(atan2(x[2]-1,x[1]-1)) "),degree =1,f=force)
      
    
    file=results_dir+shape+'.h5'
    print('save to',file)
    hdf5=HDF5File(MPI.comm_world, file,'r')
    hdf5.read(u, 'u')
    # sigma=0
    # u.vector()[:]=u.vector()[:]+np.random.normal(0,sigma,u.vector()[:].size)
    basis=np.column_stack([assemble_R(basis_id) for basis_id in range(9)])

    np.savetxt('basis/'+shape+'.dat',basis)



====================================================================================================
mechanoChemML\workflows\systemID\systemID.py
====================================================================================================
"""
Zhenlin Wang 2019
systemID workflow
"""

import numpy as np
from sklearn import linear_model
from sklearn.metrics.pairwise import cosine_similarity

import h5py as h5
from mechanoChemML.src import stepwiseRegression as ST

def getlist_str(option, sep=',', chars=None):
    """Return a list from a ConfigParser option. By default, 
     split on a comma and strip whitespaces."""
    list0 = [(chunk.strip(chars)) for chunk in option.split(sep)]
    list0 = [x for x in list0 if x]
    return list0


def getlist_int(option, sep=',', chars=None):
    """Return a list from a ConfigParser option. By default, 
     split on a comma and strip whitespaces."""
    list0 = option.split(sep)
    list0 = [x for x in list0 if x]
    if (len(list0)) > 0:
        return [int(chunk.strip(chars)) for chunk in list0]
    else:
        return []


def getlist_float(option, sep=',', chars=None):
    """Return a list from a ConfigParser option. By default, 
     split on a comma and strip whitespaces."""
    list0 = option.split(sep)
    list0 = [x for x in list0 if x]
    if (len(list0)) > 0:
        return [float(chunk.strip(chars)) for chunk in list0]
    else:
        return []

class systemID:
    """
  Class of system ID 
  """
    def __init__(self):
        self.parse_sys_args()
        self.read_config_file()
        self.results={
          'model':[0],
          'prefactor':np.zeros(0),
          'loss':np.zeros(0),
          'cos_similiarity':np.zeros(0),
          'sum_cos_similiarity':np.zeros(0)
        }
        

    def debugger(self):
        import logging
        logger = logging.getLogger('root')
        FORMAT = "[%(filename)s:%(lineno)s - %(funcName)20s() ] %(message)s"
        logging.basicConfig(format=FORMAT)
        logger.setLevel(logging.DEBUG)
        self.logger = logger

    def read_config_file(self):
        """ """
        from configparser import ConfigParser, ExtendedInterpolation
        config = ConfigParser(interpolation=ExtendedInterpolation())
        config.read(self.args.configfile)
        config['TEST']['FileName'] = self.args.configfile
        self.config = config

    def parse_sys_args(self):
        import argparse, sys, os
        parser = argparse.ArgumentParser(description='Run Variational System Identification', prog="'" + (sys.argv[0]) + "'")
        parser.add_argument('configfile', type=str, help='configuration file')

        args = parser.parse_args()
        self.args = args
    
    def setup_model(self):        
        F_criteria=getlist_float(self.config['StepwiseRegression']['F_criteria'])
        F_switch=[]
        if len(F_criteria)>1:
          F_switch=getlist_int(self.config['StepwiseRegression']['F_switch'] )
        basis_drop_strategy=self.config['StepwiseRegression']['basis_drop_strategy']
        regression_method=self.config['StepwiseRegression']['regression_method']
        anchor_index=getlist_int(self.config['StepwiseRegression']['anchor_index'] )
        if len(anchor_index)==0 :
          anchor_index=[-1]
        alpha_lasso=0
        alpha_ridge=0
        ridge_cv=[-1]
        n_jobs=1
        if regression_method=='ridge':
          alpha_ridge=self.config['StepwiseRegression'].getfloat('alpha_ridge')
        elif regression_method=='lasso':
          alpha_lasso=self.config['StepwiseRegression'].getfloat('alpha_lasso')
        elif regression_method=='ridge_cv':
          ridge_cv=getlist_float(self.config['StepwiseRegression']['ridge_cv'])
        elif regression_method!='linear_regression':
          print('bad regression_method')
          exit()
          
        
        
        return ST.stepwiseR(F_criteria=F_criteria,F_switch=F_switch,basis_drop_strategy=basis_drop_strategy,anchor_index=anchor_index,alpha_lasso=alpha_lasso,alpha_ridge=alpha_ridge, ridge_cv=ridge_cv,n_jobs=n_jobs)

    def identifying(self,basis):
        print('\n-------------- Identifying --------------  ')
        strategy=self.config['VSI']['identify_strategy']
        if strategy=='specified_target':
          target_index=self.config['VSI'].getint('target_index')
          self.results['model']=self.stepwise_regression(basis,target_index)
          self.results['prefactor']=self.results['model'].gamma_matrix[:,-1]
          self.results['loss']=self.results['model'].loss[-1]
        elif strategy=='confirmation_of_consistency':
          self.confirmation_of_consistency(basis)
    
    def stepwise_regression(self,basis,target_index):
      model=self.setup_model()
      y=basis[:,target_index]
      _,n_base_orign=basis.shape
      basis_index=np.delete(np.arange(n_base_orign),target_index)
      theta_matrix=basis[:,basis_index]
      model.stepwiseR_fit(theta_matrix,y)
      return model
      
    def confirmation_of_consistency(self,basis):
      print('\n-------------- running confirmation_of_consistency... --------------  ')
      _,n_base_orign=basis.shape
      
      self.results['model']=[self.stepwise_regression(basis,key) for key in range(n_base_orign)]
      self.results['prefactor']=np.zeros((n_base_orign,n_base_orign))
      self.loss=np.zeros(n_base_orign)
      for i in range(n_base_orign):
        basis_id_theta=np.delete(np.arange(n_base_orign),i)
        self.results['prefactor'][basis_id_theta,i]=self.results['model'][i].gamma_matrix[:,-1]
        self.results['prefactor'][i,i]=-1
        self.loss[i]=self.results['model'][i].loss[-1]
        
      self.results['cos_similiarity']=cosine_similarity(np.transpose(self.results['prefactor']))
      self.results['sum_cos_similiarity']=np.sum(np.abs(self.results['cos_similiarity']),0)

  


====================================================================================================
setup.py
====================================================================================================
from setuptools import find_packages, setup

with open("README.md", "r") as f:
    long_description = f.read()

with open("requirements.txt", "r") as f:
    required = f.read()

with open("VERSION.md", "r") as f:
    current_version = f.read().strip()

setup(
        name="mechanoChemML",
        version=current_version,
        packages=find_packages(),
        description="A machine learning software library for computational materials physics",
        long_description=long_description,
        long_description_content_type="text/markdown",
        py_modules=["mechanoChemML"],
        url='https://github.com/mechanoChem/mechanoChemML',
        author='Xiaoxuan Zhang',
        author_email='zhangxiaoxuan258@gmail.com',
        package_dir={'mechanoChemML': 'mechanoChemML'},
        package_data={'mechanoChemML': [ 
        'workflows/active_learning/monte_settings.json.tmpl', 'workflows/active_learning/surrogate_weights/*.txt']},
        classifiers=[
            "Programming Language :: Python :: 3.7",
            "License :: OSI Approved :: BSD License",
            ],
        install_requires=required,
        extras_require = {
            "dev":[
                "pytest>=3.6",
                ],
            },
        )


====================================================================================================
VSIAdjointRotatorCuff\adjoint_HGO_CG2_20231012.py
====================================================================================================
from ufl import *
from dolfin import *
import numpy as np
from dolfin_adjoint import *
import os
import random
import sys
from functools import reduce
from stepwise_minimization import *
from numpy import linalg as LA
from boundaryConditions import *
from itertools import product
import time

parameters["mesh_partitioner"] = "ParMETIS"
parameters["dof_ordering_library"] = "Boost"

def forward_loss(theta, tendonName, condition, disp, meshName, 
                 lossFactor1, lossFactor2, listFactor):

 #################################### Define mesh ############################
  tendonDate = tendonName[6:]
  
  mesh=Mesh()
  with XDMFFile(tendonDate + "/convergenceAnalysis_V2/mesh/Tendon" + tendonDate+ 
                "_" + condition + "_" + meshName + ".xdmf") as infile:
    infile.read(mesh)
  
  V = VectorFunctionSpace(mesh, "Lagrange", 2)
  W = FunctionSpace(mesh, "DG", 0)

  x=SpatialCoordinate(mesh)
  dof_coordinates = V.tabulate_dof_coordinates()                    
  dof_coordinates.resize((V.dim(), mesh.geometry().dim()))                   
  
  ######################### Define regions to apply boundary conditions  ##########################
  dim = mesh.topology().dim()
  facetfct = MeshFunction('size_t', mesh, dim - 1)
  facetfct.set_all(0)
  mesh.init(dim - 1, dim) # relates facets to cells

  equationDict = globals()[tendonName]["equations"][condition].items()
  keyList = list(globals()[tendonName]["equations"][condition].keys())
  
  for f in facets(mesh):
    for key, equation in equationDict:
      if equation(f):
        facetfct[f.index()] = keyList.index(key) + 1

  n = FacetNormal(mesh)

  # verify that the bottom side is encastered and not rolling - verified
  q_degree = 5

  # Define functions
  u = Function(V,name='u')
  u_real = Function(V,name='u')
  boneDisp = Function(V, name='boneDisp')
  
  file_U = HDF5File(MPI.comm_world, tendonDate + 
                    "/convergenceAnalysis_V2/displacementHighRes/Tendon" + tendonDate +
                    "_" + meshName + "_U_" + condition + "_" + str(disp) + "_CG2.h5", 'r')
  file_U.read(u_real, 'U_' + condition + str(disp))
  file_U.read(boneDisp, 'U_' + condition + str(disp))

  # This is to calculate the predictions
  v = TestFunction(V) 
  # Kinematics
  d = len(u)
  I = Identity(d)          # Identity tensor
  F = I + grad(u)          # Deformation gradient    

  J=det(F)             
  C = F.T*F               # Right Cauchy-Green tensor

  # Invariants
  invC=inv(C)              
  I1 = tr(C)
  I2=0.5*(I1*I1-tr(C*C) )
  I3  = det(C)

  # Fiber direction
  file_UVW = HDF5File(MPI.comm_world, tendonDate + 
                      "/convergenceAnalysis_V2/UVWHighRes/Tendon" + tendonDate +
                      "_" + meshName + "_UVW_" + condition + "_CG2.h5", 'r')

  a = Function(V, name='UVW')
  file_UVW.read(a, '/UVW_' + condition)

  I4=dot(a,C*a)

  barI1=J**(-2/3)*I1
  barI2=J**(-4/3)*I2

  dx_ = dx(metadata={'quadrature_degree': q_degree})
  
  I14 = theta[4]*I1+(1.-3.*theta[4])*I4-1.
  
  force_used = np.loadtxt(tendonDate + "/forceData/" + tendonDate + "_MedianForceData_" + \
                          condition + "_" + str(disp) + "mm.txt")

  P=2*F*(theta[0]*(J-1)*J*invC + \
       theta[1]*(J**(-2./3.)*I-1./3.*barI1*invC) + \
        theta[2]*I14*exp(theta[3]*I14*I14)*(theta[4]*I + \
          (1.-3.*theta[4])*outer(a,a)))

  dss = Measure("ds", domain=mesh, subdomain_data=facetfct,
                subdomain_id=keyList.index("tendon")+1, metadata={'quadrature_degree': q_degree})

  if MPI.comm_world.rank == 0:
    print("Condition: ", condition)
    print("Load: ", disp)
  
  #Define traction on top surface
  normal_to_surf = n
  for i,factor in enumerate(listFactor):
    if MPI.comm_world.rank == 0:
      print("Step: ", factor)

    valueBC = Function(V)
    valueBC.assign(project(factor*boneDisp, V))
    # valueBC.vector()[:] = factor*boneDisp.vector()

    bcs = []

    for key in keyList:
      bc_key = DirichletBC(V, valueBC, facetfct, keyList.index(key) + 1)
      bcs.append(bc_key)
    
    traction = dot(P, normal_to_surf)

    R = inner(P,grad(v))*dx_ - dot(traction,v)*dss #- dot(u,normal_to_surf)*dssHead
    Jac=derivative(R, u)
    
    if MPI.comm_world.rank == 0:
      print('===============PRE load finished===============')
    
    problem = NonlinearVariationalProblem(R, u, bcs, Jac)
    solver = NonlinearVariationalSolver(problem)

    prm = solver.parameters
    prm["newton_solver"]["absolute_tolerance"] = 1E-8
    prm["newton_solver"]["relative_tolerance"] = 1E-9
    prm["newton_solver"]["maximum_iterations"] = 10
    # if i == 0:
    #   prm['newton_solver']['relaxation_parameter'] = 0.5
    #   prm["newton_solver"]["maximum_iterations"] = 100
    # else:
    #   prm['newton_solver']['relaxation_parameter'] = 1.
    #   prm["newton_solver"]["maximum_iterations"] = 10
    prm["newton_solver"]["linear_solver"] = 'mumps'
    prm["newton_solver"]["error_on_nonconvergence"] = False
    solver.solve()
  
  load_dir = Constant((-1.0, 0.0, 0.0))
  loadOnFace = dot(traction, load_dir)*dss # This is what needs to change with the other loading modes
  P_ext = force_used
  
  ######################## Calculate losses ##########################
  
  # Calculate maximum displacement to make loss dimensionless
  dim_ureal = u_real.function_space().num_sub_spaces()
  umax_real = u_real.vector()[:]
  umax_real = umax_real.reshape((-1, dim_ureal))
  umax_real = np.linalg.norm(umax_real, axis=1)
  umax_real = np.max(umax_real)

  comm = u_real.function_space().mesh().mpi_comm()
  max_global = MPI.max(comm, umax_real)

  volume = assemble(Constant((1))*dx(domain=mesh))

  alphaMatrix = Constant(((1., 0., 0.),(0., 1., 0.),(0., 0., 1.)))
  loss1 = assemble((inner(u - u_real, dot(alphaMatrix,u - u_real))/(max_global**2))*dx)*lossFactor1/volume
  loss2 = pow(assemble(loadOnFace) - P_ext,2)*lossFactor2/P_ext**2

  #penaltySum = sum(float(theta[i].values()[0])**2 for i in range(4))

  loss = loss1 + loss2 #+ 0.5*lam*penaltySum
  if MPI.comm_world.rank == 0:
    print("Loss1 = ", loss1)
    print("Loss2 = ", loss2)
    print("Loss = ", loss)
  
  return loss

def addLosses(theta, tendonStamp, meshName, lossFactor1, lossFactor2, 
              combinationList, listFactor):
  loss = 0.
  for k in range(len(combinationList)):
    loss += forward_loss(theta, "Tendon" + tendonStamp, combinationList[k][0],
                        combinationList[k][1], meshName, lossFactor1,
                        lossFactor2, listFactor)
  return loss

if __name__ == "__main__":
    
  ###################################### Adjoint running #########################
  method = "L-BFGS-B"
  #method = "SLSQP"
  #method = 'BFGS'
  
  tendonStampList = ["20231012", "20231017", "20231107", "20231114", 
                     "20231201", "20231206", "20231212", "20240503", 
                     "20240517", "20241127_1", "20241127_2", "20241219"]
  indexList = [0,1,2,3,4,5,6,7,8,9,10,11]
  indexList = [0]
  conditionList = ["Intact", "Torn"]
  dispList = ["1", "2"]
  combinationList = np.array(list(product(conditionList, dispList)))
  combinationList = combinationList[1:,] # this leaves the intact set, 1 mm load out

  lossFactor1 = 1.
  lossFactor2 = 1.e-2

  listFactor = [0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.]

  lam = 0.
  meshName = "Coarse"
  identifier = 'LF1E_6_NoBounds_'+ meshName + 'Mesh' 
  
  paramsFile = 'AllCoeffs_AllTendons_' + identifier + '_HGOHighOrderI1I2_SquaredNorm_PureHGO.dat'
  data = np.loadtxt(paramsFile)
  for index in indexList:
    adjointTrialsFile = tendonStampList[index] + '/results/HGOHighOrderI1I2_SquaredNorm/' + \
                tendonStampList[index] + "adjoint_log_"+ identifier + "_LF21E_2.dat"
    derivativeFile = tendonStampList[index] + '/results/HGOHighOrderI1I2_SquaredNorm/' + \
                tendonStampList[index] + "derivative_log_"+ identifier + "_LF21E_2.dat"
    def eval_cb_save(m):
      if MPI.comm_world.rank == 0:
        print ("m = ", m)
        with open(adjointTrialsFile, 'a') as solver_log:
          np.savetxt(solver_log, m[None, :])

    def derivative_cb(j, dj, m):
      if MPI.comm_world.rank == 0:
        print("j = %f, dj = %s, m = %s." %(j, [float(k) for k in dj], [float(k) for k in m]))
        print(f"grad norm = {np.linalg.norm([float(k) for k in dj]):.2e}")
        combined = np.array([j] + [float(k) for k in dj] + [float(k) for k in m] + [np.linalg.norm([float(k) for k in dj])])
        with open(derivativeFile, 'a') as derivativeLog:
          np.savetxt(derivativeLog, combined[None, :])
      return dj

    if MPI.comm_world.rank == 0:
      print("Solving tendon: ", tendonStampList[index])

    parameters_value = data[index,:]
    
    theta=[0]*len(parameters_value)
    for j in range(len(parameters_value)):
        theta[j]=Constant(parameters_value[j])
    
    if MPI.comm_world.rank == 0:
      print("parameters_value=",parameters_value)
    
    control_index = [0,1,2,3,4]
    bounds=np.zeros((2,len(control_index)))
    bounds[0,:]= 0.	#All terms positive
    bounds[1,:]= np.inf
    bounds[0,0] = parameters_value[0]
    bounds[1,0] = parameters_value[0]*5.
    bounds[0,1] = parameters_value[1]*0.1
    bounds[1,1] = parameters_value[1]*10.
    bounds[0,2] = parameters_value[2]
    bounds[1,2] = parameters_value[2]*20.
    bounds[0,3] = parameters_value[3]
    bounds[1,3] = parameters_value[3]*50.
    bounds[1,4] = 1./3.

    control_parameter=[Control(theta[i]) for i in control_index]
    reduced_functional = ReducedFunctional(addLosses(theta, tendonStampList[index],
                     meshName, lossFactor1, lossFactor2, combinationList,
                     listFactor), control_parameter, derivative_cb_post=derivative_cb)

    ###Starting adjoint
    if MPI.comm_world.rank == 0:
      print("Starting adjoint for tendon: ", tendonStampList[index])
    
    results_opt = minimize(reduced_functional, method = method, bounds=bounds,
                    tol = 1e-8, options = {'ftol':1.0e-11, 'gtol':1.0e-11, 'maxiter':200, 
                               "disp": True},callback = eval_cb_save)
    if MPI.comm_world.rank == 0:
      print('==========Adjoint Finished==========')
      
    results_opt=np.array(results_opt)
    if MPI.comm_world.rank == 0:
      print('Optimized results = ',results_opt)
    
    alphaMatIdentifier = "1"
    
    save_to_file = str(tendonStampList[index]) + '/results/HGOHighOrderI1I2_SquaredNorm/' \
            + str(tendonStampList[index]) + '_adjoint_lam_'\
                + str(lam) + '_' + identifier + '_alpha_' + \
                    alphaMatIdentifier + '_LF21E_2.dat'
    np.savetxt(save_to_file,results_opt)

====================================================================================================
VSIAdjointRotatorCuff\boundaryConditions.py
====================================================================================================
Tendon20231012 = {
  "equations": {
    "Intact": {
      "tendon": lambda f: f.normal(0)<-0.8 and f.exterior() and f.midpoint()[0]<-15.,
      "enthesis": lambda f: f.normal(1)<-0.7 and f.exterior() and \
        f.midpoint()[0]-3.18e-4*f.midpoint()[2]**4+0.0033*f.midpoint()[2]**3-\
            0.0023*f.midpoint()[2]**2-0.3170*f.midpoint()[2]-0.7776 > 0.,
      "head": lambda f: f.normal(1)<-0.3 and f.exterior() and \
        f.midpoint()[0]-3.18e-4*f.midpoint()[2]**4+0.0033*f.midpoint()[2]**3-\
            0.0023*f.midpoint()[2]**2-0.3170*f.midpoint()[2]-0.7776 < 0. and \
                f.midpoint()[0]-0.0551*f.midpoint()[2]**2-\
                    0.2501*f.midpoint()[2]+13.8244 > 0.
    },
    "Torn": {
    "tendon": lambda f: f.normal(0)<-0.8 and f.exterior() and f.midpoint()[0]<-15.,
    "enthesis": lambda f: f.normal(1)<-0.7 and f.exterior() and \
      f.midpoint()[0]-3.18e-4*f.midpoint()[2]**4+0.0033*f.midpoint()[2]**3-\
        0.0023*f.midpoint()[2]**2-0.3170*f.midpoint()[2]-0.7776 > 0. \
          and (f.midpoint()[0]-0.0010*f.midpoint()[2]**4\
            -0.0139*f.midpoint()[2]**3 - 0.1176*f.midpoint()[2]**2 \
                - 0.3674*f.midpoint()[2] - 3.4498 < 0. or \
                    f.midpoint()[2]+0.0009*f.midpoint()[0]**3+\
                        0.1801*f.midpoint()[0]**2\
                        -3.2451*f.midpoint()[0] + 9.6708 > 0.),
    "head": lambda f: f.normal(1)<-0.5 and f.exterior() and \
      f.midpoint()[0]-3.18e-4*f.midpoint()[2]**4+0.0033*f.midpoint()[2]**3-\
        0.0023*f.midpoint()[2]**2-0.3170*f.midpoint()[2]-0.7776 < 0. and \
            f.midpoint()[0]-0.0551*f.midpoint()[2]**2-\
              0.2501*f.midpoint()[2]+13.8244 > 0.
    }
  }
}

====================================================================================================
VSIAdjointRotatorCuff\nonlinear_VSI_HGO_ConvergedMesh_CG2.py
====================================================================================================
from ufl import *
from dolfin import *
import numpy as np
import random
import sys
from functools import reduce
from stepwise_minimization_HGOConverged import *
from numpy import linalg as LA
from boundaryConditions import *
from itertools import product
import os

num_theta=12
num_basis=9
theta=[]
for i in range(num_theta):
  theta.append(Constant(0.0))

def nonlinear_VSI_HGO(target_array, tendonName, condition, disp, loss_factor2, meshName, current_activate_index,
                      target_index):
  #################################### Define mesh ############################
  tendonDate = tendonName[6:]
  
  mesh=Mesh()
  with XDMFFile(tendonDate + "/convergenceAnalysis_V2/mesh/Tendon" + tendonDate+ 
                "_" + condition + "_" + meshName + ".xdmf") as infile:
    infile.read(mesh)
    
  V = VectorFunctionSpace(mesh, "Lagrange", 2)

  x=SpatialCoordinate(mesh)
  dof_coordinates = V.tabulate_dof_coordinates()                    
  dof_coordinates.resize((V.dim(), mesh.geometry().dim()))                          
  
  ######################### Define regions to apply boundary conditions  ##########################
  dim = mesh.topology().dim()
  # print("dim = ", V.dim())
  facetfct = MeshFunction('size_t', mesh, dim - 1)
  facetfct.set_all(0)
  mesh.init(dim - 1, dim) # relates facets to cells

  equationDict = globals()[tendonName]["equations"][condition].items()
  keyList = list(globals()[tendonName]["equations"][condition].keys())
  
  for f in facets(mesh):
    for key, equation in equationDict:
      if equation(f):
        facetfct[f.index()] = keyList.index(key) + 1

  n = FacetNormal(mesh)

  q_degree = 5

  ########################### Define functions, invariants, fiber direction ######################################
  u = Function(V,name='u') 

  v = TestFunction(V) 
  # Kinematics
  d=len(u)
  I = Identity(d)          # Identity tensor
  F = I + grad(u)          # Deformation gradient    
  
  J=det(F)
  C = F.T*F               # Right Cauchy-Green tensor
  B=F*F.T
  
  # Invariants
  invC=inv(C)              
  I1 = tr(C) # volumetric
  I2=0.5*(I1*I1-tr(C*C) ) # biaxial state
  I3  = det(C) # triaxial

  # Fiber direction
  file_UVW = HDF5File(MPI.comm_world, tendonDate + 
                      "/convergenceAnalysis_V2/UVWHighRes/Tendon" + tendonDate +
                      "_" + meshName + "_UVW_" + condition + "_CG2.h5", 'r')

  a = Function(V, name='UVW')
  file_UVW.read(a, '/UVW_' + condition)

  I4=dot(a,C*a)

  barI1=J**(-2./3.)*I1
  barI2=J**(-4./3.)*I2
  logJ = ln(J)

  dss = Measure("ds", domain=mesh, subdomain_data=facetfct,
                subdomain_id=keyList.index("tendon")+1, metadata={'quadrature_degree': q_degree})

  I14 = theta[4]*I1+(1.-3.*theta[4])*I4-1.

  ################## Candidate functions for strain energy density ###########################
  hS0 = (J-1)*J*invC
  # hS0 = 0.5*logJ*invC
  hS1 = (J**(-2./3.)*I-1./3.*barI1*invC)
  # hS1 = 0.5*I - 0.5*invC - (1./3.)*logJ*invC

  # Compressible anisotropic part
  hS2 = theta[3]*I14*exp(theta[3]*I14*I14)*(theta[4]*I + (1.-3.*theta[4])*outer(a,a))

  # I1 terms
  hS3 = 2*(barI1-3)*(J**(-2./3.)*I-1./3.*barI1*invC) # quadratic
  hS4 = 3*(barI1-3)**2*(J**(-2./3.)*I-1./3.*barI1*invC) # cubic
  hS5 = 4*(barI1-3)**3*(J**(-2./3.)*I-1./3.*barI1*invC) # fourth order

  # I2 terms
  hS6 = 2*(barI2-3)*(J**(-2./3.)*barI1*I- J**(-4./3.)*C-2./3.*barI2*invC) # quadratic
  hS7 = 3*(barI2-3)**2*(J**(-2./3.)*barI1*I- J**(-4./3.)*C-2./3.*barI2*invC) # cubic
  hS8 = 4*(barI2-3)**3*(J**(-2./3.)*barI1*I- J**(-4./3.)*C-2./3.*barI2*invC) # fourth order

  # Here we build the basis function
  basis_pool=[0]*num_basis
  basis_pool[0]=theta[0]*inner(2*F*hS0, grad(v) )*dx(metadata=
                                                     {'quadrature_degree': q_degree})
  basis_pool[1]=theta[1]*inner(2*F*hS1, grad(v) )*dx(metadata=
                                                     {'quadrature_degree': q_degree})
  basis_pool[2]=theta[2]*inner(2*F*hS2, grad(v) )*dx(metadata=
                                                     {'quadrature_degree': q_degree})
  basis_pool[3]=theta[5]*inner(2*F*hS3, grad(v) )*dx(metadata=
                                                     {'quadrature_degree': q_degree})
  basis_pool[4]=theta[6]*inner(2*F*hS4, grad(v) )*dx(metadata=
                                                     {'quadrature_degree': q_degree})
  basis_pool[5]=theta[7]*inner(2*F*hS5, grad(v) )*dx(metadata=
                                                     {'quadrature_degree': q_degree})
  basis_pool[6]=theta[8]*inner(2*F*hS6, grad(v) )*dx(metadata=
                                                     {'quadrature_degree': q_degree})
  basis_pool[7]=theta[9]*inner(2*F*hS7, grad(v) )*dx(metadata=
                                                     {'quadrature_degree': q_degree})
  basis_pool[8]=theta[10]*inner(2*F*hS8, grad(v) )*dx(metadata=
                                                     {'quadrature_degree': q_degree})

  # Define residual
  R=0
  for i in range(len(basis_pool)):
    R+=basis_pool[i]

  ########################### Define displacement and force for each tendon ##################

  file_U = HDF5File(MPI.comm_world, tendonDate + 
                    "/convergenceAnalysis_V2/displacementHighRes/Tendon" + tendonDate +
                    "_" + meshName + "_U_" + condition + "_" + str(disp) + "_CG2.h5", 'r')
  force_used = np.loadtxt(tendonDate + "/forceData/" + tendonDate + 
                          "_MedianForceData_" + condition + "_" + str(disp) + "mm.txt")

  P=2*F*(theta[0]*hS0 + theta[1]*hS1 + theta[2]*hS2 + theta[5]*hS3 + theta[6]*hS4 
         + theta[7]*hS5 + theta[8]*hS6 + theta[9]*hS7 + theta[10]*hS8)

  boneDisp = Function(V, name='boneDisp')

  ##################################### VSI implementation ###################################
  
  for i in range(len(theta)):
    theta[i].assign(0.0)
  theta[target_index].assign(1.0)
   
  target_array_index=0
  for i in current_activate_index:
    theta[i].assign(target_array[target_array_index])
    target_array_index+=1
  
  # Assign displacement 
  file_U.read(u, 'U_' + condition + str(disp))
  file_U.read(boneDisp, 'U_' + condition + str(disp))
  normal_to_surf = n
  
  # Calculate traction, external force, and residual
  load_dir = Constant((-1.0, 0.0, 0.0))
  traction = dot(P, normal_to_surf)
  loadOnFace = dot(traction, load_dir)*dss
  P_ext = force_used
  tem=assemble(R-dot(traction,v)*dss) 

  # Apply boundary conditions

  for key in keyList:
    bc_key = DirichletBC(V, boneDisp, facetfct, keyList.index(key) + 1)
    bc_key.apply(tem)
  
  tem=tem[:]
  loss1 = np.inner(tem,tem)/(np.size(tem)*P_ext**2) 
  loss2 = pow(assemble(loadOnFace) - P_ext,2)/(P_ext**2) 
  loss_factor1 = 1.
#   print("loss1 = ", loss_factor1*loss1)
#   print("loss2 = ", loss_factor2*loss2)
  lam=0. # penalty term
  a = [0]*len(theta)
  penaltySum = 0
  for i in range(len(theta)):
    a[i]= theta[i].values()
    penaltySum += a[i]**2

  loss = loss_factor1*loss1 + loss_factor2*loss2 + 0.5*lam*penaltySum
  return loss 

def add_up_losses(target_array, tendonStamp, meshName, loss_factor2, combination, current_activate_index,
                      target_index):
  losses = 0.

  for i in range(len(combination)):
      losses += nonlinear_VSI_HGO(target_array, tendonStamp, str(combination[i][0]), 
                                  str(combination[i][1]), loss_factor2, meshName, current_activate_index,
                                  target_index)
  return losses
  

###################### Here's where the code execution starts ####################
if __name__ == "__main__":
  target_index=11
  activate_basis_index=[0,1,2,3,4,5,6,7,8,9,10]
  coeffs0=np.zeros(len(activate_basis_index))
  coeffs0 = np.array([1., 1., 1., 1., 1./3., 1., 1., 1., 1., 1., 1.]) 
  print("target_index=",target_index)
  
  meshName = "Coarse"
  conditionList = ["Intact", "Torn"]
  dispList = ["1", "2"]
  combinationList = np.array(list(product(conditionList, dispList)))
  combinationList = combinationList[1:,]
  loss_factor2 = 1e-6
  tendonStampList = ["20231012", "20231017", "20231107", "20231114", "20231201", 
                     "20231206", "20231212", "20240503", "20240517", "20241127_1",
                     "20241127_2", "20241219"]
  indexList = [0,1,2,3,4,5,6,7,8,9,10,11]
  indexList = [0]
  
  identifier = 'LF1E_6_NoBounds_CoarseMesh'
  for index in indexList:
    print("Tendon ", tendonStampList[index])
    loss0 = add_up_losses(coeffs0, "Tendon" + tendonStampList[index], meshName, loss_factor2, 
                          combinationList, activate_basis_index, target_index)
    print("Loss0 = ", loss0)
    
    bounds=np.zeros((coeffs0.size,2))
    bounds[:,0]= 0.#1e-5	#All terms positive
    bounds[:,1] = np.inf
    bounds[0,0] = 0.1
    bounds[1,0] = 0.04
    bounds[3,0] = 1.e-4
    bounds[4,1] = 1./3.
    num_activate_index=len(activate_basis_index)
    gamma_matrix=np.zeros((num_activate_index,num_activate_index))
    args={'num_theta':num_theta,'activate_basis_index':activate_basis_index,
            'target_vector_index':target_index,
            'frozen_index':[0,1,2,3,4], 'max_eliminate_step':6,
            'method':'SLSQP','bounds':bounds, 
            'tendonStamp': "Tendon" + tendonStampList[index], 
            'meshName': meshName, 'combination': combinationList, 
            'lossFactor': loss_factor2}
    
    folder_name = tendonStampList[index] + '/results/HGOHighOrderI1I2_SquaredNorm'

    if not os.path.exists(folder_name):
        os.makedirs(folder_name)
    
    save_to_file= folder_name + '/NoStag_' + identifier + '_CG2.dat'

    print(save_to_file)
    method_options={'disp':False,'ftol':1.0e-15,'eps': 1e-10,'maxiter': 1000 }

    gamma_matrix, loss=stepwise_minimization_HGOConverged(add_up_losses, coeffs0, 
                                                args_dict=args, grad_f=[],
                                                method_options=method_options,
                                                save_to_file=save_to_file)
    
    save_gamma= folder_name + '/gamma_NoStag_' + identifier + '_CG2.dat'
    np.savetxt(save_gamma,gamma_matrix)

====================================================================================================
VSIAdjointRotatorCuff\stepwise_minimization_HGOConverged.py
====================================================================================================
import numpy as np
import scipy.optimize as sp
import os
import sys

def iter_cb(m):
  print ("results = ", m)
  
def stepwise_minimization_HGOConverged(obj_f, x0, args_dict,F_threshold=1.0e16, method_options={}, grad_f=None, save_to_file=None):
  ##

  callback=None
  bounds=None
  current_activate_index=args_dict['activate_basis_index']
  target_index=args_dict['target_vector_index']
  num_theta=args_dict['num_theta']
  num_base_orign=len(current_activate_index)
  meshName = args_dict['meshName']
  tendonStamp = args_dict['tendonStamp']
  combination = args_dict['combination']
  lossFactor = args_dict['lossFactor']
  
  frozen_index=[]
  max_eliminate_step=num_base_orign-1
  if 'frozen_index' in args_dict.keys():
    frozen_index=args_dict['frozen_index']
  if 'max_eliminate_step' in args_dict.keys():
    max_eliminate_step=args_dict['max_eliminate_step']
  if 'method' in args_dict.keys():
    method=args_dict['method']
  if 'bounds' in args_dict.keys():
    bounds=args_dict['bounds']
    
  if 'disp' in method_options.keys():
    if method_options['disp']==True:
      callback=iter_cb
  gamma_matrix=np.empty((num_theta,max_eliminate_step+1))
  gamma_matrix[:]=np.NaN
  gamma_matrix[target_index,:]=1
  
  if save_to_file!=None:
    f=open(save_to_file,'ab')
  
  loss=np.zeros(max_eliminate_step+1)  
  res=sp.minimize(obj_f, x0,jac=grad_f, method=method, 
                  args=(tendonStamp, meshName, lossFactor, combination,
                        current_activate_index,target_index),
                        bounds=bounds, options=method_options,callback=callback )
  x0=res.x
  gamma_matrix[current_activate_index,0]=x0
  loss[0]=res.fun
  print('==============================================================')
  print('step=',0, ' current_activate_index=',current_activate_index,
        ' x0=',gamma_matrix[:,0],' loss=',loss[0] )
  if save_to_file!=None:
    info=np.reshape(np.append(gamma_matrix[:,0],(loss[0])),(1,-1))
    np.savetxt(f,info)
    f.flush()
    os.fsync(f.fileno())

  for step in range(len(current_activate_index)-1):
    if step==max_eliminate_step:
      break
    num_activate_index=len(current_activate_index)
    gamma_matrix_tem=np.zeros((num_activate_index-1,num_activate_index))
    loss_tem=np.ones(num_activate_index)*1.0e20 # why*1e20?
    for j in range(len(current_activate_index)):
      try_index=current_activate_index[j]
      # continue if j is in the frozen_index
      if try_index in frozen_index:
        continue
        
      current_activate_index_tem=np.delete(current_activate_index,j)
      x0_tem=np.delete(x0,j)
      bounds_tem=None
      if 'bounds' in args_dict.keys():
        bounds_tem=np.delete(bounds,j,0)
      res=sp.minimize(obj_f, x0_tem,jac=grad_f,  method=method, 
                      args=(tendonStamp, meshName, lossFactor, combination,
                            current_activate_index_tem,target_index),
                      bounds=bounds_tem, 
                      options=method_options,callback=callback)
      gamma_matrix_tem[:,j]=res.x
      loss_tem[j]=res.fun
    
    drop_index=np.argmin(loss_tem) 
    print('loss_try=',loss_tem)
    loss_try=loss_tem[drop_index]  
    F=(loss_try-loss[step])/loss[step]*(num_base_orign-num_activate_index+1)
    
    if F<F_threshold:
      current_activate_index=np.delete(current_activate_index,drop_index)
      x0=np.delete(x0,drop_index)
      if 'bounds' in args_dict.keys():
        bounds=np.delete(bounds,drop_index,0)
      gamma_matrix[current_activate_index,step+1]=gamma_matrix_tem[:,drop_index]
      loss[step+1]=loss_try
      
    else:
      break
    if save_to_file!=None:
      info=np.reshape(np.append(gamma_matrix[:,step+1],(loss[step+1])),(1,-1))
      np.savetxt(f,info)
      f.flush()
      os.fsync(f.fileno())
    print('==============================================================')
    print('step=',step+1, ' current_activate_index=',current_activate_index,' x0=',gamma_matrix[:,step+1],' loss=',loss[step+1] )
    
  return gamma_matrix, loss
